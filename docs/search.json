[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MADS Computing",
    "section": "",
    "text": "Preface\nThis site contains lecture notes and materials for the data engineering and computing sequence of the Master of Science in Applied Data Science program.\nThis course is primarily for students in the MADS program. Other students will only be admitted with the instructor’s permission.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "data-engineering.html",
    "href": "data-engineering.html",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "Basic information",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering.html#basic-information",
    "href": "data-engineering.html#basic-information",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "Fall 2025, mini 2\nMonday and Wednesday, 9:30-10:50am\nTBA\nInstructor: Julia Walchessen\nTA: Anna Rosengart\nOffice hours:\n\nAnna: TBA\nJulia: Monday/Wednesday 11:00-12:00, Baker 129E",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering.html#course-description",
    "href": "data-engineering.html#course-description",
    "title": "Data Engineering and Distributed Environments",
    "section": "Course description",
    "text": "Course description\nStatisticians and data scientists in industry are increasingly expected to work with data from complex sources: not just spreadsheets and CSV files, but relational databases, distributed databases, and streams, often integrating data from dozens of different systems. This course introduces the basic principles of data engineering, beginning with relational databases and continuing to distributed databases in the cloud. Students will learn SQL, practice using Python to query databases, and be introduced to cloud computing.\nThis course is primarily for students in the Master of Science in Applied Data Science program. Students in this course should have prior experience programming in Python (such as through 36-650), using Git, and working from the command-line shell.",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering.html#learning-objectives",
    "href": "data-engineering.html#learning-objectives",
    "title": "Data Engineering and Distributed Environments",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this course, students will be able to:\n\nDesign SQL tables to store complex, frequently updated data.\nWrite SQL queries to group, aggregate, and summarize data stored in multiple interconnected tables.\nUse Python to execute SQL queries and dynamically act upon stored data.\nLoad data into SQL databases and update existing data.\nDevelop Python data pipelines that ingest data from multiple sources, load it into a database, and produce automated reports summarizing that data.\nRun data pipelines on cloud computing resources.\nWork with data stored in distributed file systems.",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering.html#books-and-references",
    "href": "data-engineering.html#books-and-references",
    "title": "Data Engineering and Distributed Environments",
    "section": "Books and references",
    "text": "Books and references\nThere is no required book for this course. We recommend the following reference resources:\n\nMark Lutz, Learning Python, 5th edition, O’Reilly. Available free through the CMU library.\nAlan Beaulieu, Learning SQL, O’Reilly. Also available free.\nThe PostgreSQL manual.",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering.html#homework-and-project",
    "href": "data-engineering.html#homework-and-project",
    "title": "Data Engineering and Distributed Environments",
    "section": "Homework and project",
    "text": "Homework and project\nThis course features regular homework assignments, due Wednesday afternoons. Many of these will involve writing code to complete particular data tasks. Homework assignments will be posted on Canvas. Homework must be submitted as a PDF on Gradescope, and can be produced either with a Jupyter Notebook or in a Quarto document with Python code cells.\nIt’s your responsibility to ensure the PDF file is legible in Gradescope. This includes ensuring the pages are a reasonable size, and also using headings, formatting, and text to make it easy to find your answers. You should not include extraneous code or output not relevant to the problems.\nYou are expected to follow consistent style in your code. For Python code, the style we will use is PEP 8, the standard Python style guide. Style will be part of the homework grade.\nAlong with the homework, there will be a semester-long project, completed in groups of 3-4 students. The project will be completed in several parts throughout the semester. The project will culminate with a final submission at the end of the semester in the form of a GitHub repository containing working code.\nParts of the project will be coordinated with 36-611, so you can practice relevant professional skills.\n\nLate work\nFor homework submissions, you will have three “grace days” you can use throughout the semester. Each time you use a grace day for an assignment, you get 24 hours extra to submit the assignment. You do not need any excuse to use grace days. Once you have used all three grace days, late work will not be accepted.\nThis system is meant to allow you flexibility, so that ordinary problems (minor illness, forgot a deadline, had to finish another class’s big assignment, traveled to an event) don’t harm you, and so you do not need my permission to handle unexpected problems. If you experience a serious emergency that prevents you from completing work for a longer time, contact me so we can make arrangements.",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering.html#attendance-and-participation",
    "href": "data-engineering.html#attendance-and-participation",
    "title": "Data Engineering and Distributed Environments",
    "section": "Attendance and participation",
    "text": "Attendance and participation\nClass attendance and participation is essential. If there’s any one message to be learned from pedagogical research, it’s that listening passively to a lecture is not a good way to learn how to think about complicated problems. I will often ask class questions or ask you to complete short activities in small groups. You are expected to attend class and participate in these activities.\nWhen attending class, you are expected to follow all COVID-19 precautions required by the University.",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering.html#grading",
    "href": "data-engineering.html#grading",
    "title": "Data Engineering and Distributed Environments",
    "section": "Grading",
    "text": "Grading\nThe homework and project will be the basis of course grades: 60% homework, 40% project.\nFinal grades will be based on this scale: A = [93, 100]; A- = [90, 93); B+ = [87, 90); B = [83, 87); B- = [80, 83); C+ = [77, 80); C = [73, 77); C- = [70, 73); D = [60, 70); R = [0, 60).",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering.html#academic-integrity",
    "href": "data-engineering.html#academic-integrity",
    "title": "Data Engineering and Distributed Environments",
    "section": "Academic integrity",
    "text": "Academic integrity\nDiscussing homework and projects with your classmates is allowed and encouraged, and helping explain ideas to each other is a core part of the academic experience. But it is important that every student get practice working on their own. This means that all the work you turn in must be your own. You must devise and write your own code, generate your own graphics, and write your own solutions and reports.\nYou may use external sources (books, websites, papers) to\n\nLook up Python documentation, find useful packages, find explanations for error messages, or remind yourself about the functions to do some task,\nFind reference materials on statistical methods,\nClarify material from the course notes or examples.\n\nBut external sources must be used to support your work, not to obtain your work. You may not use them to copy code, text, or graphics without attribution. You may not use any prior course’s or textbook’s homework solutions in any way. This prohibition applies even to students who are re-taking the course. Do not copy old solutions (in whole or in part), and do not “consult” or read them. Doing any of that is cheating, making any feedback you get meaningless and any evaluation based on that assignment unfair.\nIf you do use any material from other sources, you must clearly mark its source. Text taken from other sources must be in quotation marks with citations; figures from other sources need a caption indicating the source; and code from other sources must have a comment indicating the source. We must be able to determine who wrote any material you submit, and you must not falsely imply that you completed work actually done by others.\n\nGenerative AI\nSome of you may be tempted to use generative AI tools like ChatGPT, Gemini, Llama, or Claude to complete some of your work in this course. These tools can help explain code and debug problems. However, the same rules described above apply: your use of generative AI must support your work. You may not simply copy and paste questions into generative AI and submit the answers as your own work. To build expertise in data engineering, you must practice basic skills so you can master the core concepts; if you outsource the basic skills, you will never get the practice you need to become an expert.\nI will consider the use of generative AI tools beyond these boundaries to be “unauthorized assistance”, as defined in the University Policy on Academic Integrity.\n\n\nPenalties\nPlease talk to me if you have any questions about this policy. Any form of cheating or plagiarism is grounds for sanctions to be determined by the instructor, including grade penalties or course failure. Students taking the course pass/fail may have this status revoked. I am also obliged in these situations to report the incident to your academic program and the appropriate University authorities. Please refer to the University Policy on Academic Integrity.",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering.html#accommodations-for-students-with-disabilities",
    "href": "data-engineering.html#accommodations-for-students-with-disabilities",
    "title": "Data Engineering and Distributed Environments",
    "section": "Accommodations for students with disabilities",
    "text": "Accommodations for students with disabilities\nIf you have a disability and have an accommodations letter from the Disability Resources office, I encourage you to discuss your accommodations and needs with me as early in the semester as possible. I will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, I encourage you to contact them at access@andrew.cmu.edu.",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering.html#diversity-and-inclusion",
    "href": "data-engineering.html#diversity-and-inclusion",
    "title": "Data Engineering and Distributed Environments",
    "section": "Diversity and inclusion",
    "text": "Diversity and inclusion\nWe must treat every individual with respect. We are diverse in many ways, and this diversity is fundamental to building and maintaining an equitable and inclusive campus community. Diversity can refer to multiple ways that we identify ourselves, including but not limited to race, color, national origin, language, sex, disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic information. Each of these diverse identities, along with many others not mentioned here, shape the perspectives our students, faculty, and staff bring to our campus. We, at CMU, will work to promote diversity, equity and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice. We acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a campus community that increasingly embraces these core values.\nEach of us is responsible for creating a safer, more inclusive environment.\nUnfortunately, incidents of bias or discrimination do occur, whether intentional or unintentional. They contribute to creating an unwelcoming environment for individuals and groups at the university. Therefore, the university encourages anyone who experiences or observes unfair or hostile treatment on the basis of identity to speak out for justice and support, within the moment of the incident or after the incident has passed. Anyone can share these experiences using the following resources:\n\nCenter for Student Diversity and Inclusion: csdi@andrew.cmu.edu, (412) 268-2150\nReport-It online anonymous reporting platform. username: tartans password: plaid\n\nAll reports will be documented and deliberated to determine if there should be any following actions. Regardless of incident type, the university will use all shared experiences to transform our campus climate to be more equitable and just.",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering.html#wellness",
    "href": "data-engineering.html#wellness",
    "title": "Data Engineering and Distributed Environments",
    "section": "Wellness",
    "text": "Wellness\nAll of us benefit from support during times of struggle. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is almost always helpful.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 or visit their website. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help.",
    "crumbs": [
      "Data Engineering and Distributed Environments"
    ]
  },
  {
    "objectID": "data-engineering/data-pipeline.html",
    "href": "data-engineering/data-pipeline.html",
    "title": "1  The Data Pipeline",
    "section": "",
    "text": "1.1 ETL\nIn this mini and the following one, we’ll discuss several pieces of how to work with large datasets in practice:\nBut first, we must look at the entire process end-to-end: the data pipeline, following the data from when it arrives at the company to its storage, use, and reuse. As statisticians, we often gloss over most steps of the pipeline, and start our analysis with a nicely curated CSV containing all the relevant information. But in practice, someone must do the curation—and more importantly, someone must design the data pipeline, and think about the life of the data from beginning to end. What data should be collected, where should it be stored, and how can it be put to use?\nAs we discuss data pipeline design, let’s consider a motivating example.\nThe first part of the process is to get the data. This is not as easy as it sounds. The data you need comes from many sources in many different formats, and you must work to get what you need.\nThe usual acronym for this process is ETL: extract, transform, and load. You must extract the relevant data from various sources, transform it to be consistent and match whatever schema you use to store it, and load it into your choice of database or storage system. This might happen repeatedly as you obtain updated data, or when you decide you need to change what data is extracted and stored.\nWe can see that ETL can be very complicated in its own right. Data always seems to come in the wrong format, it’s frequently updated, and people want to use it for many different things. ETL involves designing a core storage system for the parts you want to keep, and writing all the necessary code to load data into that storage system. For this class, that storage system will often be a SQL database.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Data Pipeline</span>"
    ]
  },
  {
    "objectID": "data-engineering/data-pipeline.html#etl",
    "href": "data-engineering/data-pipeline.html#etl",
    "title": "1  The Data Pipeline",
    "section": "",
    "text": "Exercise 1.1 (Lead generation for HamsterCard) Credit card companies need to market their cards to prospective customers, since each customer is a potential source of revenue. That means finding new people who don’t have the card but could be interested in getting one. Prospective customers are called leads, and companies try to acquire leads so they can send them advertisements and convince them to sign up.\n\n\n\nOne of HamsterCard’s partner companies generating leads. Liberty Meadows by Frank Cho.\n\n\nHamsterCard acquires leads in several ways:\n\nThey buy customer lists from partner banks, with customer names, addresses, and basic financial information. These are provided as tab-delimited text files.\nThey buy email lists from marketing companies (that themselves bought the email addresses from other online services). The lists include email addresses, variables giving interests and demographic information learned or inferred by the marketing companies, and perhaps information about other products that email is known to be associated with. The lists are given as CSVs.\nThey buy subscriber lists from Hamsters Monthly, Hamster Fancy, and other top newsletters and magazines. These lists give email addresses, subscription dates, and click-through rates measuring how many times the subscriber clicked on links in the email newsletters. The lists are in a shared SQL database that HamsterFacts has read-only access to.\nThey buy subscription lists from Hamster Facts, a text message service that sends hourly hamster facts texts. The subscription list has phone numbers, but no other information about the subscribers. The list is provided as a text file with one phone number per line.\n\nSuppose you’ve been assigned to build a database of leads by combining these sources, so the marketing team can use them to send emails, postcards, and texts.\nFor each step in the extract, transform, and load process, what decisions do you need to make? What problems might occur?",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Data Pipeline</span>"
    ]
  },
  {
    "objectID": "data-engineering/data-pipeline.html#feature-engineering",
    "href": "data-engineering/data-pipeline.html#feature-engineering",
    "title": "1  The Data Pipeline",
    "section": "1.2 Feature engineering",
    "text": "1.2 Feature engineering\nWhen the ETL process is done, hopefully you have all the data you need in a convenient format. Now you need to solve some business questions using your data, and since you’re a statistician, you’ll probably have to fit some models.\nThe variables we use in our models don’t always match the variables stored in our databases. Some differences are trivial: a categorical variable might be turned into a one-hot encoded vector, for instance. But some differences are more substantial.\nFeature engineering refers to the process of manipulating data to extract new features (predictors) that can be used in fitted models. Let’s consider an example.\n\nExample 1.2 (Fraud detection for HamsterCard) Credit card fraud is a major problem, and every credit card issuer must check transactions for signs of fraud so fraudulent transactions can be rejected or reversed. HamsterCard thus has an automated system that must make classifications: when each transaction is received, it gets features about that transaction, and it must classify it as “fraudulent”, “questionable”, or “valid”. Fraudulent transactions will be blocked, questionable ones may get checked by sending a text or email to the cardholder, and valid transactions will go through.\nFor each transaction, HamsterCard receives the following information:\n\nThe date and time\nThe vendor (such as the store the card is being used at)\nThe geographic location of the purchase, if in person, or a code indicating if the purchase was made online or over the phone\nThe card number\nThe purchase amount\n\nWith only these features, a classifier would have a very hard time. So before giving the information to the classifier, HamsterCard’s system looks up additional information, such as\n\nthe number of times this customer has made purchases with the same vendor previously\nthe average amount this customer spends per purchase\nthe number of past fraudulent transactions for this customer\n\nGetting this information may require pulling data from multiple databases inside HamsterCard, but it would make the fraud detector much more accurate.\n\nPart of being a data scientist in a large company, then, is feature engineering: identifying data that may be relevant for a statistical problem, and pulling out interesting features from it. These features may not be stored in the original data but instead are calculated from it. The challenge is knowing what to calculate, and this is why a good data scientist must understand their data sources and the problems they are trying to solve.\nIn this course, a lot of feature engineering will involve writing SQL queries to fetch appropriate data from our database.\n\nExercise 1.2 (Feature engineering for fraud) Suggest three additional features that HamsterCard may want to extract from its data sources to assist in the fraud detection task in Example 1.2.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Data Pipeline</span>"
    ]
  },
  {
    "objectID": "data-engineering/data-pipeline.html#model-building",
    "href": "data-engineering/data-pipeline.html#model-building",
    "title": "1  The Data Pipeline",
    "section": "1.3 Model building",
    "text": "1.3 Model building\nYou’ll spend plenty of time on this in your other classes, so we won’t discuss it here.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Data Pipeline</span>"
    ]
  },
  {
    "objectID": "data-engineering/data-pipeline.html#model-deployment",
    "href": "data-engineering/data-pipeline.html#model-deployment",
    "title": "1  The Data Pipeline",
    "section": "1.4 Model deployment",
    "text": "1.4 Model deployment\nOnce you’ve built a model, it needs to be put to use. Sometimes this means you use the model to answer some research questions, write a report, and give a presentation about what you’ve learned, since the project goals were to answer some questions. This is what you’ve been learning to do in other courses, like when you write data analysis reports for 36-617.\nBut in other cases, the model needs to be used in a product. Your model for recommending movies to streaming service subscribers has to be used to make recommendations; your model for fraud detection needs to be used to detect fraud in new data. This usually means applying your model to new data beyond the training dataset, and that means trouble.\n\n1.4.1 Training\nA model put into production use may be used for many months or years. Over that time, additional training data may be collected. It may be useful to update the model with the new data as it’s collected, so the model is always trained with the largest and most up-to-date possible dataset.\nThat implies you must choose: How often should the model be updated?\n\nExercise 1.3 (Re-training a HamsterCard fraud detector) Consider HamsterCard’s fraud detector in Example 1.2. Each day, HamsterCard processes millions of new transactions, and in principle it could feed these transactions into its fraud detection model to retrain it.\nWhat reasons might there be to retrain the model as quickly as possible, say every day?\nWhy might it be a problem to use the previous day’s data to train the fraud detector for the next day?\n\nNow, it’s important not to automate this too much. If you retrain the model automatically every day or week, then immediately start using the new model, you may run into problems. What if the new data causes the model to perform worse? What if a batch of unusual data arrives and biases the model? You’re going to need a process to check the model every time it is updated, and only put it to use if it passes the checks. In fact, we’ll need to monitor the model’s performance even if we’re not updating it, as we’ll see in Section 1.5.\n\n\n1.4.2 Prediction\nThere are different ways to put a predictive model into production.\nOne approach is batch prediction. Predictions are made in batches, say every week or every month, and in large quantity. That batch of predictions is then used to do various things until the next batch is produced. The predictions are produced by some automated system that runs periodically, and perhaps we can afford a slow system: if a batch takes a full day to produce, that’s fine if we’re only doing it once a month.\nThe opposite is online prediction: predictions are made in real time as new observations come in. The predictions are immediately used in the product, and so it is often important for the predictions to be made very quickly. We can’t afford a slow, complicated model that takes many minutes to look up necessary data and make its prediction; we might deliberately use a simpler but less accurate model because we do not have time for a more accurate but slower model.\nThe word “latency” is often used to describe the time between something being requested and it being delivered, and so we could define a model’s latency as the time between providing an observation and receiving a prediction for that observation. In batch predictions, we can tolerate high latency; in online predictions, we may need the latency to be as small as possible.\n\nExercise 1.4 In Example 1.1, we listed four tasks HamsterCard must do with data. Each of these would involve a model (or several).\nFor each task, identify if predictions would be made in batches or online, and how much latency can be tolerated.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Data Pipeline</span>"
    ]
  },
  {
    "objectID": "data-engineering/data-pipeline.html#sec-pipeline-monitoring",
    "href": "data-engineering/data-pipeline.html#sec-pipeline-monitoring",
    "title": "1  The Data Pipeline",
    "section": "1.5 Monitoring",
    "text": "1.5 Monitoring\nWe statisticians tend to think of models as static objects. We spend time carefully constructing a model, then we fit the final version, and then we use it. But models used for prediction on new data must face the real world, and the real world sucks.\n\n1.5.1 Distribution shift\nEver since you first learned to fit regression models, you were taught not to extrapolate beyond the range of your data. But whenever you use a model trained on the past to predict observations received in the present, you are implicitly extrapolating: you are assuming that the same relationship between \\(X\\) and \\(Y\\) is present now as was present when you fit the model (Hume 1748). You are also assuming the population distribution of \\(X\\) is roughly the same as it was when you trained the model.\nLet’s examine each of these in turn.\n\nDefinition 1.1 (Concept drift) Regression models typically try to approximate \\(\\E[Y \\mid X = x]\\), where \\(Y \\in\n\\R\\) and \\(x \\in \\R^p\\); classifiers try to approximate \\(\\Pr(Y = k \\mid X = x)\\) for a class \\(k\\), or try to pick the class with the highest probability. When we train models using a sample, we are hoping that the estimate we obtain from that sample is a good approximation in the population.\nBut what if the true relationship \\(\\E[Y \\mid X = x]\\) changes? Our model is now an approximation to the wrong relationship. Depending on the change, our predictions could become much less accurate. The same applies for a change in the true classification probability \\(\\Pr(Y = k \\mid X = x)\\).\nThis problem is called concept drift.\n\nAnother possibility is that the distribution of \\(X\\) may change even while \\(\\E[Y\n\\mid X = x]\\) stays the same.\n\nDefinition 1.2 (Covariate shift) Suppose we obtained our training sample from some distribution \\(F_\\text{train}\\), but over time, new observations begin to come from some different distribution \\(F_\\text{new}\\).\nThis problem is called covariate shift.\n\nThis is much more akin to the extrapolation you’re familiar with. If \\(F_\\text{new}\\) produces values of \\(X\\) that are very different from those drawn from \\(F_\\text{train}\\), then your model will have to extrapolate beyond the range of \\(X\\) it was trained on. This problem is more severe when \\(X\\) is multidimensional (i.e. you are predicting using many covariates), because a new distribution could produce combinations of features your model was never trained on, even if individually they are within the same range.\nBoth concept drift and covariate shift can cause the performance of a predictive model to decline over time.\n\nExample 1.3 (Predicting house prices) Consider a model that predicts a house’s value using features like its size, age, number of bedrooms, and so on—the kind of model that sites like Zillow and Redfin use to estimate values of houses that are not for sale.\nIf house prices suddenly go up in an area due to increased demand, this can cause concept drift: you have the same houses with the same \\(X\\) values, but the value \\(Y\\) is suddenly much higher. If you do not update your model, it will continue predicting the old prices.\nSimilarly, if preferences change and people suddenly hate large, spacious homes because of how much time they have to spend sweeping the floors, this would be concept drift.\nOn the other hand, if the model were trained on an area with mostly single-family homes, but then a developer came in and built hundreds of units of townhomes and condos, that could cause covariate shift: the new housing units are very different than those the model was trained on, and the model will have to extrapolate to predict their values.\n\n\nExercise 1.5 Give examples of how concept drift and covariate shift could affect HamsterCard’s fraud detection system.\n\n\n\n1.5.2 Feedback loops\nWe statisticians are used to think of our models as being separate from the world being modeled. The models live in our computer; the data comes from the world out there. Nothing we do in R or Python affects the real world.\nBut if you’re building a product that uses your models to make decisions that do something in the real world—send advertising, block a user, purchase a stock, or whatever—those decisions may affect the world and affect future data you collect.\nA simple example: Many police departments keep extensive data on reported crimes and arrests, and use this data to predict locations where high crime or arrest rates are expected. They then send additional police patrols to those areas. If those patrols are more likely to conduct traffic stops, search suspicious pedestrians, and issue tickets for loitering, they are creating more reported events in the data—and hence contribute to the area being predicted to have more events in the future. Police may be sent to areas based on their own work in those areas, not because they have an intrinsically higher crime rate than other areas. This is a feedback loop: the model’s output is creating data that will affect its outputs.\n\nExercise 1.6 Give an example of how HamsterCard’s marketing models—which choose potential customers to target with advertising—could create a feedback loop.\n\nPreventing feedback loops can be difficult. After all, the point of the model is often to make changes in the real world. There is no one strategy to prevent them, but in particular situations we may be able to.\nFor example, recommender systems can easily have feedback. If you recommend movies to viewers based on what you think they’ll like, but estimate whether they’ll like movies based on how many other people watch and enjoy them, you may tend to make popular movies more popular while rarely recommending movies that people haven’t viewed yet. This creates a “rich get richer” situation: nobody watches unknown movies, so you don’t know if anyone will like them, so you don’t recommend them to anyone. This can be avoided by deliberately recommending a mix of movies highly rated by your algorithm and less well-known movies with high uncertainty.\n\n\n1.5.3 Tracking performance\nTogether, these problems suggest an important fact: We cannot simply train a model, deploy it, and never look back. We must periodically check that the model is still performing well and serving its intended purpose.\nWhat you monitor depends on what is available to you. If you eventually observe outcomes for your predictions, you can measure their accuracy or success rate and track it over time. You can also track metrics about the typical inputs to your model, the distribution of covariates, or other features relevant to your prediction.\n\nExercise 1.7 Give some examples of metrics HamsterCard might track for its marketing models.\n\n\n\n\n\nHume, David. 1748. An Enquiry Concerning Human Understanding. London: A. Millar.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Data Pipeline</span>"
    ]
  },
  {
    "objectID": "data-engineering/database-fundamentals.html",
    "href": "data-engineering/database-fundamentals.html",
    "title": "2  Database Fundamentals",
    "section": "",
    "text": "2.1 Relational databases\nIn statistics and data science classes, you often work with data provided to you as files. Your homework or project requires you to analyze some data, so the professor gives you a CSV file to download and open in R or Python. This is also standard practice in scientific data analysis: a scientist collects some data and sends you a CSV or spreadsheet to analyze.\nBut in businesses, data is constantly being updated and changed. You might be asked to do things like:\nAll of these would involve building a system that runs constantly and gets updated with the latest data. That data may come from many sources at once: in a large company, you might have sales data from in-person stores, billing data from the online store, analytics data tracking visitors to the website, marketing data from ad campaigns, product inventory data from the company logistics system, visitor data from a company that uses cell phones to track how many people visit your stores, and many other things besides.\nWorse, all of that data comes from systems run by entirely different parts of the company, all using different software sold by different companies. And all of that data is being constantly changed by many people at the same time. Using a CSV file would be impossible: how do you let hundreds of people work on the same CSV at once, ensuring their work is always using the latest version of the file even as other people edit it?\nThis is why we use databases.\nA database is a system for organizing information so it can be efficiently accessed, updated, and managed, even when the data is large and the updates are frequent. Relational databases (RDBs) have been the most common type of database since the 1970s and are still very important. They are organized around data tables with fixed columns and rows for each record or observation.\nRelational databases tend to be design-first systems. First, you specify a schema for your data—giving the tables, the columns they contain, and the types of data stored in each column—and then you enter data that conforms to that schema. A properly designed schema can provide very flexible and powerful queries.\nOnce you have a schema and data, RDBs make it easy to select, filter, and order data, and are particularly suited for combining data from multiple tables—their explicit modeling of the relationships between tables is the source of their name. This is commonly done using a (mostly) standardized language called SQL, which stands for Structured Query Language. Requests written in SQL are called queries.\nMany of the fundamental relational query operations will be familiar to you, since they are supported in many packages for working with data. For example, in R, the dplyr package provides a set of functions for querying, modifying, and joining data frames, and these are very similar to the operations supported by relational databases.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Database Fundamentals</span>"
    ]
  },
  {
    "objectID": "data-engineering/database-fundamentals.html#relational-databases",
    "href": "data-engineering/database-fundamentals.html#relational-databases",
    "title": "2  Database Fundamentals",
    "section": "",
    "text": "2.1.1 Tables\nThe basic unit of data storage in an RDB is the table. Tables are also sometimes called relations, schemas, and entities in an RDB context. There may be many tables in a single database, so a database is a collection of related tables. A table is defined by its attributes, or columns, each of which has a name and a type.\nYou can think of tables as being much like data frames in R or Python, since each row defines a mapping from attribute names (the columns) to values (stored in that column). Unlike data frames in R or Python, the rows do not have an order; there is no notion of row indices unless you create a column for that purpose.\nAlso unlike data frames in R or Python, tables in relational databases are designed in advance: the columns and their types are specified first, before creating data. While it is possible to modify a table, one does not add and delete columns from tables nearly as often as you do in R or Python.\n\n\n2.1.2 Data types\nThe type of a piece of data describes the set of possible values that data can have and the operations that can apply to it.\nIn an RDB, we specify the type of each column in advance. PostgreSQL, for instance, supports a wide variety of data types, including:\n\nNumeric types, such as integers, fixed-precision floating point numbers, arbitrary precision real numbers, and auto-incrementing integers (serial).\nText, including fixed-length and arbitrary character strings.\nMonetary values\nDates and times\nBoolean values\nGeometric types, such as points, lines, shapes\nElements in sets\nJSON structures\n\nDatabase systems understand how to operate on these types, allowing you to search for all dates that are on a Wednesday, or all text containing particular substrings, and so on.\n\n\n2.1.3 Relationships between tables\nWe can think of tables as representing some entity that we are modeling in our problem. For example, a students table might contain enrollment information for a university, where each row represents a single student. Each row in the courses table might represent a single course offered in a specific semester.\nWe link tables to define relationships among entities.\nFor example, each course is linked to multiple students who are enrolled in it. A separate enrollment table may have one row per student per course, identifying the student and the course they are enrolled in.\nA good design of the database tables can make it more efficient to query these relationships, such as to list the students enrolled in a class.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Database Fundamentals</span>"
    ]
  },
  {
    "objectID": "data-engineering/database-fundamentals.html#the-client-server-model",
    "href": "data-engineering/database-fundamentals.html#the-client-server-model",
    "title": "2  Database Fundamentals",
    "section": "2.2 The client-server model",
    "text": "2.2 The client-server model\nSo far, relational databases sound much like using a collection of data frames in Python or R. But there is a key difference. Most (but not all!) SQL database systems are based on a client-server model.\n\nServer: A program running continuously on some server. Accepts requests (potentially from many users at the same time) to update and query data, and stores the canonical version of the database.\nClient: Any program that can connect to the server, send queries, and receive results from it.\n\nThe client and server need not be on the same computer, or even on the same continent. Often, companies with large databases will have a central database server with huge hard drives and lots of memory; business systems (like an inventory tracker or logistics system) will send their queries to this database to keep it up-to-date, while analysts will sit on their laptops and send queries to produce reports for their bosses.\nA key feature of the client-server model is that the server can serve multiple clients at the same time. The server goes to extraordinary efforts to ensure that it can process the analyst’s queries while simultaneously recording every new sale on the company’s website—and while keeping all results consistent and complete.\nAnd because the server is separate from the client, the server need not be written in the same programming language, and clients implemented in many different languages can access the same server. You can connect to database servers using code written in Python, R, C, Java, Ruby, PHP, or any one of dozens of languages that have the right packages. Because relational databases are so popular, almost any reasonable language you might use will have packages for connecting to popular relational database systems.\nThe client-server model also means that the server can enforce rules about who is allowed to access, update, or delete any particular data. Each client has a username and password, and every table has a set of permissions defining which users are permitted to see it and what operations they are allowed to perform. These permissions can get very detailed, allowing only very specific types of queries for specific users. Companies commonly use this for internal security, and to ensure nobody accidentally deletes important data they did not intend to.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Database Fundamentals</span>"
    ]
  },
  {
    "objectID": "data-engineering/database-fundamentals.html#querying-databases",
    "href": "data-engineering/database-fundamentals.html#querying-databases",
    "title": "2  Database Fundamentals",
    "section": "2.3 Querying databases",
    "text": "2.3 Querying databases\nTo query a database means to send a request to the database server for data matching specific criteria, to send new data to add to the database, or to request deletions or updates. If a query alters the database by adding or changing data, the server automatically stores the updated data—you do not have to explicitly “save” the new data.\nMost relational databases use a (mostly) standardized language called SQL, or Structured Query Language. Your query is written in SQL and submitted directly to the database, which returns a table of matching results (or information about which records have been updated or deleted).\nBecause the language is standardized, you can use SQL regardless of the database system your company has chosen to use. Many different relational database systems (PostgreSQL, MySQL, MariaDB, Oracle, Microsoft SQL Server, Firebird, …) exist, and while they all have unique features and quirks that mean SQL for one won’t work perfectly with another, the basic ideas translate between them all.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Database Fundamentals</span>"
    ]
  },
  {
    "objectID": "data-engineering/database-fundamentals.html#sec-acid-guarantees",
    "href": "data-engineering/database-fundamentals.html#sec-acid-guarantees",
    "title": "2  Database Fundamentals",
    "section": "2.4 ACID guarantees",
    "text": "2.4 ACID guarantees\nAn RDB stores our data, and we read and operate on that data through requests sent to the database. These requests can be grouped into transactions. A transaction may be a group of multiple queries (insertions, updates, and so on) that complete a particular task.\nModern RDBs may receive many transactions at once, often operating on the same pieces of data. Particular care is needed to ensure that transactions are performed reliably and consistently.\nFor example, consider what would happen in the following cases:\n\nA transaction for a commercial payment is transferring money from your bank account and to another account. But the process ends after the money is deducted from one account but before adding it to the other.\nA similar transaction completes just before the power goes out in the server room\nA similar transaction completes even though you don’t have enough money in your account to make the payment.\n\nThese are all boundary cases, but they can happen. And if they do, the viability of the entire system can be compromised.\nSo, RDBs are designed to make several strong guarantees about their performance, the so-called ACID guarantees:\n\nAtomic: A transaction either succeeds entirely or fails leaving the database unchanged.\nConsistent: A transaction must change the database in a way that maintains all defined rules and constraints.\nIsolated: Concurrent execution of transactions (by multiple users accessing the database at the same time) results in a transformation that would be obtained if the transactions were executed serially.\nDurable: Once a transaction is committed, it remains so even in the face of crashes, power loss, and other errors.\n\nThis is another advantage of RDBs over ad hoc data storage.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Database Fundamentals</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql.html",
    "href": "data-engineering/sql.html",
    "title": "3  SQL Basics",
    "section": "",
    "text": "3.1 Connecting to our database server\nLet’s learn how to write basic SQL queries. We’ll start by writing the queries by hand, and instead of using Python to submit them, we’ll use the psql command-line program. This program allows you to type queries and get responses back from the server.\nIf you need help writing SQL queries, finding functions in Postgres, and so on, consult the PostgreSQL manual. It is among the best-written and most comprehensive manuals you can get for open-source software.\nThe Department runs a PostgreSQL server in Microsoft Azure for this course. You should receive an automated email with your username and password.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql.html#connecting-to-our-database-server",
    "href": "data-engineering/sql.html#connecting-to-our-database-server",
    "title": "3  SQL Basics",
    "section": "",
    "text": "3.1.1 Azure Data Studio\nA simple way to work with PostgreSQL in Azure is Microsoft’s Azure Data Studio. Download and install it. Once you’ve installed it, find the Extensions button on the left and search for the PostgreSQL extension by Microsoft, and install it.\nYou can now connect to our PostgreSQL server with the following information:\n\n\n\nField\nValue\n\n\n\n\nConnection type\nPostgreSQL\n\n\nServer name\npinniped.postgres.database.azure.com\n\n\nAuthentication type\nPassword\n\n\nUser name\nYour username\n\n\nPassword\nYour password\n\n\nDatabase\nSelect either your database or examples\n\n\n\nOnce connected, you’ll be able to see all the databases and tables your account can access on the left. The examples database is for examples used in class, and you can read it but not modify it. You have your own database, under your Andrew ID, that you can work in.\nWhen you connect, you’ll get a query window (or can create one with File→New Query) for writing queries. You can select at the top which database to run those queries in.\nYou can also create a notebook (based on Jupyter notebooks) to access a specific database. For example, open the list of databases on the left, right click on examples, and select “New Notebook”. You will use these notebooks for your homework.\nNote: You will not be able to connect through the eduroam wireless network, since it blocks PostgreSQL connections. If you’re on campus, always use CMU-SECURE.\n\n\n3.1.2 Entering SQL statements\nSQL consists of a sequence of statements.\nEach statement is built around a specific command, like SELECT or INSERT, with a variety of modifiers and optional clauses (like WHERE or LIMIT).\nCommand names and modifiers are not case-sensitive. You can write SELECT, select, or sEleCt. It is standard style to use upper case command names and lower case for table and column names.\nSQL statements can span several lines, and all SQL statements end in a semi-colon (;).\nKeep in mind: strings are delimited by single quotes 'like this', not double quotes \"like this\". To write a single quote in a string, write it twice: 'can''t' represents the string can't.\nSQL comments are lines starting with --.\n\n\n3.1.3 Submitting assignments\nYour SQL homework assignments should be completed as notebooks in Azure Data Studio. Use File→New Notebook to create one, and connect it to the right database. (These are a lot like Jupyter notebooks, or the notebooks you may have used in Visual Studio Code in 36-650.)\nUse text cells to create headings labeling each problem, and to include text answers to any questions that ask for your comments. Use SQL cells for your queries. Use LIMIT clauses on queries returning lots of rows so the output isn’t enormous—we don’t want to scroll through 50 pages of results to find your next homework problem.\nSave the notebook as a Jupyter .ipynb file. When you’re ready to submit your homework, upload the notebook to Gradescope.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql.html#sec-example-db",
    "href": "data-engineering/sql.html#sec-example-db",
    "title": "3  SQL Basics",
    "section": "3.2 Our example database",
    "text": "3.2 Our example database\nTo learn how to write queries, we’ll start with a database already filled with example data. This database is for a hypothetical online learning platform where students see material and then take short quizzes. We’ll start by looking at the events table, which records every time a student interacted with some element of the system, such as a quiz item. Table 3.1 shows a few example rows.\n\n\n\nTable 3.1: A few rows from the events table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntime\npersona\nelement\nlatency\nscore\nfeedback\n\n\n\n\n17\n2015-07-11 09:42:11\n3271\n97863\n329.4\n240\nConsider…\n\n\n18\n2015-07-11 09:48:37\n3271\n97864\n411.9\n1000\n\n\n\n19\n2015-07-08 11:22:01\n499\n104749\n678.2\n750\nThe mean is…\n\n\n22\n2015-07-30 08:44:22\n6742\n7623\n599.7\n800\nTry to think of…\n\n\n24\n2015-08-04 23:56:33\n1837\n424933\n421.3\n0\nPlease select…\n\n\n32\n2015-07-11 10:11:07\n499\n97863\n702.1\n820\nWhat does the…\n\n\n99\n2015-07-22 16:11:27\n24\n88213\n443.0\n1000\n\n\n\n\n\n\n\nYou can load this data into your own database. Download events.sql and load it in Azure Data Studio. You will see a file full of SQL queries that create the table. At the top, select your database connection, then press Run to run all the queries and create the table.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql.html#selecting-data",
    "href": "data-engineering/sql.html#selecting-data",
    "title": "3  SQL Basics",
    "section": "3.3 Selecting data",
    "text": "3.3 Selecting data\nThe SELECT command is how we query the database. It is versatile and powerful command.\nThe simplest query is to look at all rows and columns of a table:\nSELECT * FROM events;\nThe * is a shorthand for “all columns.”\nSelects can include expressions, not just column names, as the quantities selected. And we can use as clauses to name (or rename) the results.\nSELECT 1 AS one;\nSELECT persona AS person FROM events;\nMost importantly, we can qualify our queries with conditions that refine the selection. We do this with the WHERE clause, which accepts a logical condition on any expression and selects only those rows that satisfy the condition. The conditional expression can include column names (even temporary ones) as variables.\nSELECT * FROM events WHERE id &gt; 20 AND id &lt; 40;\nWe can also order the output using the ORDER BY clause:\nSELECT score, element FROM events\nWHERE persona = 1150\nORDER BY element, score;\nSpecifying two columns (element, score) means the results will be sorted by the first column, and any ties broken using the second column.\nWe can limit the number of results using LIMIT:\nSELECT score, element FROM events\nWHERE persona = 1150\nORDER BY element, score\nLIMIT 10;\n\nExercise 3.1 (Basic SELECT queries) Craft queries to do the following in the events table:\n\nList all ids, persona, score where a score &gt; 900 occurred.\nList all persona (sorted numerically) who score &gt; 900. Can you eliminate duplicates here? (Hint: Consider SELECT DISTINCT.)\n\n\n\n3.3.1 Expressions and built-in functions\nSQL queries can contain expressions, such as multiplication and addition. For example:\nSELECT 2 * score, element FROM events\nWHERE persona &lt; 1201\nLIMIT 2;\nRelational databases also typically have a library of built-in functions that can do various useful things. The chapter on built-in functions and operators in the PostgreSQL manual lists numerous such functions: mathematical functions, string concatenation and formatting, date and time operations, comparisons, …\nThese functions can be used anywhere you’d expect a value. For example:\nSELECT log(score) AS log_score, element,\n       upper(feedback) AS shouty_feedback\nFROM events\nLIMIT 10;\nYou can think of these functions as being inherently vectorized: given a column of data, they return an entire column of results, much like applying a function to a column of a Pandas data frame.\n\n\n3.3.2 Data types\nEach value in a SQL database has a type controlling the operations you can perform with it: text, numbers, dates, and so on.\nJust as expressions in Python or R can “cast” data from one type to another (like as.character(7) in R, or str(7) in Python), in SQL you can cast to different data types. The CAST() function does this. For example, to convert a string specifying a timestamp to a timestamp object, we can write\nSELECT CAST('2020-10-01 11:00:00' AS TIMESTAMP);\nThe string now has the timestamp type, so functions that work on timestamps (listed in the chapter on date/time functions) can work on it:\nSELECT date_part('dow', CAST('2020-10-01 11:00:00' AS TIMESTAMP));\nThe database will automatically reject operations on the wrong type of data, or that try to insert the wrong type into a column.\nOne special value is NULL. Most data types can be NULL, which represents a missing value, must like NA in R or None in Python. By default, most types allow NULL values, so if you want to enforce that a value must always be provided, you must declare the column to be NOT NULL (see Section 3.7 below).\n\n\n3.3.3 Grouping and aggregate functions\nSometimes we want to calculate something from many rows, not just one row. For example, most databases have count(), max(), min(), and sum() functions. These are all aggregate functions, meaning they take many values and produce a single value.\nFor example, let’s calculate the average score obtained by students:\nSELECT avg(score) FROM events;\nThis produces a single row: the average score. Any aggregate function takes many rows and reduces them to a single row. This is why you can’t write this:\nSELECT persona, avg(score) FROM events;\nTry it; why does Postgres complain?\nWe often want to apply aggregate functions not just to whole columns but to groups of rows within columns. This is the province of the GROUP BY clause. It groups the data according to a specific value, and aggregate functions then produce a single result per group.\nFor example, if I wanted the average score for each separate user, I could write:\nSELECT persona, avg(score) AS mean_score\nFROM events\nGROUP BY persona\nORDER BY mean_score desc;\nYou can apply conditions on the groups. While WHERE filters the individual rows, HAVING filters the groups after aggregation. For example, we can find groups with average score above 50:\nSELECT persona, avg(score) AS mean_score\nFROM events\nWHERE moment &gt; CAST('2014-10-01 11:00:00' AS timestamp)\nGROUP by persona\nHAVING avg(score) &gt; 300\nORDER BY mean_score DESC;\n\nExercise 3.2 (Grouping and aggregating) Using the events table,\n\nList all personas whose maximum latency is greater than 575, and calculate their average scores. Sort the results by increasing average score.\nList all persona whose average score &gt; 600. You will need to do a GROUP BY as above. (Hint: use HAVING instead of WHERE for the aggregate condition.)\nProduce a table showing how many times each instructional element was practiced. The COUNT() aggregate function counts the number of rows in each group.\nList all personas and, as a string, the month and year of their first event (by moment) in the table. (For example, “January 2014”.) Limit this to the first 10 results.\nHint: Check the PostgreSQL documentation on data type formatting to see how to convert a timestamp to a string, particularly tables 9.27 and 9.31.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql.html#inserting-data",
    "href": "data-engineering/sql.html#inserting-data",
    "title": "3  SQL Basics",
    "section": "3.4 Inserting data",
    "text": "3.4 Inserting data\nThe INSERT command specifies data to insert.\nThe basic template is\nINSERT INTO &lt;tablename&gt; (&lt;column1&gt;, ..., &lt;columnk&gt;)\n       VALUES (&lt;value1&gt;, ..., &lt;valuek&gt;);\nIf the column names are excluded, then values for all columns must be provided. You can use DEFAULT in place of a value for a column with a default setting.\nYou can also insert multiple rows at once:\nINSERT INTO &lt;tablename&gt; (&lt;column1&gt;, ..., &lt;columnk&gt;)\n       VALUES (&lt;value11&gt;, ..., &lt;value1k&gt;),\n              (&lt;value21&gt;, ..., &lt;value2k&gt;),\n              ...\n              (&lt;valuem1&gt;, ..., &lt;valuemk&gt;);\nFor example:\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1211, 29353, 824, 'C', 'How do the mean and median differ?');\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1207, 29426, 1000, 'A', 'You got it!');\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1217, 29433,  842, 'C', 'Try simplifying earlier.'),\n              (1199, 29435,    0, 'B', 'Your answer was blank'),\n              (1207, 29413, 1000, 'C', 'You got it!'),\n              (1207, 29359,  200, 'A', 'A square cannot be negative');\nBy default, INSERT does not return a table of results. But you can ask it to:\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1217, 29433,  842, 'C', 'Try simplifying earlier.'),\n              (1199, 29435,    0, 'B', 'Your answer was blank'),\n              (1207, 29413, 1000, 'C', 'You got it!'),\n              (1207, 29359,  200, 'A', 'A square cannot be negative')\nRETURNING id;\nThis query returns a table with 4 rows and 1 column: the id column that is automatically filled in (because it is SERIAL). You can return one or more columns or expressions in RETURNING: it takes a list of columns just like SELECT.\n\nExercise 3.3 (Inserting invalid data) Write a query to insert data into the events table, but provide the wrong data type for one column (such as a string instead of an integer). Report the message you receive. Which of the ACID guarantees (Section 2.4) is the database enforcing here?",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql.html#updating-data",
    "href": "data-engineering/sql.html#updating-data",
    "title": "3  SQL Basics",
    "section": "3.5 Updating data",
    "text": "3.5 Updating data\nThe UPDATE command allows us to modify existing entries in any way we like. The basic syntax looks like this:\nUPDATE table\n    SET col1 = expression1,\n        col2 = expression2,\n        ...\n    WHERE condition;\nBeware: If you omit the WHERE clause, the update will be applied to every row.1\nThe UPDATE command can update one or more columns at once.\nAs with INSERT, you can use RETURNING with UPDATE queries. Here it returns one row per row that was modified, and you can choose the columns returned.\n\nExercise 3.4 (Updating data) In the events table,\n\nSet the answer for events with id &gt; 800 to the letter C.\nUpdate the scores to subtract 50 points for every hint taken (in the hints column). Ensure that the score stays above 0. Use RETURNING to get the id and the new score for each updated event.\nHint: The GREATEST() function returns the maximum of its arguments, so GREATEST(-5, 0, -2) is 0.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql.html#deleting-data",
    "href": "data-engineering/sql.html#deleting-data",
    "title": "3  SQL Basics",
    "section": "3.6 Deleting data",
    "text": "3.6 Deleting data\nThe DELETE command allows you to remove rows from a table that satisfy a condition. The basic syntax is:\nDELETE FROM table WHERE condition;\nBeware: If you omit the WHERE clause, all rows in the table will be deleted immediately.\nAs before, you can use RETURNING to get results from DELETE. There will be one row per deleted row.\n\nExercise 3.5 (Gems data) The file gems.sql contains the commands needed to create a table called gems filled with some random data. Connect to your personal database. Open the file and run the queries inside to create the table.\nTry\nSELECT * FROM gems LIMIT 10;\nto see a few examples of the data in the new gems table.\nNow try writing some queries:\n\nSet the label to 'thin' for all gems with fewer than 10 facets.\nSet the label to 'wow' and the price to $100 for all gems with more than 20 facets.\n(The price column is defined to be of type money. To convert a number to money, you can write cast('100.0' as money). The money type is useful because it represents a number with fixed decimal precision; by default, this is two decimal digits. It stores these exactly, unlike floating-point numbers, which can’t represent some simple decimals.)\nDelete all gems with fewer than four facets.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql.html#sec-creating-schemas",
    "href": "data-engineering/sql.html#sec-creating-schemas",
    "title": "3  SQL Basics",
    "section": "3.7 Creating and updating table schemas",
    "text": "3.7 Creating and updating table schemas\n\n3.7.1 Selecting the schema\nBefore making a table, we must choose what columns it should have, what they will be named, and what types they have.\nWe’ll discuss the selection of columns in more detail in Section 4.1. Broadly, we choose the columns to describe whatever entity is in the table: if each row is a student, the columns should be features of the students. Their names should be reasonably descriptive without being so long as to make typing queries a pain.\nTypes require careful choice. PostgreSQL supports dozens of types. The most common are:\n\nBooleans: boolean can be either true or false (or yes and no, on or off, or 1 and 0)\nNumeric\n\ninteger for integers\nnumeric for arbitrary-precision numbers (i.e. to any number of decimal places – making them slow to manipulate)\ndouble precision (or just double) for 64-bit floating point numbers\nserial for an integer column whose default value is always the last value plus 1 (great for IDs)\nmoney for monetary amounts with fixed precision\n\nStrings\n\ntext is the general-purpose text type, for strings up to any length. This should be your default choice in PostgreSQL. (However, the SQL standard does not include text, so some databases don’t have it, forcing everyone to use varchar.)\nchar(n) gives a fixed-length string of a certain length, padded with spaces, so char(10) always is 10 characters long\nvarchar(n) gives variable-length strings up to a certain length, so varchar(10) can be any length up to 10 characters.\n\nDates and times\n\ntimestamp records a specific point in time with date and time. Optionally, timestamp with time zone records the time zone associated with the event.\ndate gives a date with no time of day\ntime gives a time of day (from midnight to midnight), but no date, optionally with a time zone.\ninterval represents the interval between two timestamps, and is what you get when you subtract two timestamps\n\nEnumerated types are user-created. Each value takes one of several defined options, much like a factor in R.\n\nThere are many other types and options; view the full Postgres list for more.\n\n\n3.7.2 Creating tables\nTo create a new table, we use the CREATE TABLE command. In its most basic form, it looks like\nCREATE TABLE name (attribute1 TYPE1, attribute2 TYPE2, ...);\nA simple products table specifying products for sale is:\nCREATE TABLE products (\n       product_id INTEGER,\n       name TEXT,\n       price DOUBLE,\n       sale_price DOUBLE\n);\nHere we have chosen five columns and specified their types, out of the list of types supported by PostgreSQL.\nHere’s a fancier version:\nCREATE TABLE products (\n       product_id SERIAL PRIMARY KEY,\n       name TEXT,\n       price NUMERIC CHECK (price &gt; 0),\n       sale_price NUMERIC CHECK (sale_price &gt; 0),\n       CHECK (price &gt; sale_price)\n);\nThe SERIAL type specifies that product_id is automatically set to an increasing integer whenever we add a row. We have also specified it is the primary key, meaning it uniquely identifies rows in this table. We’ve also set constraints on price, sale price, and their relationship.\nPostgreSQL will reject any data of the wrong type or violating the constraints:\nINSERT INTO products (name, price, sale_price)\nVALUES ('duck', 'duck', 'goose');\n\nINSERT INTO products (name, price, sale_price)\nVALUES ('kirk action figure', 50, 52);\nWe can also specify default values using DEFAULT and require entries to not be null. Here’s an even fancier products table:\nCREATE TABLE products (\n       product_id SERIAL PRIMARY KEY,\n       name TEXT NOT NULL,\n       quantity INTEGER DEFAULT 0,\n       price NUMERIC CHECK (price &gt; 0),\n       sale_price NUMERIC CHECK (sale_price &gt; 0),\n       CHECK (price &gt; sale_price)\n);\n\nExercise 3.6 (Creating a table) Write the CREATE TABLE command for a table called students containing the following columns:\n\nStudent ID, a unique integer\nStudent name, which cannot be null\nDate of birth (as a date without a time), which must be before the present\nA boolean flag for whether each student has completed orientation, false by default\nThe tuition charged for that student, which must be greater than 0\nThe scholarship amount that student receives, which must be less than the tuition amount, and is 0 by default\n\nYou may choose appropriate column names, and should choose appropriate data types from the many types supported by PostgreSQL. Chapter 5 of the manual gives great detail on defining tables, constraints, defaults, and other features.\nWrite an INSERT query that adds four example students to the table. Write another INSERT query that attempts to insert invalid data, proving that your definition enforces the criteria above.\n\n\n\n3.7.3 Altering tables\nThe ALTER TABLE command allows you to change a variety of table features. This includes adding and removing columns, renaming attributes, changing constraints or attribute types, and setting column defaults. See the full documentation for more.\nA few examples using the most recent definition of products:\n\nLet’s rename product_id to just id for simplicity.\nALTER TABLE products\n      RENAME product_id TO id;\nLet’s add a brand_name column.\nALTER TABLE products ADD brand_name TEXT DEFAULT 'generic' NOT NULL;\nLet’s drop the discount column\nALTER TABLE products DROP discount;\nLet’s set a default value for brand_name.\nALTER TABLE products\n      ALTER brand_name SET DEFAULT 'generic';\n\n\n\n3.7.4 Deleting tables\nThe command is DROP TABLE.\nDROP TABLE products;\nTry it, then type \\d at the prompt.\nBeware: This command is immediate and permanent (unless you have a backup).",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql.html#sec-joins-and-foreign-keys",
    "href": "data-engineering/sql.html#sec-joins-and-foreign-keys",
    "title": "3  SQL Basics",
    "section": "3.8 Joins and foreign keys",
    "text": "3.8 Joins and foreign keys\nAs we will see shortly, principles of good database design tell us that tables represent distinct entities with a single authoritative copy of relevant data. This is the DRY principle (Don’t Repeat Yourself) in action, in this case eliminating data redundancy.\nAn example of this in the events table are the persona and element columns, which point to information about students and components of the learning environment. We do not repeat the student’s information each time we refer to that student. Instead, we use a link to the student that points into a separate personae table. Table 3.2 shows a few rows of this table.\n\n\n\nTable 3.2: The first few rows of the personae table.\n\n\n\n\n\nid\nfirstname\nlastname\nbirthdate\naccount_balance\n\n\n\n\n1\nArnold\nPerlstein\n1988-06-02\n$1700.02\n\n\n2\nCarlos\nRamon\n1988-04-27\n$0.00\n\n\n3\nDorothy\nHudson\n1989-01-06\n$-406.79\n\n\n4\nKeesha\nFranklin\n1988-03-15\n$0.00\n\n\n\n\n\n\nBut if our databases are to stay DRY in this way, we need two things:\n\nA way to define links between tables (and thus define relationships between the corresponding entities), and to enforce that these links are valid.\nAn efficient way to combine information across these links.\n\nThe former is supplied by foreign keys and the latter by joins. We will tackle both in turn.\n\n3.8.1 Foreign keys\nA foreign key is a field (or collection of fields) in one table that uniquely specifies a row in another table. We specify foreign keys in PostgreSQL using the REFERENCES keyword when we define a column or table. A foreign key that references another table must be the value of a unique key in that table, though it is most common to reference a primary key.\nFor example, the events table was defined using REFERENCES:\nCREATE TABLE events (\n    id SERIAL PRIMARY KEY,\n    moment TIMESTAMP,\n    persona INTEGER REFERENCES personae (id),\n    -- more columns go here:\n    ...\n);\nForeign keys can also be added (and altered) as table constraints that look like FOREIGN KEY (&lt;key&gt;) REFERENCES &lt;table&gt;.\nNow try this\nINSERT INTO events (persona, element, score)\nVALUES (-1, 123, 1000);\nNotice that the insertion did not work—-and the entire transaction was rolled back—because the foreign key constraint was violated. There was no persona with ID -1.\nSo let’s fix it. Try it!\nINSERT INTO personae VALUES (-1, 'Englebert', 'Humperdinck', '1936-05-02', 100.0);\n\nINSERT INTO events (persona, element, score)\nVALUES (-1, 123, 1000);\nA foreign key is allowed to be NULL, meaning it points to nothing, unless you include a NOT NULL constraint in the column definition.\n\n\n3.8.2 Joins\nSuppose we want to display features of an event with the name and course of the student who generated it. If we’ve kept to DRY design and used a foreign key for the persona column, this seems inconvenient.\nThat is the purpose of a join. For instance, we can write:\nSELECT personae.lastname, personae.firstname, events.score, events.moment\nFROM events\nJOIN personae ON events.persona = personae.id\nWHERE moment &gt; CAST('2015-03-26 08:00:00' AS timestamp)\nORDER BY moment;\nJoins incorporate additional tables into a select. This is done by appending to the from clause:\nFROM &lt;table&gt; JOIN &lt;table&gt; ON &lt;condition&gt; ...\nwhere the on condition specifies which rows of the different tables are included. And within the select, we can disambiguate columns by referring them to by &lt;table&gt;.&lt;column&gt;. Look at the example above with this in mind.\nYou can think of FROM &lt;table&gt; JOIN &lt;table&gt; as producing a new virtual table formed by the two joined together, and the SELECT operates on this. You can join this virtual table to others: FROM &lt;table&gt; JOIN &lt;table&gt; ON &lt;condition&gt; JOIN &lt;table&gt; and so on.\nAfter joining two or more tables, you can operate on the resulting table just like in any other queries, with grouping, aggregates, and so on.\nWe will start by seeing what joins mean in a simple case, using these tables:\nCREATE TABLE a (id SERIAL PRIMARY KEY, name TEXT);\nINSERT INTO a (name)\n       VALUES ('Pirate'),\n              ('Monkey'),\n              ('Ninja'),\n              ('Flying Spaghetti Monster');\n\nCREATE TABLE b (id SERIAL PRIMARY KEY, name TEXT);\nINSERT INTO b (name)\n       VALUES ('Rutabaga'),\n              ('Pirate'),\n              ('Darth Vader'),\n              ('Ninja');\nLet’s look at several kinds of joins. (There are others, but this will get across the most common types.)\n\n3.8.2.1 Inner join\nAn inner join produces the rows for which attributes in both tables match. (If you just say JOIN in SQL, you get an inner join; the word INNER is optional.)\nSELECT * FROM a INNER JOIN b on a.name = b.name;\n\n\n3.8.2.2 Full outer join\nA full outer join produces the full set of rows in all tables, matching where possible but NULL otherwise.\nSELECT * FROM a FULL OUTER JOIN b on a.name = b.name;\n\n\n3.8.2.3 Left outer join\nA left outer join produces all the rows from A, the table on the “left” side of the join operator, along with matching rows from B if available, or null otherwise. (LEFT JOIN is a shorthand for LEFT OUTER JOIN in PostgreSQL.)\nSELECT * FROM a LEFT OUTER JOIN b ON a.name = b.name;\n\nExercise 3.7 (Set operations) Write queries to:\n\nSelect all the rows of A that are not in B.\nSelect the rows of A not in B and the rows of B not in A.\n\nHint: To check if a value is null, use value IS NULL.\n\n\nExercise 3.8 (Joining educational data) Besides the events and personae tables, our database also contains an elements table providing information about each instructional element (such as a quiz question) within the system. Here is its schema:\nCREATE TABLE elements (\n       id SERIAL PRIMARY KEY,\n       content TEXT,\n       points_max INTEGER CHECK (points_max &gt;= 0),\n       module INTEGER, -- references a modules table, if we had one\n       hint TEXT,\n       max_tries INTEGER CHECK (max_tries &gt; 0)\n);\nWrite queries to:\n\nList all events, with the score they received, the name of the student completing the event, their birthdate, and the maximum number of points possible.\nThe same, but only for events where the student’s score is higher than the number of points possible.\nList all elements IDs that have never been part of an event.\nCalculate the average score for all events for each element.\nSum the scores for all students born after June 1, 1988.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql.html#indices",
    "href": "data-engineering/sql.html#indices",
    "title": "3  SQL Basics",
    "section": "3.9 Indices",
    "text": "3.9 Indices\nWhen we know we will search on certain fields regularly, it can be helpful to create an index, which speeds up those particular searches. An index tells PostgreSQL to keep a data structure (typically a tree) for a particular column so that it can identify all rows matching a query without scanning the entire table.\nFor example, PostgreSQL automatically indexes any column declared as a PRIMARY KEY. In the events table, the id column is the primary key, so a query like\nSELECT * FROM events WHERE id &gt; 200 AND id &lt; 300;\nwill use the index. PostgreSQL will search the tree (in something like \\(O(\\log\nn)\\) time, where \\(n\\) is the number of rows) and pull the matching rows of the table, rather than looping over the entire table and checking every row.\nWhile PostgreSQL automatically indexes primary keys, we can also tell it explicitly to add indexes for specific columns. For example, if we know we’ll often be searching events by start time, we could do\nCREATE INDEX event_moment_index\nON events (moment);\nEach index has a name so that we can identify it when we use DROP INDEX to remove it. Postgres also uses these names if we ask it to explain how it will conduct a query. By default, this index is based on a B-tree, but there are other index types that are useful for other things. Hash indices, for instance, are very fast for equality (SELECT * FROM foo WHERE col == 'something'). See the indexes chapter of the manual for more information on the syntax and type of indexes.\nIndices are particularly important for tables with many thousands or millions of rows. Companies often have tables with gigabytes of data, and looping through every row to find matching results would be prohibitively slow. Worse, joining two large tables requires matching up rows from the two tables, and would be even slower. A clever selection of indices can make these operations much faster.\nMuch of the engineering work in database software is in the so-called query planner, which takes your query, information about the size of each table, and the available indices to determine the most efficient way to execute the query. (Should it filter the data first, then join? Or join and then filter? Which of several indices should it use to filter first, so it has less work to do for subsequent operations?)\nIt is possible to develop an entire career around building database tables, specifying their indices, and writing queries that both solve business questions and are extremely fast for enormous databases.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql.html#footnotes",
    "href": "data-engineering/sql.html#footnotes",
    "title": "3  SQL Basics",
    "section": "",
    "text": "I once accidentally omitted the WHERE clause on an UPDATE query to change a user’s password in a database, and hence changed everyone’s password. On a live website with thousands of users. I had to very frantically get the previous day’s backup and figure out how to reset passwords to those contained in the backup.↩︎",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data-engineering/advanced-sql.html",
    "href": "data-engineering/advanced-sql.html",
    "title": "4  Advanced SQL",
    "section": "",
    "text": "4.1 Schema design principles\nThe key design principle for database schema is to keep the design DRY – that is, eliminate data redundancy. The process of making a design DRY is called normalization, and a DRY database is said to be in “normal form.”\nThe basic modeling process:",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced SQL</span>"
    ]
  },
  {
    "objectID": "data-engineering/advanced-sql.html#sec-schema-design",
    "href": "data-engineering/advanced-sql.html#sec-schema-design",
    "title": "4  Advanced SQL",
    "section": "",
    "text": "Identify and model the entities in your problem\nModel the relationships between entities\nInclude relevant attributes\nNormalize by the steps below\n\n\nExample 4.1 (Managing songs) Consider a database to manage songs:\n\n\n\nAlbum\nArtist\nLabel\nSongs\n\n\n\n\nTalking Book\nStevie Wonder\nMotown\nYou are the sunshine of my life, Maybe your baby, Superstition, …\n\n\nMiles Smiles\nMiles Davis Quintet\nColumbia\nOrbits, Circle, …\n\n\nSpeak No Evil\nWayne Shorter\nBlue Note\nWitch Hunt, Fee-Fi-Fo-Fum, …\n\n\nHeadhunters\nHerbie Hancock\nColumbia\nChameleon, Watermelon Man, …\n\n\nMaiden Voyage\nHerbie Hancock\nBlue Note\nMaiden Voyage\n\n\nAmerican Fool\nJohn Couger\nRiva\nHurts so good, Jack & Diane, …\n\n\n\nThis seems fine at first, but why might this format be problematic or inconvenient?\n\nIt’s difficult to get individual songs from a long list in one column\nIf we want to store additional information about each artist or label, where do we put it?\nArtists often have “Best Of” albums that repeat many songs from previous albums; if we store information about each song, does it get duplicated?\nA few thoughts:\n\nWhat happens if an artist changes names partway through his or her career (e.g., John Cougar)?\nSuppose we want mis-spelled “Herbie Hancock” and wanted to update it. We would have to change every row corresponding to a Herbie Hancock album.\nSuppose we want to search for albums with a particular song; we have to search specially within the list for each album.\n\n\nLet’s write this schema as Album (artist, name, record_label, song_list), where Album is the entity and the labels in parentheses are its attributes. To normalize this design, we will add new entities and define their attributes so each piece of data has a single authoritative copy.\n\n\n4.1.1 Give entities unique identifiers\nFirst, we give entities unique identifiers. This will be their primary key.\nKey features of a primary key are that it is unique, non-null, and it never changes for the lifetime of the entity.\nSome entities already have unique identifiers attached to them – but you should consider whether those identifiers can ever change. Social Security numbers are often used to uniquely identify people, but they can be changed in rare circumstances; books can be uniquely identified by ISBNs, but these change for new editions or formats; and sometimes supposedly unique numbers get accidentally reused by other people you can’t control.\n\n\n4.1.2 Give each attribute a single value\nWhat does each attribute describe? What attributes are repeated in Albums, either implicitly or explicitly?\nConsider the relationship between albums and songs. An album can have one or more songs; in other words, the attribute song_list is non-atomic (it is composed of other types, in this case a list of text strings). The attribute describes a collection of another entity – Song.\nSo, we now have two entities, Album and Song. How do we express these entities in our design? It depends on our model. Let’s look at two ways this could play out.\n\nAssume (at least hypothetically) that each song can only appear on one album. Then Album and Song would have a one-to-many relationship.\n\nAlbum(id, title, label, artist)\nSong(id, name, duration, album_id)\n\nQuestion: What do our CREATE TABLE commands look like under this model?\nAlternatively, suppose our model recognizes that while an album can have one or more songs, a song can also appear on one or more albums (e.g., a greatest hits album). Then, these two entities have a many-to-many relationship.\nThis gives us two entities that look like:\n\nAlbum(id, title, label, artist)\nSong(id, name, duration)\n\nThis is fine, but it doesn’t seem to capture that many-to-many relationship. How should we capture that?\n\nAn answer: This model actually describes a new entity – Track. The schema looks like:\n\nAlbum(id, title, label, artist)\nSong(id, name, duration)\nTrack(id, song_id, album_id, index)\n\n\n\n\n\n4.1.3 Make non-key attributes depend only on the primary key\nThis step is satisfied if each non-key column in the table serves to describe what the primary key identifies.\nAny attributes that do not satisfy this condition should be moved to another table.\nIn our schema of the last step (and in the example table), both the artist and label field contain data that describes something else. We should move these to new tables, which leads to two new entities:\n\nArtist(id, name)\nRecordLabel(id, name, street_address, city, state_name, state_abbrev, zip)\n\nEach of these may have additional attributes. For instance, producer in the latter case, and in the former, we may have additional entities describing members in the band.\nWe could then update our Album schema to use these as foreign keys: Album(id, title, label_id, artist_id).\n\n\n4.1.4 Make non-key attributes independent of each other\nConsider RecordLabel. The state_name, state_abbrev, and zip code are all non-key fields that depend on each other. (If you know the zip code, you know the state name and thus the abbreviation.)\nThis suggests to another entity State, with name and abbreviation as attributes. And so on.\n\n\n4.1.5 Use your judgment to decide when to stop\nIf we take this process to the extreme, we could end up with dozens of tables, each describing a unique entity and linked to dozens of other tables. Indeed, if you look up “database normalization”, you will find there are extensive and detailed definitions of different degrees of normalization and the properties required of tables in each.\nWe can exercise our judgment to determine the amount of normalization required; real-world applications rarely have perfectly normalized tables. For example, if you’re building a database of songs for your own use to track your record collection, you probably don’t care too much that RecordLabel has both a state_name and a state_abbrev; you don’t care too much about the label, and for the queries you’re doing, it’s not important that these could be moved to their own State table. It’s convenient to just have them in RecordLabel and be able to get them without a join.\nBut if you were building an authoritative song database for a music streaming service, you may need your database schema to be much more robust and detailed: you need to track enough information to know where to send royalty checks, you have millions of songs, you deal with thousands of record labels with similar-sounding names that change owners and locations, and the cost of messing up is high; so you’d like your database to be as accurate and consistent as possible at all times. Structuring the database in normal form lets the RDB software help you ensure it is consistent.\n\nExercise 4.1 (Schema for songs) Based on the discussion above, write a series of CREATE TABLE commands to create a set of tables for tracking songs. Your schema should support songs that appear on multiple albums, basic information about artists (such as their name and year of birth), information about record labels, song titles and duration, and other things from the discussion above.\n\n\nExercise 4.2 (Schema for sports) Pick your favorite sport (or, at least, a sport you know something about). Consider designing a database to track players, teams, and games. Write a schema (as a series of CREATE TABLE statements) to store information about them. Your schema should support queries that:\n\nIdentify which players were on a team at a particular time (as players can move between teams)\nIdentify which team won a particular game, and where that game was played\nFind all the games a particular player was involved in. (You don’t have to track if they sat out particular games, just which games their team played in.)\nFind all games played in a particular place.\n\nYou can support additional query types, but start with a schema that can handle these.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced SQL</span>"
    ]
  },
  {
    "objectID": "data-engineering/advanced-sql.html#sec-transactions",
    "href": "data-engineering/advanced-sql.html#sec-transactions",
    "title": "4  Advanced SQL",
    "section": "4.2 Transactions",
    "text": "4.2 Transactions\nTransactions are the fundamental mechanism relational databases use to manage both atomicity and concurrency. How do we ensure multiple queries run atomically (either all succeed or all fail), and how do we allow multiple people to be changing the database at the same time? To see the problem, consider an example.\n\nExample 4.2 (The need for transactions) Suppose we’re working with an R data frame, except it is shared: multiple people can edit it at once, and their edits instantaneously change the data frame. (There’s no easy way to do this in base R; we’re just using R syntax to illustrate the problem a SQL database faces.)\nUser 1 and user 2 are both working with the bank_accounts data frame, which stores bank balances and information. User 1 needs to record that account 777 transferred $500 to account 778; user 2 needs to record that account 777 withdrew $50 in cash.\nEach column shows one user’s operations; each line is one time point, so two side-by-side lines occur at the same time:\n\n\n\n# User 1:\n\na777 &lt;- bank_accounts |&gt;\n  filter(account_id = 777)\nstart &lt;- a777$balance\n\na778 &lt;- bank_accounts |&gt;\n  filter(account_id = 778)\n\na777$balance &lt;- start - 500\na778$balance &lt;- a778$balance + 500\n\n\n# User 2:\n\na777 &lt;- bank_accounts |&gt;\n  filter(account_id = 777)\namount &lt;- a777$balance\n\na777$balance &lt;- amount - 50\n\n\n\nSuppose both accounts start with $1000. The desired outcome is that account 777 ends with $450 ($500 sent to 778 and $50 withdrawn) and 778 ends with $1500. But instead:\n\nUser 1 stores $1000 in start.\nUser 2 stores $1000 in amount.\nUser 2 updates account 777’s balance to be amount - 50, or $950.\nUser 1 updates account 777’s balance to be start - 500, or $500.\nUser 1 updates account 778’s balance to be $500 higher, or $1500.\n\nAt the end, 777’s balance is $500, not $450! All because of the timing of the operations. If user 1’s work was guaranteed to happen first, without user 2 interfering, this wouldn’t happen.\n\nPostgreSQL uses Multiversion Concurrency Control, which means that each database user sees a snapshot of the database as it was at some specific point in time. If someone else is changing the database at the same time, this does not affect queries that are currently running, so the data they see remains self-consistent.\nThe basic mechanism is straightforward, though the implementation is complicated. Queries are grouped into transactions using BEGIN and COMMIT statements. The queries within a transaction do not update the data seen by other users until COMMIT “commits” the changes. An error in any query in the transaction halts the entire transaction and “rolls back” its changes, so it is as if it never happened.\n\nExample 4.3 (Concurrency with transactions) Suppose we’re implementing Example 4.2 using PostgreSQL and its default concurrency settings. Each user submits a transaction:\n-- User 1:\nBEGIN;\n\nUPDATE bank_accounts\nSET balance = balance - 500::money\nWHERE account_id = 777;\n\nUPDATE bank_accounts\nSET balance = balance + 500::money\nWHERE account_id = 778;\n\nCOMMIT;\n\n-- User 2\nBEGIN;\n\nUPDATE bank_accounts\nSET balance = balance - 50::money\nWHERE account_id = 777;\n\nCOMMIT;\nInternally, PostgreSQL must do the same work as in Example 4.2—but it prevents any problems from occurring:\n\nUser 1 begins by changing account 777. This locks the row, preventing other changes until user 1’s transaction is complete.\nUser 2 then attempts to change it. The UPDATE query must wait for the lock to be released, so their query is stalled.\nUser 1 commits their transaction with the updated balances, releasing the lock on account 777.\nUser 2’s query can now go ahead, subtracting $50 from the account’s balance.\n\nThis avoids the problem entirely, at the cost of sometimes requiring queries to wait for others to complete.\n\nPostgreSQL’s locking system automatically locks individual rows or entire tables when UPDATE, INSERT, or other commands make it necessary. The locks are released when the transaction is committed, allowing other changes to be made to the same row or table. SELECT does not lock anything, but PostgreSQL does ensure that a SELECT only sees the version of the database as it was when the query began: if a SELECT involves complicated joins and aggregations that take some time to execute, and the database is changed by another user while those happen, the SELECT’s results will not be affected by those changes. This prevents strange inconsistencies where rows change while they’re being sorted and joined.\nTransactions also provide the atomicity required by the ACID guarantees: PostgreSQL ensures that either all queries in a transaction succeed or none of them do.\nFor example, suppose a transaction involves 10 queries doing various updates, but the 6th query causes an error. The entire transaction will be aborted and PostgreSQL will refuse to execute any further queries until you send a ROLLBACK command, which reverts the database back to its state before the transaction started.\nIn Example 4.3, atomicity is important for user 1: if an error occurs between the two UPDATE queries, we would have subtracted $500 from one account but not added it to the other, and the $500 would be lost. Grouping the queries into a transaction makes this impossible.\nPostgreSQL also supports “savepoints”, which are like checkpoints inside a transaction. (Or quicksaves, if you play video games.) Within one transaction, we can make multiple savepoints, and revert back to a specific changepoint instead of rolling back the entire transaction. Again, the changes only become visible to other users when the entire transaction is committed, but this gives us flexibility in dealing with errors and selectively undoing changes. See the SAVEPOINT reference for examples.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced SQL</span>"
    ]
  },
  {
    "objectID": "data-engineering/advanced-sql.html#subqueries",
    "href": "data-engineering/advanced-sql.html#subqueries",
    "title": "4  Advanced SQL",
    "section": "4.3 Subqueries",
    "text": "4.3 Subqueries\nSQL allows subqueries, where part of a query is itself defined by another query.\nThe simplest case is in FROM clauses. Normally a FROM clause specifies a table, or multiple tables combined with joins. But we can also select from the table of results of another query. Here’s a trivial example using the tables defined in Section 3.2:\nSELECT * FROM\n(SELECT moment, persona, score\n FROM events) AS e\nINNER JOIN\n(SELECT id, firstname, lastname\n FROM personae) AS p\nON e.persona = p.id;\nThe subqueries are given in parentheses. Note they can’t refer to things outside of the subqueries; essentially, Postgres runs the subqueries first, gets the tables of results, and then uses them like it would use any other table in the outer query.\nThis query, of course, could be written easily without the subequeries. But often in complicated queries with WHERE, GROUP BY, and HAVING clauses, it can be easier to understand when written with subqueries than when written as one large query.\nYou can also use subqueries within a WHERE clause. These are called subquery expressions.\nSELECT moment, persona, score FROM events\nWHERE persona IN (SELECT id FROM personae\n                  WHERE account_balance &lt; 0::money);\nHere the subquery returns exactly one column (id), and so IN checks if each event’s persona is contained in that column. This query could be written equivalently as a join, but sometimes it’s easier to work with subqueries to put together a larger query out of pieces, instead of writing one large query to do everything.\nThere are various other functions/operators that can be used on subqueries as well, such as IN, NOT IN, EXISTS, ANY, ALL, and SOME.\nFor example, this looks like an inner join:\nSELECT moment, persona, score FROM events\nWHERE EXISTS (SELECT 1 FROM personae WHERE id = events.persona);\nHere EXISTS is an operator that looks for there to be at least one row in the subquery. The subquery does refer to the outer tables and variables, so you can think of this subquery as being run once per row in events, to determine if there is a matching persona. If the subquery does not return any rows, that event is not included in the final results.\n\nExercise 4.3 (Translating subqueries) Translate the following queries to JOIN queries without any subqueries.\n\n   SELECT moment, persona, score FROM events\n   WHERE persona IN (SELECT id FROM personae\n                     WHERE account_balance &lt; 0::money);\n   SELECT moment, persona, score FROM events\n   WHERE EXISTS (SELECT 1 FROM personae\n                 WHERE id = events.persona);",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced SQL</span>"
    ]
  },
  {
    "objectID": "data-engineering/advanced-sql.html#more-advanced-grouping",
    "href": "data-engineering/advanced-sql.html#more-advanced-grouping",
    "title": "4  Advanced SQL",
    "section": "4.4 More advanced grouping",
    "text": "4.4 More advanced grouping\nSection 3.3.3 introduced aggregate functions and the GROUP BY clause, which allowed us to split data into groups and summarize the groups with aggregates. GROUP BY is fairly restrictive, however: we can only split the data into disjoint groups by unique values of a column, or unique combinations of several columns.\nSometimes we want grouping and aggregation over overlapping groups. For example, if I have data on students at CMU, I might want averages by their department, by college, and overall averages for the university. With GROUP BY that requires running three different queries.\nInstead, we can provide grouping sets, where we provide multiple sets of variables to group by and aggregate over. For example:\nSELECT element, persona, AVG(score), COUNT(*)\nFROM events\nGROUP BY GROUPING SETS ((element), (persona), ());\nThis query asks Postgres to group by three different sets of grouping variables: element, persona, and (), which represents no grouping (aggregate all the data). The result looks like this:\n element | persona |         avg          | count\n---------+---------+----------------------+-------\n         |         | 507.6059701492537313 |  1005\n   29409 |         | 335.8750000000000000 |     8\n   29406 |         | 588.1764705882352941 |    17\n   29408 |         | 572.2666666666666667 |    15\n   29395 |         | 623.5454545454545455 |    11\n   29430 |         | 588.0000000000000000 |    10\n[...]\n         |    1162 | 567.5384615384615385 |    13\n         |    1233 | 437.6666666666666667 |     3\n         |    1157 | 333.5000000000000000 |    12\n         |    1247 | 508.4117647058823529 |    17\n         |    1189 | 546.6666666666666667 |     6\n[...]\nThe blanks here represent NULLs. The first row is the overall average score of all events. The next rows are averages for specific elements, followed by averages for specific personas. Rows grouped by element have a NULL for persona and vice versa.\nNotice the SELECT could list both the element and persona columns. If we simply did GROUP BY element, we couldn’t list persona in the SELECT column list because it’s not the grouping column or an aggregate—but here, Postgres is smart enough to understand that it’s in some of the grouping sets, so it will fill it in for those rows and leave it NULL otherwise.\nThere are other shortcuts. Often we do hierarchical aggregates, like the CMU example above. If we had a students table with columns for department and college, we could write\nSELECT department, college, COUNT(*)\nFROM students\nGROUP BY ROLLUP (college, department);\nThis will first group by (college, department), producing one row per department in each college, then group by college, then group by (), producing (hypothetical) results like\n college | department | count\n---------+------------+-------\n         |            | 16335\nDietrich | Statistics |   632\nDietrich |    History |   127\n[...]\nDietrich |            |  2310\n     MCS |            |  6847\n[...]\nFinally, we can group by every combination of a set of variables. GROUP BY CUBE (a, b, c) will group by (a, b, c); then by (a, b), (a, c), and (b, c); then by a, and so on, all the way up to ().",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced SQL</span>"
    ]
  },
  {
    "objectID": "data-engineering/advanced-sql.html#sec-sql-window",
    "href": "data-engineering/advanced-sql.html#sec-sql-window",
    "title": "4  Advanced SQL",
    "section": "4.5 Window functions",
    "text": "4.5 Window functions\nIn Section 3.3.3, we saw how GROUP BY and aggregate functions allowed us to calculate aggregate values across many rows. We could, for instance, get the average score per student, or the maximum latency per student, or the earliest event, or…\nIn each of those uses, we were limited to getting one row per group. We had to write our SELECT clauses to ensure each column we requested had one result per group.\nBut sometimes I want to get individual rows of data, with additional information attached about the group. For example, using the events table from Section 3.2, how could we get the score for every individual event, compared to that student’s average score for all events?\nWe could do this with a clever subquery (Exercise 4.4), but an alternative is a window function. While an aggregate function takes a group and produces a single row for that group, a window function can calculate an aggregate for every row.\nFor example, let’s get each event’s score, and the z-score of that event relative to the student’s average:\nSELECT\n  id, persona, element, score,\n  ((score - avg(score) OVER (PARTITION BY persona)) /\n   stddev_samp(score) OVER (PARTITION BY persona)) AS z_score\nFROM events\nLIMIT 10;\navg() and stddev_samp() are aggregate functions, but OVER turns them into a window function. The OVER clause tells Postgres which rows to calculate the aggregate for. Here we’ve told it to PARTITION BY persona, so each row gets the average and standard deviation calculated from rows with the same persona.\nWindow functions don’t have to operate on the entire partition. For instance, if we specify ORDER BY in the OVER clause, the window is (by default) the first row in the partition up to the current row, but not the following rows. For instance, this query gets the z score relative to all previous events by the user, but not future ones:\nSELECT\n  id, persona, element, score, moment,\n  ((score - avg(score) OVER (PARTITION BY persona ORDER BY moment)) /\n   stddev_samp(score) OVER (PARTITION BY persona ORDER BY moment)) AS z_score\nFROM events\nLIMIT 10;\nInstead of repeating the PARTITION BY clause, we can name a window:\nSELECT\n  id, persona, element, score, moment,\n  ((score - avg(score) OVER p) /\n   stddev_samp(score) OVER p) AS z_score\nFROM events\nWINDOW p AS (PARTITION BY persona ORDER BY moment)\nLIMIT 10;\nNaturally, there are many additional options to customize this; you could have the window be the following rows, exclude the current row (like a studentized z score), be only some of the rows in the partition (by including a FILTER before OVER), or many other complicated variations.\nWindow functions can only be used in the SELECT column list and in ORDER BY, but not in GROUP BY, HAVING, or WHERE clauses, because they’re calculated after these are run.\nThere are also specific window functions that are not aggregate functions. For example, rank() gives the rank of each row within the partition.\nSee the window function tutorial and reference manual page for details.\n\nExercise 4.4 (Writing window functions with a subquery) Write a query to get the z-score for every event score, relative to the student’s overall average score. Write this using a join and a subquery, rather than by using a window function.\nThe result should have the same columns as the first version in the window function example above.\nNote: Once you write the query, it’s worth thinking about how much more difficult it would be to do the second window function example above, where we included ORDER BY. The simple aggregation approach that worked here would not work for that query.\n\n\nExercise 4.5 (Window functions) Write each of the following queries using window functions.\n\nFor each event, report the difference between the student’s score and the average score of all students who did that element. (element identifies the specific quiz question or instructional item being scored.) Report columns for the student ID, element ID, score, and difference from average.\nFor each event, give the rank of the student’s score out of all students who completed that element, where 1 means they got the highest score. Report columns for the student, element, score, and rank.\nHint: The rank() function, described in table 9.63 of the manual, will be useful here. There are examples in the official tutorial.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced SQL</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql-code.html",
    "href": "data-engineering/sql-code.html",
    "title": "5  Using SQL from Code",
    "section": "",
    "text": "5.1 SQL in Python\nIt’s nice to be able to type queries into psql and see results, but most often you’d like to do more than that. You’re not just making a database to run handwritten queries – you’re using it to store data for a big project, and that data then needs to be used to fit models, make plots, prepare reports, and all sorts of other useful things. Or perhaps your code is generating data which needs to be stored in a database for later use.\nRegardless, you’d like to run queries inside R, Python, or your preferred programming language, and get the results back in a form that can easily be manipulated and used.\nFortunately, PostgreSQL – and most other SQL database systems – use the client-server model of database access. The database is a server, accessible to any program on the local machine (like the psql client) and even to programs on other machines, if the firewall allows it.\nPsycopg is a popular PostgreSQL package for Python. Check out the reference guide for full documentation of all its functions and features. It can be installed by using\nPyscopg is based on two core concepts: connections and cursors. Both are represented by Python objects. A connection object represents your connection to the database server, and allows you to create transactions and commit them. A cursor object represents one particular interaction with that connection: you can submit a query, then use the cursor to fetch its results.\nTo connect:\nconn is now a connection object. (See Section 5.2 below on how to store your password securely; including it directly in your Python files is not a good idea.) We can create a cursor and use it to submit (“execute”) a query:\nNotice the use of interpolation to put variables into the query – see Section 5.3 below. Thou shalt not concatenate values directly into the SQL string.\nIf we do a SELECT, we can get the results with a for loop or the fetchone and fetchmany methods:\nThe execute method is used regardless of the type of query.\nIf you use pandas for your data frames, you can also convert the results of any query directly into a pandas data frame:\nThe psycopg3 package automatically starts a SQL transaction (Section 4.2) when you create a connection with psycopg.connect(). You must explicitly commit the transaction if you are inserting data:\nIf you do not explicitly commit the transaction, it will be rolled back when your script exits.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using SQL from Code</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql-code.html#sql-in-python",
    "href": "data-engineering/sql-code.html#sql-in-python",
    "title": "5  Using SQL from Code",
    "section": "",
    "text": "pip install \"psycopg[binary]\"\n\n\nimport psycopg\n\nconn = psycopg.connect(\n    host=\"pinniped.postgres.database.azure.com\", dbname=\"yourusername\",\n    user=\"yourusername\", password=\"yourpassword\"\n)\n\ncur = conn.cursor()\n\nbaz = \"walrus\"\nspam = \"penguin\"\n\ncur.execute(\"INSERT INTO foo (bar, baz, spam) \"\n            \"VALUES (17, %s, %s)\", (baz, spam))\n\n\n# Python concatenates adjacent string literals, which makes it\n# easy to split queries across lines:\ncur.execute(\"SELECT time, persona, element, score \"\n            \"FROM events\")\n\n# iterating over tuples; each tuple is one row:\nfor row in cur:\n    # Destructuring lets us assign each tuple entry to a variable\n    time, persona, element, score = row\n\n    # do something with the results here\n\n# alternately, one at a time:\nrow = cur.fetchone()\n\n\nimport pandas as pd\n\nd = pd.read_sql_query(\"SELECT persona, element FROM events WHERE id = %(id)s\",\n                      conn, params = {'id': 17})\n\nfor event in events:\n    cur.execute(\"INSERT INTO events (...)\")\n\nconn.commit() # commit transaction\nconn.close() # close connection",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using SQL from Code</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql-code.html#sec-storing-sql-password",
    "href": "data-engineering/sql-code.html#sec-storing-sql-password",
    "title": "5  Using SQL from Code",
    "section": "5.2 Storing your SQL password",
    "text": "5.2 Storing your SQL password\nThe code above stores your password right in the source file. This is a bad idea. If the code is ever shared with anyone, posted online, or otherwise revealed, anyone who sees it now has your database username and password and can view or modify any of your data. If you commit the file to Git, your password is now in your Git history forever. Fortunately, there are ways to work around this.\nA simple approach is to create a file called credentials.py that looks like this:\nDB_USER = \"yourusername\"\nDB_PASSWORD = \"yourpassword\"\nThen, in other files, you can use import credentials and use credentials.DB_PASSWORD instead of writing in your password. Add credentials.py to your .gitignore to avoid accidentally committing it.\nLarge companies often use systems for managing passwords and tokens required by scripts, such as Vault or cloud-specific systems like Azure Key Vault and the AWS Secrets Manager. These are much better than creating hidden files.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using SQL from Code</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql-code.html#sec-safe-sql",
    "href": "data-engineering/sql-code.html#sec-safe-sql",
    "title": "5  Using SQL from Code",
    "section": "5.3 Practicing safe SQL",
    "text": "5.3 Practicing safe SQL\nSuppose you’ve loaded some data from an external source – a CSV file, input from a user, from a website, another database, wherever. You need to use some of this data to do a SQL query.\ncur.execute(\"SELECT * FROM users WHERE username = '\" + username + \"' \" +\n            \"AND password = '\" + password + \"'\"))\nNow suppose username is the string '; DROP TABLE users;--. What does the query look like before we send it to Postgres?\nSELECT * FROM users\nWHERE username = ''; DROP TABLE users; -- AND password = 'theirpassword'\nWe have injected a new SQL statement, which drops the table. Because -- represents a comment in SQL, the commands following are not executed.\n\n\n\nxkcd #327\n\n\nLess maliciously, the username might contain a single quote, confusing Postgres about where the string ends and causing syntax errors. Or any number of other weird characters which mess up the query. Clever attackers can use SQL injection to do all kinds of things: imagine if the password variable were foo' OR 1=1 – we’d be able to log in without knowing the right password!\n(For a time in the early 2000s, SQL injection was incredibly common, and numerous services were hacked using it. It still crops up when unwary developers don’t think about their queries.)\nWe need a better way of writing queries with parameters determined by the code. Fortunately, database systems provide parametrized queries, where the database software is explicitly told “this is an input, with this value” so it knows not to treat it as SQL syntax. For example:\nusername = \"'; DROP TABLE users;--\"\npassword = \"walruses\"\n\ncur.execute(\"SELECT * FROM users \"\n            \"WHERE username = %(user)s AND password = %(pass)s\",\n            {\"user\": username, \"pass\": password})\nIn this form, psycopg sends the query (with placeholders) and the values separately, so that the SQL server knows which parts are values and which parts are part of the SQL syntax. It is impossible for anyone to maliciously slip different SQL syntax into your queries.\nConsult the psycopg documentation on query parameters for more details and options.\nYou should always use this approach to insert data into SQL queries. You may think it’s safe with your data, but as the documentation notes:\n\n\nDon’t manually merge values to a query: hackers from a foreign country will break into your computer and steal not only your disks, but also your cds, leaving you only with the three most embarrassing records you ever bought. On cassette tapes.\nIf you use the % operator [or f-strings] to merge values to a query, con artists will seduce your cat, who will run away taking your credit card and your sunglasses with them.\nIf you use + to merge a textual value to a string, bad guys in balaclava will find their way to your fridge, drink all your beer, and leave your toilet seat up and your toilet paper in the wrong orientation.\n\n\nYou may find older code that does not use parametrized queries, but instead concatenates queries together with escapes. In short, before concatenating values into a query, you check that their types match what they should be (e.g. integer values really are integers), and for strings, you replace ' with '', the escape for representing ' within a string. Then you concatenate.\npsycopg doesn’t actually contain a function for escaping strings, because it’s too easy to mess up and forget to escape something, or to escape something incorrectly—just use a parametrized query, which cannot go wrong. But you may find escaping commonly used in other languages.1 If you’ve ever seen text with apostrophes come out of a website with extra backslashes, you’ve seen a poorly done escaping system (as other SQL databases use \\' to escape single quotes).\n\nExercise 5.1 (SQL injection) Suppose we have a users table with username, email, and admin columns. The admin column is a boolean (true/false) that determines whether the user has administrative privileges in the application.\nThe application includes a form to update your email address. When you submit the form, it does the following:\ncur.execute(\"UPDATE users \"\n            \"SET email = '\" + email_address \"' \"\n            \"WHERE username = '\" + username + \"'\")\nSuggest an email address and username to submit so that your user account will be given administrative privileges.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using SQL from Code</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql-code.html#handling-errors",
    "href": "data-engineering/sql-code.html#handling-errors",
    "title": "5  Using SQL from Code",
    "section": "5.4 Handling errors",
    "text": "5.4 Handling errors\nQuery errors are converted into Python exceptions. For example, here I’m trying to insert a country that’s already in the countries table, and the country code is defined to be a primary key, so it must be unique:\nIn [7]: cur.execute(\"INSERT INTO countries (country_code, country_name) VALUES ('us', 'Estados Unidos')\")\n---------------------------------------------------------------------------\nUniqueViolation                           Traceback (most recent call last)\n&lt;ipython-input-7-5df5a6b218e6&gt; in &lt;module&gt;\n----&gt; 1 cur.execute(\"INSERT INTO countries (country_code, country_name) VALUES ('us', 'Estados Unidos')\");\n\n~/miniconda3/lib/python3.6/site-packages/psycopg/cursor.py in execute(self, query, params, prepare, binary)\n    546                 )\n    547         except e.Error as ex:\n--&gt; 548             raise ex.with_traceback(None)\n    549         return self\n    550\n\nUniqueViolation: duplicate key value violates unique constraint \"countries_pkey\"\nDETAIL:  Key (country_code)=(us) already exists.\nYou can use Python’s exception management features (try, except, finally, etc.) with these exceptions just like with any others:\ntry:\n    cur.execute(\"INSERT INTO countries (country_code, country_name) \"\n                \"VALUES ('us', 'Estados Unidos')\")\nexcept psycopg.errors.UniqueViolation as e:\n    # do something with the exception here\nSee the Python errors and exceptions tutorial for more information.\nNotice that the exception is a UniqueViolation. Psycopg translates Postgres error codes into distinct Python exceptions, so if you use except to catch them, you can catch different types of Postgres errors and handle them differently. The psycopg.errors module defines the exception types that are available.\nIf an error occurs in any query in your transaction, any subsequent queries will produce errors like:\nInFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block\nThis means you need to call conn.rollback() to roll back the transaction explicitly so you can start over. Or you can use the transaction features described below.\n\nExercise 5.2 (Handling SQL errors) Load the events data into your own database so that you have permission to edit it. (You can open the SQL file in Azure Data Studio to run it in your database, creating the table.)\nWrite a short Python script using psycopg to connect to the database. Have the script issue an INSERT query that causes a foreign key constraint violation. Use try and except to catch the exception and print a message about the error.\nYour script should only catch foreign key constraint violations, and not, say, SQL syntax errors or other unrelated problems.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using SQL from Code</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql-code.html#sec-transactions-python",
    "href": "data-engineering/sql-code.html#sec-transactions-python",
    "title": "5  Using SQL from Code",
    "section": "5.5 Transactions in Python",
    "text": "5.5 Transactions in Python\nPsycopg provides convenient ways to manage transactions (Section 4.2). (See the psycopg manual chapter for full details.)\nThe most convenient method is with transaction contexts. Python provides a feature called context managers that allow actions to be automatically run to create and destroy resources, and psycopg reuses these to create and commit (or rollback) transactions. Context managers are created with the with statement.\nFor example:\nconn = psycopg.connect(...)\ncur = conn.cursor()\n\nwith conn.transaction():\n    # This creates a new transaction\n\n    cur.execute(\"INSERT INTO foo ...\")\n    cur.execute(\"INSERT INTO bar ...\")\n\n# Now the block is done and committed. But if there was an exception raised in\n# the block that was not caught, the context manager would automatically call\n# conn.rollback() for us.\nWhat’s great is that these can be nested! For instance, suppose you’re loading lots of data by looping through a data file doing INSERT. You want the entire data to be loaded or not loaded; you don’t want it to crash halfway through and only half the data is loaded. But you also want to recover from bad rows and keep inserting the rest. You could do the following:\nnum_rows_inserted = 0\n\n# make a new transaction\nwith conn.transaction():\n    for row in big_data_set:\n        try:\n            # make a new SAVEPOINT -- like a save in a video game\n            with conn.transaction():\n                # perhaps a bunch of reformatting and data manipulation goes here\n\n                # now insert the data\n                cur.execute(\"INSERT INTO foo ...\", ...)\n        except Exception as e:\n            # if an exception/error happens in this block, Postgres goes back to\n            # the last savepoint upon exiting the `with` block\n            print(\"insert failed\")\n            # add additional logging, error handling here\n        else:\n            # no exception happened, so we continue without reverting the savepoint\n            num_rows_inserted += 1\n\n# now we commit the entire transaction\nconn.commit()\nThis lets us handle errors in individual rows without stopping or losing all the data we have inserted, but still ensures that other users see all or none of the data, not part of it.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using SQL from Code</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql-code.html#batching-many-queries",
    "href": "data-engineering/sql-code.html#batching-many-queries",
    "title": "5  Using SQL from Code",
    "section": "5.6 Batching many queries",
    "text": "5.6 Batching many queries\nWhen the database is on a different machine than your Python script, the delay in sending messages between them can become a problem. Each time you use cur.execute() to run a query, your Python script must send several messages to Postgres with the query and the parametrized data; then it must wait for PostgreSQL to send back the results and indicate it’s ready for the next query.\nFrom the CMU network to our Azure database server, I measured a 25-millisecond round trip. That means it takes 25 milliseconds to send a message and get a response back, even when the response is sent instantly. The round-trip time to a server in the UK was closer to 100 milliseconds; if you’re on crappier Internet than the CMU connection, you can easily have 200-millisecond round trips to some servers.\nThat means every query to our Azure database must take at least 25 milliseconds, plus whatever time it takes Postgres to do the actual work. That imposes a limit of 40 queries per second—lower if your Internet is slower or if the queries take Postgres some time to execute.\nThis is a problem if we’re executing large batches of queries, e.g. if we’re sending many INSERT commands to load lots of data. Being limited to 40 rows per second when you have thousands of rows is a big problem.\nFortunately there is an alternative. PostgreSQL now supports pipeline mode. In this mode, you can send a stream of many queries to Postgres without waiting for their results; Postgres will send the results back to you as they become available. You could send a batch of 100 without having to wait \\(100 \\times 25\\) milliseconds (2.5 seconds!), and only wait briefly at the end to receive the final confirmation that they’re done.\nPipeline mode can be invoked manually (see the manual page), but it can also be done with the executemany() method. For example:\ncur.executemany(\"INSERT INTO tablename (foo, bar, baz) VALUES (%s, %s, %s)\",\n                [(\"foo for row 1\", 9, \"baz1\"),\n                 (\"foo for row 2\", -9, \"baz2\"),\n                 (\"foo for row 3\", 16, \"ducks\")])\nAs of psycopg version 3.1, this automatically sends the batch of rows in pipeline mode. Each entry in the list is a tuple of values defining one row; the query is hence sent three times.\nThis can be used for INSERT queries or any other query where you want to run the same query many times with different values.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using SQL from Code</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql-code.html#building-automated-workflows",
    "href": "data-engineering/sql-code.html#building-automated-workflows",
    "title": "5  Using SQL from Code",
    "section": "5.7 Building automated workflows",
    "text": "5.7 Building automated workflows\nThere are a few common use-cases for using SQL within Python code (or other languages):\n\nSupporting an interactive application, like a web site or an app. The program responds to actions by the user by performing SQL queries and displaying the results. Facebook, for instance, originally stored all its posts and comments in a SQL database, so viewing the website or app involves running many SQL queries to fetch content to display.\nAutomating tasks that are done occasionally. For example, I might occasionally have to set up accounts for MADS students on a database server, so I could write a Python script that loops over the course roster and runs the necessary commands for each student.\nAutomating workflows that run on a schedule, such as a data pipeline that gets the latest data from several sources, processes it, and loads it into a database.\n\nThe integrity and error-handling requirements for these cases can vary, and so part of your design process is to consider how to handle problems. For example:\n\nIn an interactive website, recovering from errors may not be important; if the system isn’t working, you can check your Facebook feed later. Simply catch exceptions and display an error message to the user. It may also be helpful to log detailed information about the error to a file so that the software developer can debug it later.\nIn a manually run task, like loading new users, it may be best to present the error to the user, let them fix any issues, and then let them start over. That means not partially loading data or leaving the task half-done, because then redoing it is much harder.\nBut a scheduled workflow may be used for many other tasks later: your data pipeline loads data that an automated reporting system uses, and its reports get sent to another system for analysis, and so on. If your job fails, the downstream tasks can’t run at all. If there’s a problem loading one or two rows of data, you may want to load the rest so the other jobs can run on schedule – but record which data failed to load so an analyst can examine it, fix the problem, and manually add it later. But if the integrity of the later analyses depends on loading everything perfectly – as in an accounting system – you may have to catch the error, roll back, and alert someone to investigate.\n\nIn each case, the decision depends on the task requirements. In some tasks, the integrity of data is key, and any error indicates a serious problem that must be fixed before the database can be updated; in others, minor issues can be bypassed for later investigation.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using SQL from Code</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql-code.html#exercises",
    "href": "data-engineering/sql-code.html#exercises",
    "title": "5  Using SQL from Code",
    "section": "5.8 Exercises",
    "text": "5.8 Exercises\n\nExercise 5.3 (Automated reporting) Write a Python script called event-report.py. This script should take a command-line argument specifying a date (such as 2015-03-23), so it can be invoked like this:\npython event-report.py 2015-03-23\nWhen it runs, it should connect to the Postgres database and query the events table for all events in the week preceding the given date. It should produce a table of the top 10 students that week, ranked by their cumulative scores on all events during that week. The table should include their name, birthday, score from that week, rank, and overall average score across all weeks.\nPrint this table out as a nicely formatted table. The tabulate package can help format it, or you can use the pandas to_string() method if you have a pandas data frame.\nOnce your code works, paste it into a notebook cell to submit with your homework. Include a text cell with the formatted output from your script, run with 2015-03-23 as the date.\n\n\nExercise 5.4 (NBA teams) The nba-teams.txt data file contains information about each team in the NBA.\nWrite a table schema to store this data. Provide the CREATE TABLE command with your submission. Include a primary key and appropriate NOT NULL constraints. Consider using enumerated types to represent columns that can only take on one of several fixed values (i.e. like a factor in R).\nWrite Python code to read the data from nba-teams.txt and run a series of INSERT commands to populate the table. The code should loop through the entire file and repeatedly run INSERT. When it’s done, it should print out a summary of how many rows were successfully inserted.\nBe sure to commit the transaction so the data is saved to the database.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using SQL from Code</span>"
    ]
  },
  {
    "objectID": "data-engineering/sql-code.html#footnotes",
    "href": "data-engineering/sql-code.html#footnotes",
    "title": "5  Using SQL from Code",
    "section": "",
    "text": "For example, PHP used to have mysql_escape_string() to escape strings being sent to MySQL. Except it didn’t keep track of which character encoding was being used, so it could escape strings incorrectly. It was replaced with mysql_real_escape_string(), and eventually by parametrized queries.↩︎",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using SQL from Code</span>"
    ]
  },
  {
    "objectID": "data-engineering/text-search.html",
    "href": "data-engineering/text-search.html",
    "title": "6  Full Text Search",
    "section": "",
    "text": "6.1 Text searching tasks\nSo far our searching, grouping, and aggregation has focused on numerical columns and numerical summaries of them. But text data is quite common. We can easily store text in PostgreSQL by using the TEXT column type, but often we want ways to efficiently search through text. This problem is known as full text search, because you’d like to search using the entire contents of the text, not just features or tags attached to it.\nFull text search requires special consideration because doing it well is a lot of work. A naive approach—looping through every row of data and checking if the text matches a search query—would be impossibly slow and wouldn’t even be very good at searching, so we must plan ahead to make full text search effective.\nText is complicated. As you can imagine, there are many ways you might want to search it.\nThere are a variety of basic text operators that can help with simple tasks, and we’ll address those first. Then we’ll move on to more advanced full-text search.\nBut first: What is text, and how is it stored?",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Full Text Search</span>"
    ]
  },
  {
    "objectID": "data-engineering/text-search.html#text-searching-tasks",
    "href": "data-engineering/text-search.html#text-searching-tasks",
    "title": "6  Full Text Search",
    "section": "",
    "text": "Example 6.1 (Movie titles) You run a website that hosts movie information and reviews. Users type the title of a movie—or parts of it—into a search box, and your system must find the matching movies to list for them.\nIf you have a movies table with a title column, a simple query is\nSELECT * FROM movies\nWHERE title = %s\nfilling in the user’s entry for the placeholder. But they may type in only part of the title (everything everywhere to find Everything Everywhere All at Once), they might misspell it (the avngers), or they might misremember it and get parts wrong (star wars the return of the sith). How do you find the best match?\n\n\nExample 6.2 (Citation search) You’re analyzing a database of scientific papers to find out which ones cite each other. Each paper has a Digital Object Identifier (DOI), a unique ID in the form doi:10.NNNN/suffix, where NNNN is a four-digit (or longer) number and the suffix can be any string of characters not including spaces.\nReference lists for papers often give the DOI for the papers they cite, so you’d like to find all papers with DOIs in them. How do you search for text matching this pattern?\n\n\nExample 6.3 (Legal documents) You’re suing a large tech company because you believe they violate antitrust law. Through subpoenas, you have obtained millions of their internal emails, and you want to search the emails for ones mentioning monopoly, competition, collusion, and other such words and phrases. But those words have many variations: monopolies, monopolistic, competitive, competitors, colluding, etc.\nIf the emails are in a table with subject and body columns containing their contents, how do you effectively search them?",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Full Text Search</span>"
    ]
  },
  {
    "objectID": "data-engineering/text-search.html#character-encoding",
    "href": "data-engineering/text-search.html#character-encoding",
    "title": "6  Full Text Search",
    "section": "6.2 Character encoding",
    "text": "6.2 Character encoding\nComputer memory is binary: it stores only 0s and 1s. It follows that text must be stored as numbers, not as text. That requires a character encoding: a system to number each character in a given alphabet, so they can be stored as numbers and then displayed as the correct characters.\nSince the 1960s, the common standard has been the American Standard Code for Information Interchange (ASCII). ASCII reflects the biases of American computer engineers in the 1960s: it uses only 7 bits to encode each number, allowing only \\(2^7 = 128\\) possible characters.1 These were used to encode the upper- and lowercase Latin alphabet, spaces, newlines, punctuation marks, and a variety of control characters that tell a teletype machine to do something rather than displaying text. (For example, there’s a character that tells the machine to ring a bell to alert the operator to do something.)\nNotably, ASCII does not include any accented characters (like é, ñ, or ö), characters from any other alphabet (Cyrillic, Arabic, Devanagari, Hangul, etc.), or symbols from any other writing system (Chinese, Japanese, etc.).\nThis led to an explosion of character encoding systems specialized for specific scripts, each supporting only some scripts but not others. You’ll encounter a few of the most common:\n\nISO 8859-1, also called Latin-1: Uses 8 bits instead of 7, allowing it to extend ASCII with accented characters used in many European languages. Commonly also seen as Windows-1252 or CP-1252, which adds some additional characters.\nGB 2312 and GB 18030, Chinese government standards supporting simplified and traditional Chinese characters.\nShift JIS, used for Japanese.\nWindows-1251, used for Cyrillic.\n\nNowadays, the most common encoding is UTF-8, which is designed to cover almost every script used in the world (and many historic ones no longer in use). We’ll discuss it in more detail in Section 6.2.3.\n\n6.2.1 Character encoding mishaps\nCharacter encoding problems occur when text in one encoding is interpreted as being from another encoding.\nFor example, the string can’t (notice the right curly apostrophe) can be encoded in UTF-8, which has a character for the right single quotation mark. But that character is represented as three bytes. In Windows-1252, those three bytes are read as â, €, and ™, so the string appears canâ€™t. If you don’t know the encoding of the text and pick the wrong one, you will display it incorrectly.\nThis is also annoying for searching: if you’re searching for a string that you’ve written in one encoding, but the data you’ve stored is in a different encoding, a naive search that checks the bytes stored will not find it.\n\n\n6.2.2 Converting\nOld programming languages like C do not require strings to be in any specific encoding—you can shove any bytes you want into a string, and it’s your problem if the encoding is wrong.\nModern systems like Python (since Python 3) enforce a consistent encoding. In Python, every string must be read in with a defined encoding, so Python can convert it automatically when doing any operations. For example, if I try to read a file that is not UTF-8-encoded:\nIn [5]: f = open(\"/bin/ls\", \"r\")\nIn [6]: f.readline()\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 f.readline()\n\nFile ~/miniconda3/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    319 def decode(self, input, final=False):\n    320     # decode input (taking the buffer into account)\n    321     data = self.buffer + input\n--&gt; 322     (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323     # keep undecoded input until the next call\n    324     self.buffer = data[consumed:]\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\nThe file /bin/ls is a compiled program, not text, so when Python tries to interpret the bytes as UTF-8 text, it finds some of them are not valid characters and immediately complains.\nThis is why open() has an encoding argument: so you can tell it which encoding to read, and it can automatically translate the encoding as you operate on the text.\n\n\n6.2.3 UTF-8: the only acceptable encoding\nUnicode is a standard that attempts to list the characters in all the world’s writing systems, assigning each a unique identifying number. (Right now nearly 155,000 characters are defined and numbered.) These are called code points. Each code point is, conceptually, one “thing” in the writing system.\nUnicode code points are commonly written with the notation U+NNNN. For example, U+0065 is the code point LATIN SMALL LETTER E, i.e. e. The number is written in hexadecimal.\nThere are several encodings defined in the Unicode standard, which give ways to encode the code points in binary. The most popular, now used by around 99% of websites and applications, is UTF-8.\nUTF-8 is weird and it may break some of your assumptions about how text is stored, so it’s useful to understand how it works.\nUTF-8 is a variable-width encoding. That means some characters are represented with only one byte (8 bits) and others are represented with up to 4 bytes (32 bits). The one-byte characters generally match the encodings used in ASCII, so if you UTF-8 encode English text without any accents or special characters, it will also be valid ASCII.\nThis is useful for compatibility and for compactness: the designers of UTF-8, who were Westerners who mostly dealt with the Latin alphabet, wanted to use the fewest bytes for what was (to them) the most common type of text.\nFor characters outside the basic ASCII range, a special code in the first byte indicates that the character continues to two, three, or four bytes. The encoding is designed so that software reading the bytes can quickly determine where characters start and end, so there’s no danger of accidentally reading, say, the middle two bytes of a four-byte character and interpreting them as a character of their own.\nThis can cause confusion in languages where string indexing and length is based on the number of bytes. For instance, 💩 (U+1F4A9) is encoded with four bytes, but it is only one code point. Python counts code points:\nIn [8]: len(\"💩\")\nOut[8]: 1\nDespite its quirks, UTF-8 is now almost universally supported by software and is widely used across the Internet. If you are creating text data, you should encode it as UTF-8.\n\n\n6.2.4 Normalization\nAnother quick is that Unicode provides two ways to encode many common characters. For example, the code point U+0301 represents the COMBINING ACUTE ACCENT. Place it after any character and it puts an acute accent on that character. Hence we can write é as U+0065 U+0301 (e plus an acute accent).\n(This combining feature is also used for encoding emojis that can be displayed in different skin tones: 👍🏽 is 👍 plus the U+1F3FD medium skin tone modifier.)\nBut Unicode also provides precomposed forms of common characters, so U+00E9 is LATIN SMALL LETTER E WITH ACUTE. Consequently:\nIn [14]: len(\"é\")\nOut[14]: 1\n\nIn [15]: len(\"é\")\nOut[15]: 2\nThis is another problem for searching text: a user searching for “résumé” might not find a document containing “résumé” because it’s encoded differently.\nTo solve this, Unicode defines normalization procedures that take text and turn it into a single canonical form. For instance, NFC (Normalization Form Canonical Composition) composes every character when possible, so é is always one byte, not two. (There are three other normalization schemes that produce different normalized forms, but NFC is the most commonly used.)\nTo effectively search text, then, we must both know its encoding and ensure it is normalized consistently.\n\n\n6.2.5 Language differences\nSome operations depend not just on the encoding but on the language.\nFor example: what is the uppercase version of i? In many languages, I; but in Turkish, it’s İ. What’s the lowercase version of I? In Turkish, it’s ı. (Turkish scores a point for logical consistency.)\nOperations like uppercasing and lowercasing hence depend on the language! Even Python doesn’t handle this automatically, and so if you’re serious about text, you’ll need a library like ICU (International Components for Unicode) that provides all the necessary procedures.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Full Text Search</span>"
    ]
  },
  {
    "objectID": "data-engineering/text-search.html#basic-text-operations-in-postgresql",
    "href": "data-engineering/text-search.html#basic-text-operations-in-postgresql",
    "title": "6  Full Text Search",
    "section": "6.3 Basic text operations in PostgreSQL",
    "text": "6.3 Basic text operations in PostgreSQL\nIn PostgreSQL, every database has a default encoding (hopefully UTF-8), and PostgreSQL uses that encoding to store all text and character columns. Since it knows the encoding, it can do text operations. Here are a few common ones:\nSELECT trim(' foo bar   '); -- deletes whitespace at beginning and end\n\nSELECT 'foo' || 'bar'; -- concatenate\n\nSELECT 'foo' || 4; -- convert to string and concatenate\n\n-- find location in string:\nSELECT position('doi:' in 'it is doi:10.1073/pnas.2111454118');\nThe SQL standard only defines a small set of string functions, so database engines provide their own additional functions. PostgreSQL supports many common operations you might want to do:\nSELECT starts_with('doi:10.1073/pnas.2111454118', 'doi');\n\nSELECT substr('doi:10.1073/pnas.2111454118', 5);\n\nSELECT length('💩'); -- Azure Data Studio bug\n\n6.3.1 Pattern matching\nOften, as in Example 6.1, it’s sufficient to search a text column for values matching a certain pattern. SQL provides the LIKE operator to do this. LIKE uses a special pattern syntax to define what to look for. In the patterns, % represents any sequence of zero or more characters, so we can write:\nSELECT 'doi:10.1073/pnas.2111454118' LIKE 'doi%';\n\nSELECT 'Gone with the Wind' LIKE '%with the%';\n\n-- case sensitive:\nSELECT 'Gone with the Wind' LIKE '%WITH THE%';\n\n-- or use ILIKE for case-insensitive:\nSELECT 'Gone with the Wind' ILIKE '%WITH THE%';\n\nSELECT 'Return of the Jedi' LIKE '%revenge%';\nSimilarly, _ represents any single character, but not more:\nSELECT 'foo' LIKE '_o_';\n\nSELECT 'foooo' LIKE '_o_';\nInstead of LIKE and ILIKE, you can also write ~~ or ~~*, as in 'Gone with the Wind' ~~ '%with the%'. This is not part of the SQL standard, but PostgreSQL supports them internally.\n\nExercise 6.1 (Like, totally) In the events table, the feedback column gives a textual description of the feedback to the student. Produce a query that counts how many events have the string “you” in their feedback, in upper- or lowercase.\n\n\n\n6.3.2 Regular expression matching\nLIKE provides simple patterns, but its pattern matching is quite limited. What if you want to, say, find strings containing only the digits 0-9 repeated at most 4 times, followed by at least one uppercase letter? What if you want to check if every password has at least one uppercase letter, one symbol, a Pokémon character name, and a Mersenne prime?2\nTo match more complex passwords, we can use regular expressions. There are multiple ways to write regular expressions; the SQL standard provides the SIMILAR TO operator, which uses a syntax no other software uses, but most database systems also support POSIX regular expressions. These follow a standard syntax widely used by many different systems; even software that varies typically only varies in the details and not the general picture.\nA regular expression is a specially formatted string that defines the pattern we would like to match. Basic patterns are simple: the regular expression ducks matches any portion of text containing the substring \"ducks\". But other syntax in patterns produces more complex behavior. Let’s work through basic syntax with some examples:\n\ndu.ks\n^du.ks\n^du.ks$\ndu.+ks\ndu.*ks\ndu.?ks\n\nThese wildcards are greedy: they will match everything they can, even if a shorter match is possible. However, we can change their behavior with ?:\n\ndu.*?ks\n\nWe can give a set of characters using square brackets:\n\ndu[cn]ks\n&lt;[a-zA-Z]&gt;\n&lt;[^j]&gt;\ndu[cn]ks|g[oe]+se\n\nThere are certain common sets that are represented using special escapes:\n\n\\d: any digit 0-9\n\\s: any whitespace character (space, newline, tab, etc.)\n\\w: any word character (alphanumeric and underscore)\n\nWe can define capturing groups using parentheses, then refer to them later in our pattern:\n\n(du[cn]ks|g[oe]+se)\n&lt;([a-zA-Z]+)&gt;(.*)&lt;/\\1&gt;\n\nIf you need to literally match parentheses, brackets, or other syntax, you must escape them with a backslash: \\( matches an open parenthesis.\nThere are many additional options and syntax features we can define, but it’s best to master the basics first.\nRegular expressions can be used within Postgres using the ~ operator. Regular expressions are specified as strings, so we must be careful with string escaping. For example:\nSELECT 'foo' ~ 'foo';\nSELECT 'ducks' ~ 'du.ks';\nSELECT '&lt;foo&gt;bar&lt;/foo&gt;' ~ '&lt;([a-zA-Z]+)&gt;(.*)&lt;/\\1&gt;';\nUsing regexp_match(), we can see the portion of the string that matches, or the capture groups that match:\nSELECT regexp_match('the mighty ducks', 'du.ks');\nSELECT regexp_match('&lt;foo&gt;bar&lt;/foo&gt;', '&lt;([a-zA-Z]+)&gt;(.*)&lt;/\\1&gt;');\nThere are also functions count how many matches there are; see section 9.7.3 of the manual for details.\nRegExr is a great tool to visualize your regular expressions and see what they match. There are slight syntactic differences between TODO\n\nExercise 6.2 (Practice regular expressions) For each pattern below, use regexp_match() to confirm the expression matches what you intend it to.\n\nWrite a regular expression that matches strings of the form doi:10., followed by one or more numbers, then a forward slash, then one or more alphanumeric characters. Capture the part after doi: in a capture group.\nMatch strings like [tag]text[/tag], where the two tags match and the text does not contain [/tag]. For instance, in the string \"[foo]text[/foo]bar[/foo]\", the match should only be \"[foo]text[/foo]\".\nMatch strings like key=value, capturing both the key name and the value. The key should be alphabetical and the value should be a decimal number, like 3.14. For example, pi=3.1415 should capture pi and 3.1415, but 4=7.2.4 should not match.\n\n\nFinally, there is one use case that often comes up: If you’re parsing HTML (to scrape a website, for instance), it’s tempting to use regular expressions to match the element you need. Don’t! You can’t parse HTML with regex. The syntax of HTML is too complicated for it to be possible to write a regular expression that always correctly matches HTML tags. See ?sec-scraping for the correct approach.\n\n\n6.3.3 Regular expression replacement\nOne common use of regular expressions is to replace certain patterns in text with others. Frequently this is used inside text editors to do sophisticated find-and-replace, and it gets used in many other applications processing text that needs to be reformatted to match some specification.\n\nExample 6.4 (BBCode) You may be familiar with Markdown, the simple text markup language used in systems like R Markdown and in comment boxes on many websites. Before Markdown was widely used, forum software often used BBCode, a system based on a restricted set of “tags” similar to HTML.\nFor example, [b]some phrase[/b] represents bolding the text, and might be translated into the HTML for &lt;b&gt;some phrase&lt;/b&gt; or &lt;strong&gt;some phrase&lt;/strong&gt;.\nUsing BBCode prevents users from writing HTML directly (and hence messing with the site’s formatting) and restricts them to only tags supported by the forum software.\nA simple implementation of BBCode might use regular expressions to match a set of tags and convert them to the corresponding HTML tags.\n\nMost regular expression systems support regular expression-based text replacement. In PostgreSQL this is called regexp_replace(), and takes a regular expression (to choose what to replace) and a replacement pattern. In the simplest form, the replacement is just a static string:\n-- Filter naughty words\nSELECT regexp_replace('recrudescent', 'dang|crud', '****') AS clean;\n    clean\n--------------\n re****escent\n(1 row)\nBut the pattern can also refer to the capture groups in the regular expression. For instance, we can make text dirtier:\n--- Emphasize naughty words\nSELECT regexp_replace('dang recrudescents!', '(dang|crud)', '\\1!!!') AS dirty;\n         dirty\n------------------------\n dang!!! recrudescents!\nNote only the first match was replaced. That’s the default behavior, but regexp_replace() takes additional arguments to control it. A flags argument gives a string of characters setting options, such as i for case-insensitive matching and g for global matching, meaning replacing everything in the string:\nSELECT regexp_replace('dAnG ReCruDeSCeNts!', '(dang|crud)', '\\1!!!', 'ig') AS dirty;\n           dirty\n---------------------------\n dAnG!!! ReCruD!!!eSCeNts!\n(1 row)\n\n\n6.3.4 Regular expressions in Python\nBesides PostgreSQL, you can do regular expressions in most languages, including Python. In Python they are supported by the re module of the standard library.\n\nimport re\n\n# list all matches\nre.findall('(dang|crud)', 'dang recrudescents!')\n\n['dang', 'crud']\n\n\n\n# check if entire string matches\nre.fullmatch(r\"&lt;([a-zA-Z]+)&gt;(.*)&lt;/\\1&gt;\", \"&lt;foo&gt;bar&lt;/foo&gt;\")\n\n&lt;re.Match object; span=(0, 14), match='&lt;foo&gt;bar&lt;/foo&gt;'&gt;\n\n\n\n# substitute (replace) text\nre.sub(\"(dang|crud)\", r\"\\1!!!\", \"dAnG ReCruDeSCeNts!\",\n       flags=re.IGNORECASE)\n\n'dAnG!!! ReCruD!!!eSCeNts!'\n\n\nNotice we’re using raw string syntax (r\"string\") to avoid needing to escape backslashes.\n\nExercise 6.3 (BBCode with regexp) Suppose you’re building a simple forum software and want to support BBCode (Example 6.4). Write a single re.sub() call that supports i, b, and u tags, converting them to the HTML tags &lt;i&gt;, &lt;b&gt;, and &lt;u&gt; (italic, bold, and underline).\nFor example, [i]This[/i] is a [b]test![/b] should be converted to &lt;i&gt;This&lt;/i&gt; is a &lt;b&gt;test!&lt;/b&gt;, but Your mother is a [duck]hamster[/duck] should be unchanged. However, when tags are not nested, they should not match: [b]This [i]is[/b] a test[/i] should not be replaced with anything.\n\n\nExercise 6.4 (Syntax adaptation) Different SQL engines generally support the SQL standard but add variations and special features, causing problems when you use SQL code written for one engine in another. MySQL uses backticks to delimit column and table names so they are not interpreted as SQL syntax (e.g. if you have a table named select).\nFor example, a file full of MySQL queries might contain\nCREATE TABLE `employeelist` (\n  eid INTEGER,\n  firstName varchar(31) NOT NULL DEFAULT '',\n  ... -- more columns go here\n);\nBut PostgreSQL does not support backtick syntax.\nWrite a Python regular expression to take a string containing the MySQL CREATE TABLE syntax, returning a form without the backticks around the table name.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Full Text Search</span>"
    ]
  },
  {
    "objectID": "data-engineering/text-search.html#language-aware-text-search",
    "href": "data-engineering/text-search.html#language-aware-text-search",
    "title": "6  Full Text Search",
    "section": "6.4 Language-aware text search",
    "text": "6.4 Language-aware text search\nDespite all this fancy pattern-matching, we’re still not done searching text. Recall Example 6.3: we wanted to search emails for words like monopoly, while also including variations like monopolies, monopolistic, and so on. We could write many patterns to do this, of course:\nSELECT subject, body FROM emails\nWHERE body LIKE '%monopoly%' OR body LIKE '%monopolies%' OR ...;\nBut that gets boring to write. And inefficient: to search the text, PostgreSQL must load every row of text and search through it—potentially many megabytes or gigabytes of text. (My CMU email account contains 12 gigabytes of messages, for instance.) So: We’d like to automatically handle word variants, and we’d like to index our text so we can quickly find words and phrases.\n\n6.4.1 Lexing text\nTo handle word variants, we need two pieces:\n\nA tokenizer that splits text up into discrete tokens, such as words, numbers, links, and so on. Then we can operate on tokens, rather than on individual characters.\nA lexer that normalizes tokenized text to make all variants of a word the same. The lexer might make all words lowercase, remove plural suffixes (s or es), convert conjugated verbs to their infinitive form (washed → wash, monopolized → monopolize), and remove stop words, which are very common words that are useless for searching because they appear everywhere (the, and, is, and so on). The lexer outputs lexemes, the normalized tokens.\n\nBuilding good tokenizers and lexers is hard. They must be specialized to the language and grammar. Languages are often irregular, so they need lots of special-case rules and dictionaries. Fortunately PostgreSQL already has lexers for popular languages (and you can probably find lexers for uncommon languages if you need them).\nPostgreSQL’s lexer system turns strings into tsvectors (for text search vectors). Here’s what they look like:\nSELECT to_tsvector('english', 'the quick brown fox jumps over the lazy dog');\n                      to_tsvector\n-------------------------------------------------------\n 'brown':3 'dog':9 'fox':4 'jump':5 'lazi':8 'quick':2\n(1 row)\nDespite the appearance, the tsvector is not a string. It is an internal data structure presenting the lexemes and their positions within the lexed text. Notice that the has disappeared, because it is a stopword, and lazy has been lexed into lazi. (Laziness, lazies, and similar words all lex to lazi.)\nSimilarly, the first sentence of the Gettysburg address is lexed into this tsvector:\n'ago':6 'brought':9 'conceiv':17 'contin':13 'creat':29 'dedic':21\n'equal':30 'father':8 'forth':10 'four':1 'liberti':19 'men':27\n'nation':16 'new':15 'proposit':24 'score':2 'seven':4 'upon':11 'year':5\n\n\n6.4.2 Querying documents\nNext, Postgres has a tsquery type for queries. Queries consist of lexed words, optionally in a specific order. The to_tsquery() function converts strings into this format:\nSELECT to_tsquery('english', 'the & quick & brown & fox & jumps');\n              to_tsquery\n------------------------------------\n 'quick' & 'brown' & 'fox' & 'jump'\n(1 row)\nThe query string is written using the & operator to indicate we want all of the words; notice that the was dropped (it’s a stopword) and jumps was lexed to jump. This will match the words in any order. We use the @@ operator to match a tsvector to a tsquery:\nSELECT to_tsvector('the fox brown jumps quickly') @@\n  to_tsquery('the & quick & brown & fox & jumps');\nThe query syntax supports all logical operators, not just &; we can use | (OR) and ! (negation) to express complex queries.\nAlternately, Postgres supports queries written in a way you may be more familiar with from Google and other search systems. Phrases are searched as individual words with &; phrases inside quotation marks must match exactly; writing OR between words allows any to match; and - acts as NOT. For instance:\nSELECT to_tsvector('the fox brown jumps quickly') @@\n  websearch_to_tsquery('\"the quick brown\" -dog');\n\n\n6.4.3 Indexing lexed text\nIt’s clearly inconvenient to use to_tsvector on a text column every time we want to search it. Postgres must go through the entire column, tokenize and lex the text, and then prepare to search it. We can instead ask Postgres to create an index of the tsvector version:\nCREATE INDEX name_of_index ON tablename\nUSING GIN (ts_tsvector('english', columnname));\nNow, whenever we do to_tsvector() on the column in a query, Postgres will use the index instead. A GIN index is an “inverted” index: the index will contain all the lexemes in the column and, for each lexeme, a list of all the rows containing it. The lexemes are organized in a tree so they can quickly be found in a search.\nAlternately, we can ask Postgres to store the tsvectors in a separate column. PostgreSQL can automatically generate columns that are updated whenever a row is changed:\nALTER TABLE tablename\nADD COLUMN column_index_col tsvector\nGENERATED ALWAYS AS (to_tsvector('english', columname)) STORED;\n\nCREATE INDEX column_idx ON tablename USING GIN (column_index_col);\nNow we can write our search queries in terms of column_index_col instead of writing to_tsvector() out every time.\n\n\n6.4.4 Searching with indices\nIn the examples database, I have loaded several tables containing emails from Enron. In brief, Enron was a major electricity, gas, and communications company with revenues of $100 billion dollars in 2000—until, in 2001, it was discovered that its executives used accounting fraud to fake its revenues, and the company collapsed. During the various lawsuits and criminal trails afterward, many of Enron’s internal emails were released, and they are still often used as example data in data analysis. (Often the emails between executives are treated as a social network and used in social network analysis examples, for instance.)\nThe message table contains several hundred thousand emails. I have indexed the subject and body columns containing the email contents, so we can quickly search it:\nSELECT sender, subject FROM message\nWHERE subject_index_col @@ websearch_to_tsquery('monopoly')\nLIMIT 10;\nThis query is blazing fast despite the size of the database. Even searching the bodies is fast:\nSELECT sender, subject FROM message\nWHERE body_index_col @@ websearch_to_tsquery('monopoly')\nLIMIT 10;\nOne approach is to parse the query in the FROM clause, allowing it to be used repeatedly throughout the query. This will be useful when we discuss ranking:\nSELECT sender, subject\nFROM message, websearch_to_tsquery('monopoly') AS query\nWHERE body_index_col @@ query\nLIMIT 10;\nThe query has one row and message has many; implicitly, listing two tables in FROM does a full join, so every row of message has the same query attached to it.\n\n\n6.4.5 Ranking matches\nUsually, when we search a database we want the results to be listed in some kind of useful order. Intuitively, the closest match (containing all the words in our query in roughly the same order) should be on top, as should be documents that contain the query terms many times. A document containing the query terms, but very far apart from each other and only once each, should appear near the end of the list of results.\nWe might also want to weight parts of the text differently. Perhaps a search query should apply to both the title and the text of a document, but matches in the title should be more important than matches in the text.\nIn short, we need a ranking function that supports weights. PostgreSQL provides two:\n\nts_rank() ranks matches by how many lexemes in the document match the query\nts_rank_cd() also accounts for how close the matching lexemes are to each other\n\nThe ranks are calculated so that larger ranks mean more relevant matches, so we sort in descending order. For example,\nSELECT mid, sender, subject, ts_rank_cd(body_index_col, query) AS rank\nFROM message, websearch_to_tsquery('monopoly competition') AS query\nWHERE body_index_col @@ query\nORDER BY rank DESC\nLIMIT 10;\nThese ranks do not account for the length of the document. If the document is long and hence includes the matching lexemes many times, it will rank higher than a shorter document that includes them at the same rate. That may not be desirable, so the optional third argument to ts_rank() and ts_rank_cd() selects a normalization method. The default is 0, which does no normalization, 1 normalizes by the log of the document length while 2 normalizes by the length. (There are several additional options and combinations of normalizations can be used; see section 12.3.3 of the manual for details).\nTo make a search index column that does a weighted search of multiple fields, such as the subject and body of an email, we can change how we define the index column:\nALTER TABLE tablename\nADD COLUMN column_index_col tsvector\nGENERATED ALWAYS AS (\n  setweight(to_tsvector('english', coalesce(important_column, '')), 'A') ||\n  setweight(to_tsvector('english', coalesce(less_important_column, '')), 'D')\n)\nSTORED;\n\nCREATE INDEX column_idx ON tablename USING GIN (column_index_col);\nWeights are the letters A through D, where A has highest weight and D has the lowest. Notice we use coalesce() so that if the column value is NULL, it’s replaced with the empty string; and we can use the || operator to concatenate tsvector values, so the index column contains the combined results for both columns.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Full Text Search</span>"
    ]
  },
  {
    "objectID": "data-engineering/text-search.html#exercises",
    "href": "data-engineering/text-search.html#exercises",
    "title": "6  Full Text Search",
    "section": "6.5 Exercises",
    "text": "6.5 Exercises\n\nExercise 6.5 (Find the incriminating evidence) Part of Enron’s fraud involved recording transactions early so the revenue would be included in the current quarter’s financial reports, increasing the reported quarterly revenue. (Some of that revenue wasn’t real either.) Search the Enron messages for emails by employees containing topics about revenue, quarters, and loans. Present your results in a table with the name of the sender, the subject line of the email, and its date. You can adjust your query to give what you think are the most interesting results.\n(Employee names are available in the employeelist table, which lists names and email addresses for known Enron employees.)",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Full Text Search</span>"
    ]
  },
  {
    "objectID": "data-engineering/text-search.html#footnotes",
    "href": "data-engineering/text-search.html#footnotes",
    "title": "6  Full Text Search",
    "section": "",
    "text": "Aren’t bytes 8 bits each? Yes. But ASCII’s designers were worried about data transmission and storage, which both were expensive. Dropping a bit cut costs by 1/8th. Using 7 bits also allowed use of the 8th bit as an error correcting code to detect errors when text was transmitted between computers.↩︎\nIn case your company ever considers this, consider NIST SP 800-63A, a government standard describing security practices. Section 3.1.1.2 states that government login systems “SHALL NOT impose other composition rules (e.g., requiring mixtures of different character types) for passwords”, because (as Appendix A.3 notes) these rules usually result in people picking dumb passwords like “password1”.↩︎",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Full Text Search</span>"
    ]
  },
  {
    "objectID": "data-engineering/cloud.html",
    "href": "data-engineering/cloud.html",
    "title": "7  Cloud Computing",
    "section": "",
    "text": "7.1 The olden days\nNowadays, if you work on a large, complex dataset that can’t be analyzed on your laptop, you probably use The Cloud. Companies run their websites from The Cloud, host their databases in The Cloud, and store their files in The Cloud. Giant neural networks are trained by spending millions of dollars in The Cloud.\nSo what is The Cloud? Fundamentally, it’s just a computer rented from someone else.\nBut to understand what we’re doing with The Cloud, we should perhaps go back a bit and talk about how you rent computers, and what cloud computing allows us to do.\nSuppose it’s 2005 and you work at a company that runs a large online store. You need:\nDepending on the size of your company, that might involve dozens of servers each running different software. That’s too many computers to simply buy ordinary desktop computers and stick them under your desk. Instead, you’d get specially designed “rackmount” servers, which look like 19” wide pizza boxes that can slide into a specially designed metal rack that can fit 30 or 40 machines at a time.\nIf your company is small, you might stick the rack in a suitable closet, add extra air-conditioning and power outlets, and connect them all to the Internet with the fastest connection you can afford. As your company gets larger, that gets impractical for several reasons:\nSo instead you might outsource your computing needs. Many companies run “data centers”, which are just big warehouses with fancy air conditioning, backup batteries and generators, high-speed internet connections, staff technicians, and plenty of racks with space for servers. Many of these rent out space, so you can buy servers and have them installed in their data center.\nThis is called “colocation”, because your server is located in someone else’s facility, and it was a popular approach. You still have to set up the servers and manage all the software, but the data center staff can help you replace hard drives, install new servers into racks, and so on. Many companies rent space at many data centers.\nBut there are several weaknesses to this approach:\nSo a variety of alternatives appeared. The simplest one was rental: You could rent a so-called “dedicated server”, which was basically a server that a data center already had sitting in a rack, ready to go. For a monthly fee (based on the size and speed of the server), you could get access to log in and install whatever you wanted. Don’t need it any more? Stop paying, and they’ll rent it to someone else. Need a new server? The data center has spares waiting for customers, so you can rent one. All you need is a staffer with a credit card to fill out the form to rent more servers.\nSoon even more options appeared. Many were based on virtualization: Using special software so that one large server could run several operating systems at the same time, each of which gets a fixed chunk of the disk and memory, and does not have access to the others. This could be more efficient for the data centers. Instead of renting servers of all sizes, to suit different customers, they could buy a bunch of high-end servers with lots of CPUs, lots of memory, and very big disks. Then they could rent out slices of these servers to different customers. These slices were called “virtual private servers”, or VPSs.\nIn 2005, then, your company might have chosen to colocate servers, to rent some servers, or to rent some VPSs, depending on its needs. Maybe its core infrastructure was colocated servers, but some servers were rented as needed when new projects started.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cloud Computing</span>"
    ]
  },
  {
    "objectID": "data-engineering/cloud.html#the-olden-days",
    "href": "data-engineering/cloud.html#the-olden-days",
    "title": "7  Cloud Computing",
    "section": "",
    "text": "Servers to display the website to customers\nStorage for millions of product images, manuals, and brochures\nA database of product information\nA database of customers and sales\nLogs of customer activity, search terms, product popularity, and so on\nA system to send newsletters, product announcements, and sales to customers by email\nServers to run automatic sales and accounting reports\nServers that data analysts can use to run their programs analyzing sales data\n\n\n\n\n\nServers running Wikipedia. Photo by Victor Grigas, CC-BY-SA\n\n\n\n\nComputers generate a lot of waste heat, so you need more air conditioning than an ordinary office or conference room would.\nComputers use a lot of power, so you need an electrician to do new wiring; they also don’t like power outages, so you might want a generator or batteries.\nThe more computers you have, the more time you’ll spend dealing with hardware failures. Disk drives, power supplies, and other components tend to fail, and when you have dozens of machines, you’ll be fixing lots of servers.\nSick of managing software updates on your laptop? Imagine having 100 machines that all want to update, and all have different things installed!\nNow you need employees with expertise in hardware, computer repairs, networking, systems administration, and other things your company doesn’t know much about.\n\n\n\n\n\nYou still have to set up all the software and configure the servers. That can be a very complicated task, particularly when you’re running a large service. How do you arrange load-balancing so that requests to your website get split among a hundred servers? How do you do automated backups of all your machines? How do you even keep track of all the servers you have and what they do?\nYou also need a team of people with that expertise, and people are expensive.\nAdding new servers takes time. You have to order them from a supplier, wait for them to arrive at the data center, have them installed in a rack, start installing all the necessary software, and so on—which can take weeks.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cloud Computing</span>"
    ]
  },
  {
    "objectID": "data-engineering/cloud.html#rental-computing",
    "href": "data-engineering/cloud.html#rental-computing",
    "title": "7  Cloud Computing",
    "section": "7.2 Rental computing",
    "text": "7.2 Rental computing\nBy about 2007, rented servers were maturing into a new category: cloud computing. The name “cloud” came from the ultimate goal: instead of dealing with real physical servers, there’d just be a bunch of computing resources available on-demand, for a fee. If you were drawing a diagram of your system infrastructure, you could just draw a cloud-shaped blob in the middle where the computing goes.\nCloud computing builds on the rental framework with several additional features:\n\nMinute-by-minute billing. Instead of renting servers a month at a time, you can rent them for just the time you need them.\nFast setup. If you want more servers or services, you can get them added to your account in minutes. (Also, you previously learned about container systems like Docker. Combine these with cloud computing and it’s easy to get a server running all your software in a container, in minutes.)\nIntegrated computing products. Need a database? It’s a pain to rent a server, install PostgreSQL on it, adjust all the settings, keep it updated, set up backups… why not just rent a PostgreSQL database? Cloud computing companies offer all kinds of integrated services for rent, not just bare servers.\nAccounting systems for keeping track of all your servers and what they do. Many of these systems are built for large companies that have many teams, where you need to keep track of who has the servers, and maybe need to set usage quotas and billing details separately for different parts of the company.\nAPIs to automatically rent more services, shut machines off, change their settings, and so on—so instead of using forms to request services, your Python program can do it automatically.\n\nFrom about 2010 onward, companies rapidly moved their computing over to The Cloud. It offered many advantages: great flexibility, less hardware maintenance (all handled by the cloud provider), faster setup, greater reliability… and as cloud providers can specialize in developing computing services, they could build more sophisticated services than a company could typically build on its own. Rapidly growing companies could quickly rent more services, or even scale services up and down automatically as needed to meet customer demand.\nThere are, of course, disadvantages. Renting a basic server, without any of the fancy services, is much more expensive than just buying one. Different cloud providers offer different services, so once you’ve built a system using one, it’s hard to switch to another. And while it’s nice to automatically rent more servers if your site is suddenly overloaded, it’s also easy to wake up one morning and find a $100,000 bill because one of your services got popular overnight. Or to forget to turn off services you’re not using, and pay for months of “use” you never needed. Anyone at large companies with thousands of cloud servers has stories of systems consuming thousands of dollars a month because someone forgot to turn them off.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cloud Computing</span>"
    ]
  },
  {
    "objectID": "data-engineering/cloud.html#common-products",
    "href": "data-engineering/cloud.html#common-products",
    "title": "7  Cloud Computing",
    "section": "7.3 Common products",
    "text": "7.3 Common products\nThere are several major cloud providers. Amazon is the most well-known with its Amazon Web Services system, but Microsoft Azure and Google Cloud are competing heavily, and there are many other companies offering cloud services, including IBM, Rackspace, Alibaba, and many others.\nEach has its own set of services, but there are common ones available in different forms by many providers:\n\nRented servers. Amazon calls this the Elastic Compute Cloud (EC2); Google calls this the Compute Engine; but in each case, you specify what memory and processor you want, and you get a machine to rent, just like the old VPS services.\nObject storage. Here “object” typically means “file”; but instead of offering a hard drive with lots of space, cloud providers build object storage systems. These let you organize lots of files into groups, and have the cloud provider automatically store them in a redundant way so they aren’t lost when a hard drive fails. Objects are typically available through an API you can access from various programming languages.\nDatabases. Rent a SQL database where all the software is set up for you, and you just pay by the hour.\n\nIn recent years, many cloud providers have created products focused on data science and machine learning. Many have products based on Apache Spark; many have deep learning products that can store your trained models and produce predictions on request; some even have point-and-click tools that will fit machine learning models to your data without requiring you to write any code.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cloud Computing</span>"
    ]
  },
  {
    "objectID": "data-engineering/distributed-data.html",
    "href": "data-engineering/distributed-data.html",
    "title": "8  Distributed Data and Computation",
    "section": "",
    "text": "8.1 Distributing data\nIn large companies or large scientific projects, there’s problem of scale: the amount of data we want to process is very large, so we divide it up to process on multiple machines. This is Big Data.\nThink of it this way: if you have one server with loads of RAM, a big hard drive, and a top-of-the-line CPU with many cores, and your data fits easily on that server, you’re probably best off writing parallel code and using that server. But if the data is going to be too large—or you’re receiving new data all the time—instead of buying an even bigger server, it might make more sense to buy a fleet of many smaller servers.\nComputing on many independent machines is distributed computing. It brings many challenges over just using a single machine:\nEach of these is a difficult problem. There is an ecosystem of software built to solve these problems, from the Hadoop Distributed File System to Spark to Hive to Mahout. But before we learn what the buzzwords mean, let’s talk about the structure of the problems.\nStatisticians tend to think of datasets as simple things: maybe a few CSV files, a folder full of text files, or a big pile of images. Usually our biggest storage challenge is that we have to pay for extra Dropbox storage space to share the dataset with our collaborators. If datasets change, we usually just get some new data files or a few corrections.\nBut this is not what happens in industry. Consider a few examples:\nThere are a few common features here:\nSaving to a big CSV file simply is not going to scale. Large companies may have terabytes or petabytes of data stored, with gigabytes more arriving each day. We’re also fundamentally limited by how quickly we can write data to a single hard drive (and afraid of what would happen if that drive fails).\nThis is where a distributed file system is useful.\nIn a distributed file system, data is\nTo achieve this, distributed file systems usually have several parts:\nThere are two common strategies in the distributed file systems you might use:\nFortunately for users, these distributed file systems are designed to look like any other file system you might use, and work in much the same way.\nA common block-based file system is the Hadoop Distributed File System, HDFS, though there are many others. Amazon S3 (Simple Storage Service) is a distributed object-based file system as a service, letting you send arbitrary objects to be stored on Amazon’s servers and retrieved whenever you want.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributed Data and Computation</span>"
    ]
  },
  {
    "objectID": "data-engineering/distributed-data.html#sec-distributing-data",
    "href": "data-engineering/distributed-data.html#sec-distributing-data",
    "title": "8  Distributed Data and Computation",
    "section": "",
    "text": "A web search engine. Search engines run “crawlers,” programs that browse web pages, extract text and links from the HTML, and process the data to form a search index. There may be thousands of crawlers running on different machines at once (the Web is huge!), each producing vast quantities of data, and the search index must be updated with the new data very quickly to stay relevant.\nA shopping site. A large shopping site might take many customer orders per minute, and have thousands of people browsing products all the time. To measure demand, adjust search results, recommend relevant products, and decide how much of each product to order for the warehouse, all this activity must be processed at least daily.\nAn ad network. Advertisements online are weird. When you visit a webpage containing ads, a tiny auction happens instantaneously: data on the webpage and about you is sent to different ad companies, running programs that submit bids on how much they’re willing to pay to show you an ad. An advertiser collects vast data on consumers, what pages they visit, and what ad campaigns are most profitable, and must crunch this data so it can instantaneously decide how much a particular pair of eyeballs is worth.\nA retailer. A big chain like Walmart has hundreds or thousands of stores selling tens of thousands of different products. New sales happen every second, and data arrives about warehouse supplies, pricing changes from suppliers, online searches, and so on. Pricing needs to be adjusted, products ordered, ad campaigns mailed to relevant customers, and executives keep asking analysts to make charts justifying their latest big ideas.\n\n\n\nNew data arrives regularly or continuously.\nNew data is being produced from many sources simultaneously.\nThe analysis must be updated regularly or it will no longer be useful.\nThe analysis may involve large amounts of computation that can only be done in parallel for it to be finished in time.\nThe incoming data is vast, and so is the archive.\n\n\n\n\n\nSpread across multiple machines (often dozens or hundreds). Each server has its own large and fast hard drives containing a subset of the full data.\nStored redundantly. Each new data file is stored on several different servers, to prevent it from being lost if a hard drive fails or a power supply lets out its magic smoke.\nStored near where it is useful. Ideally, the machine where the data is processed is near the particular machine where it is stored, to avoid congesting the network with data files being sent back and forth.\nCoherent. It shouldn’t be possible to change a file and have the change appear in one copy but not another redundant copy.\nAvailable in quantity. It’s often assumed that rather than wanting small parts of many files, applications will want to receive huge chunks of single files; distributed systems may not be very fast if you try accessing many small files very quickly.\n\n\n\nA bunch of servers with hard drives. Each server stores some chunk of files.\nA coordination server (or servers) that tracks the data servers and knows where every file is stored, so users can fetch data from the right servers.\nSome kind of library or client software for connecting to the distributed file system and accessing its data.\n\n\n\nBlock-based file systems: In HDFS, each file is split up into blocks, and the blocks are sent to different machines for storage. A 1 GB file might be split into 10 blocks of about 128 MB each, and those blocks might live on the same machines or different machines, based on HDFS settings and automatic criteria.\nObject-based file systems: Amazon S3 is a cloud service that stores files as objects, meaning each file is one unit that is stored and can be read. Files are not split up into blocks.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributed Data and Computation</span>"
    ]
  },
  {
    "objectID": "data-engineering/distributed-data.html#distributing-computation",
    "href": "data-engineering/distributed-data.html#distributing-computation",
    "title": "8  Distributed Data and Computation",
    "section": "8.2 Distributing computation",
    "text": "8.2 Distributing computation\nReading the above, you might think: Why not use a big relational database? As we discussed, these are designed to keep large amounts of data coherent and consistent, and to make the data available quickly on request.\nThe simplest answer is that sometimes, the data volume is more than any one database server can handle.1 But the better answer is that sometimes the computation is more than any one server can handle.\nAs we have seen, relational databases can do surprisingly complex calculations. But those calculations are limited by the size of the server (try writing a SQL query that aggregates 100GB of data on a server with 16GB of RAM!), and of course are limited by what can be expressed in the SQL language. If you want to write a calculation that works on several terabytes of data, be prepared for your relational database server to grind to a halt for hours while it processes the query.\nBut some computational tasks can be split into pieces. We can do the calculations for each piece separately, then combine the results together when we’re done. And all those calculations can be done at the same time, if we have enough computers with access to the right chunks of data.\nHence distributed computing and distributed data fit together. If we’ve built a system for spreading data across many machines, and we also have analytic tasks that can operate on chunks of data and then combine their results, we can use a distributed data system to do large-scale calculations that couldn’t be done on a single machine.\nSo how do we divide up computing tasks to be run on multiple machines, loading their input data from a distributed file system?\nThere are several conceptual models we could use. The first is MapReduce.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributed Data and Computation</span>"
    ]
  },
  {
    "objectID": "data-engineering/distributed-data.html#mapreduce",
    "href": "data-engineering/distributed-data.html#mapreduce",
    "title": "8  Distributed Data and Computation",
    "section": "8.3 MapReduce",
    "text": "8.3 MapReduce\nMapReduce dates to 2004, when Google engineers published a paper advertising the MapReduce concept as “Simplified Data Processing on Large Clusters”. By 2006 the Hadoop project existed to make an open-source implementation of the MapReduce ideas, and Hadoop soon exploded: it was the single largest buzzword of the early 2010s, being the canonical Big Data system.\nLet’s talk about the conceptual ideas before we discuss the implementation details.\nMapReduce uses the Map and Reduce operations you may have learned in functional programming, but with an extra twist: Shuffle. A MapReduce system has many “nodes”—different servers running the software—that are connected to some kind of distributed file system.\n\nMap. Each node loads one chunk of data from the distributed file system, applies some function to that data, and writes the result to an output file. The result can have a “key”, some arbitrary identify.\nShuffle. The output files are moved around so that all data with the same key is on the same node. (Or, keys are assigned to nodes, and they fetch the data with that key from the distributed file system.)\nReduce. Each node applies another function to each chunk of output data, processing all chunks with the same key.\n\nThe output of all the reduction functions is aggregated into one big list.\nThe Map and Reduce steps are parallelized: each node processes the Map function simultaneously, and each node processes the Reduce function on each key simultaneously.\nThe Shuffle step lets us do reductions that aren’t completely associative.\n\nExample 8.1 (Counting words) The standard MapReduce example is counting words in a huge set of documents. Stealing pseudocode from Wikipedia,\nfunction map(String name, String document):\n    // name: document name\n    // document: document contents\n    for each word w in document:\n        emit (w, 1)\n\nfunction reduce(String word, Iterator partialCounts):\n    // word: a word\n    // partialCounts: a list of aggregated partial counts\n    sum = 0\n    for each pc in partialCounts:\n        sum += pc\n    emit (word, sum)\nHere emit (w, 1) means w is the key for the value 1.\nIf we supply these to a MapReduce system that has many documents on many nodes, each node can apply map() to each document it stores. The emitted results are shuffled so each node contains all the output for specific words. Then each node runs reduce() for each word, and gets a word count.\n\n\nExercise 8.1 (k-means clustering) Word counting doesn’t really give you a sense of the scale of MapReduce, so let’s consider a more statistical example: parallel K-means clustering.\nFirst, a brief review of K-means. We have \\(n\\) points (in some \\(d\\)-dimensional space) we want to cluster into \\(k\\) clusters. We randomly select \\(k\\) points and use their locations as cluster centers. Then:\n\nAssign each point to a cluster, based on the distance between it and the cluster centers.\nTake all the points in each cluster and set that cluster’s center to be the mean of the points.\nReturn to step 1.\n\nThe distance calculations are the scaling problem here: each iteration requires calculating the distance between all \\(n\\) points and all \\(k\\) cluster centers.\nHow could we do K-means as a sequence of MapReduce operations?\n\n\n8.3.1 Apache Hadoop\nApache Hadoop is an implementation of the MapReduce idea, in the same way that PostgreSQL or SQLite are implementations of the SQL idea. It’s built in Java.\nHadoop handles the details: it splits up the data, assigns data chunks to compute nodes, calls the Map function with the right data chunks, collects together output with the same key, provides output to the Reduce function, and handles all the communication between nodes. You need only write the Map and Reduce functions and let Hadoop do the hard work.\nIt’s easiest to write map and reduce functions in Java, but Hadoop also provides ways to specify “run this arbitrary program to do the Map”, so you can have your favorite R or Python code do the analysis.\nHadoop can be used to coordinate clusters of hundreds or thousands of servers running the Hadoop software, to which MapReduce tasks will be distributed.\n\n\n8.3.2 Limitations\nHadoop was incredibly popular for quite a while, but it has its limitations.\n\nHadoop relies heavily on the distributed file system, storing all its intermediate results on disk. That can make it slow.\nThe MapReduce paradigm is quite restricted. There are extra features—like a Combiner step that operates on the Map’s output before it’s Reduced—but it can still be tricky to turn your task into a MapReduce operation, and it forces everything to be done in a certain order.\nMapReduce isn’t very good for interactive use, when you just want to query your data and try stuff. It works in batches, reading data files from disk and spitting out new results to disk.\nHadoop was cool for long enough that its buzzword value was wearing off.\n\nIt would be nice to have something more flexible: something where we can write complicated programs and automatically have the work split up between nodes, but without having to structure our problem as a series of Map and Reduce steps. And that’s where Spark comes in.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributed Data and Computation</span>"
    ]
  },
  {
    "objectID": "data-engineering/distributed-data.html#footnotes",
    "href": "data-engineering/distributed-data.html#footnotes",
    "title": "8  Distributed Data and Computation",
    "section": "",
    "text": "The cynical answer is that software engineers need to put more things on their résumés, so they adopt new technologies every month.↩︎",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributed Data and Computation</span>"
    ]
  },
  {
    "objectID": "data-engineering/project.html",
    "href": "data-engineering/project.html",
    "title": "9  Project: Data Pipeline",
    "section": "",
    "text": "9.1 Logistics\nIn this project, you will develop a data pipeline for ingesting messy, unstructured data, producing a nice and structured SQL database, and automatically generating reports.\nThere are two datasets to choose between. For each, the source data is somewhat messy, so you will work to organize it into a convenient format to facilitate reporting and querying. The project is meant to imitate a common situation you will face in industry: your product or business will be spewing a firehose of messy data, and it’ll be your job to turn it into a sprinkler of insight.\nThe two datasets:\nThis project will be completed in assigned groups of 2 or 3 students. Group assignments will be posted on Canvas. There will be several milestones throughout the semester when you will turn in parts of the work; at the end of the semester, you will turn in the completed project.\nThe groups will be paired: your group is matched with another group. You must use a different dataset from your matched group.\nWhile completing the project, you will also be working on a knowledge transfer activity in 36-611 based on this project. Just as in 36-613, I’ll be grading your technical work, and the knowledge transfer activity will be part of your 36-611 grade. The knowledge transfer will be with your paired group.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Project: Data Pipeline</span>"
    ]
  },
  {
    "objectID": "data-engineering/project.html#part-1-designing-the-database",
    "href": "data-engineering/project.html#part-1-designing-the-database",
    "title": "9  Project: Data Pipeline",
    "section": "9.2 Part 1: Designing the database",
    "text": "9.2 Part 1: Designing the database\nThis part is due Wednesday, November 6 at 5pm.\nIn Part 1 of the project, your team must design a table schema for the data. By “table schema” I mean the CREATE TABLE statements necessary to create database tables that fit the data. You should follow the design principles in Section 4.1 to build a normalized structure for the database that minimizes redundant information. Include primary keys, foreign keys, column types, and any appropriate constraints. It is up to you to decide how many tables you need, their names, and their contents.\nWrite your CREATE TABLE statements in a notebook. Test them out on Azure to ensure they work correctly. You do not need to load any real data into the database yet.\nIn the notebook, write comments explaining the following: What are the basic entities in your schema? (In Example 4.1, entities were things like songs, record labels, and albums, that each had their own database table.) How did you choose them and what did you do to ensure there is not redundant information in your database?\nYou may find it useful to make a Git repository to share with your project team, though this is not required. Instead you will turn in your schema and explanation on Gradescope.\nThe rest of the instructions are split up by the dataset your group is using:\n\n9.2.1 HHS data\nThe HHS data files have many columns; we won’t be interested in all of them here. The data contains the following information:\n\nA unique ID for each hospital (hospital_pk, a string)\nThe state the hospital is in (state, as a two-letter abbreviation, like PA)\nThe hospital’s name (hospital_name), street address (address), city (city), ZIP code (zip), and FIPS code (fips_code, a unique identifier for counties)\nThe latitude and longitude of the hospital (geocoded_hospital_address), formatted as a string like POINT(-91 30), where the first number is the longitude and the second is the latitude. When you load data, you will need to convert this to a format you can use1\nThe week this observation is for (collection_week)\nThe total number of hospital beds available each week, broken down into adult and pediatric (children) beds (all_adult_hospital_beds_7_day_avg, all_pediatric_inpatient_beds_7_day_avg). This can change weekly depending on staffing and facilities.\nThe number of hospital beds that are in use each week (all_adult_hospital_inpatient_bed_occupied_7_day_avg, all_pediatric_inpatient_bed_occupied_7_day_avg)\nThe number of ICU (intensive care unit) beds available and the number in use (total_icu_beds_7_day_avg and icu_beds_used_7_day_avg)\nThe number of patients hospitalized who have confirmed COVID (inpatient_beds_used_covid_7_day_avg)\nThe number of adult ICU patients who have confirmed COVID (staffed_icu_adult_patients_confirmed_covid_7_day_avg)\n\nThe data is updated weekly. In each weekly file I will provide you, each row will be one hospital, and all of the columns above will be present—so each hospital’s address, location, and so on will appear every week.\nThere are several thousand hospitals in the United States, and this data has been updated weekly for much of the pandemic, so the data contains about 580,000 rows. In raw form, with dozens of columns, it is 257 MB.\nYou will also be using a hospital quality dataset from the Centers for Medicare and Medicaid Services (CMS). We are interested in the following information in this data:\n\nA facility ID, which matches the hospital_pk in the HHS data\nThe facility’s name, address, city, state, ZIP code, and county\nThe type of hospital and its type of ownership (government, private, non-profit, etc.)\nWhether the hospital provides emergency services\nThe hospital’s overall quality rating. This quality rating is updated several times a year, and we want to be able to track each version of the quality rating. For instance, we might ask “What was the quality rating of this hospital in 2020?” and compare it to the rating in 2022.\n\n\n\n9.2.2 College Scorecard\nThe College Scorecard data files cover dozens of variables per institution; we won’t be interested in all of them here. The Data Dictionary lists all variables, their column names (VARIABLE NAME), a human readable description, and the meaning of each value. We care about the following columns:\n\nUNITID, the institution ID\nACCREDAGENCY\nPREDDEG\nHIGHDEG\nCONTROL\nREGION\nCCBASIC (TODO: this is missing before 2022; prefer IPEDS version?)\nADM_RATE\nTUITIONFEE_IN, TUITIONFEE_OUT, and TUITIONFEE_PROG\nTUITFTE\nAVGFACSAL\nCDR2 and CDR3\nScroll through the list (there are nearly 3,500 variables!) and pick some additional variables that could be interesting to analyze.\n\nThe data is updated annually. Your database should be able to store each year’s data for each university, so you can quickly look up statistics for a university in any particular year.\nYou will also be using supplementary data from the Integrated Postsecondary Education Data System (IPEDS); specifically, the directory information files. These provide annual directory information and other statistics based on surveys of institutions. Again, since this is updated annually, you must be able to track each version of the data and the dates it applies to.\nFrom the IPEDS data, obtain:\n\nAll information about the institution’s name, location, address, and similar\nAll Carnegie Classification 2021 variables\nThe Census identifiers that apply to it: Core Based Statistical Area (CBSA) and its type, the Combined Statistical Area (CSA), and the county FIPS code\nLatitude and longitude of the institution.\n\nMost of these values will not change from year to year, but they can, since colleges can change names, move, grow, or change.\nThe College Scorecard data also includes crosswalk files giving links between OPEIDs (in the College Scorecard data) and UNITIDs (in the IPEDS) data, so you can match between datasets.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Project: Data Pipeline</span>"
    ]
  },
  {
    "objectID": "data-engineering/project.html#part-2-loading-data",
    "href": "data-engineering/project.html#part-2-loading-data",
    "title": "9  Project: Data Pipeline",
    "section": "9.3 Part 2: Loading data",
    "text": "9.3 Part 2: Loading data\n\n\n\n\n\n\nNote\n\n\n\nThis part is due Friday, November 15th at 5pm. Plan ahead so you can use the Tuesday and Friday office hours.\n\n\nNow we need to write Python code to load data each week. This code will handle any conversions, cleaning, and reformatting of the data, and then run the necessary INSERT commands.\nThe necessary data files will be posted on Canvas. Do not use the original files from the sources; I am providing a few specific subsets to make your task easier.\n\n9.3.1 HHS data\n\n9.3.1.1 Weekly updates\nI will provide the HHS data to you in the form of files containing new data for each week. Each CSV file will cover one week of data. Your first script will load one file into your SQL database.\nThe script should be run from the command line like this:\npython load-hhs.py 2022-01-04-hhs-data.csv\nwhere the argument is the name of the file to load from. The script should then:\n\nLoad the CSV file.\nDo any necessary processing (such as converting -999 to None or NULL, parsing dates into Python date objects, etc.).\nExecute a series of INSERT commands to insert the relevant data into the database.\n\n\n\n9.3.1.2 CMS quality data\nSimilarly, you will write a script to load the quality data whenever it may be updated. I’ll provide you several example files you will load. The script should run from the command line like this:\npython load-quality.py 2021-07-01 Hospital_General_Information-2021-07.csv\nHere the first argument is the date of this quality data (YYYY-mm-dd) and the second argument is the CSV file name. Again, your Python script will do any necessary work and then INSERT the data.\nIf you chose to have a separate hospitals table, referenced by the tables storing the weekly updates or quality data, it will need to be updated whenever there’s a new hospital. Your scripts should automatically insert new hospitals whenever they appear. If the new quality data changes key hospital metadata (like its location), you should update your hospital data.\n\n\n\n9.3.2 College Scorecard\nI will provide you College Scorecard and IPEDS data files containing data for each year.\nYou should create two Python scripts that can be run from the command line, like so:\npython load-scorecard.py TODO-name-of-file.csv\npython load-ipeds.py TODO-name-of-file.csv\nwhere the arguments are the names of the files to load from. The scripts should then:\n\nLoad the CSV file.\nDo any necessary processing (such as converting -999 to None or NULL, parsing dates into Python date objects, etc.).\nExecute a series of INSERT commands to insert the relevant data into the database.\n\n\n\n9.3.3 Data processing requirements\n\n\n\n\n\n\nNote\n\n\n\nThese requirements apply to both project datasets.\n\n\nYour scripts should have the following common features:\n\nThey should print out a summary of how much data has been read from the CSV and how much has been successfully inserted into the database. This should allow the user to determine whether the amount of data loaded matches what they expect.\nIf the script gets partway through and crashes with an error, either all the data should be inserted or none of it. You should not be able to insert only half the data, then have to manually remove it before you can run the script again to reload the rest of the data.\nIf a row is invalid and rejected by Postgres (because a constraint failed, for instance), your script should print an error message identifying which row failed and giving information about it (such as the hospital or college affected) that helps the user figure out what’s wrong. Then your script should stop so the user can fix the issue in the data before re-running the script.\n\n\n\n9.3.4 Formatting and style requirements\nAdditionally, this work will be part of your knowledge transfer activity in 611. To facilitate transfer, you should ensure that:\n\nyour code is clearly written, separated into small understandable functions, as discussed in 36-650\nyou follow standard PEP 8 style rules for naming and formatting\nyou have docstrings for each function explaining what it does, following PEP 257 conventions\nyour code also includes a README file describing how to use your scripts to load the data\n\nIf you choose, you may create additional Python files containing functions and classes used by your data loading scripts.\n\n\n9.3.5 Submission details\nTo submit your code, create a private GitHub repository shared with your classmates. We will ask you to add us (capnrefsmmat and aerosengart) as collaborators, and you will submit a link to the repository as your Canvas submission for this part of the project.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Project: Data Pipeline</span>"
    ]
  },
  {
    "objectID": "data-engineering/project.html#part-3-analytics-and-reporting",
    "href": "data-engineering/project.html#part-3-analytics-and-reporting",
    "title": "9  Project: Data Pipeline",
    "section": "9.4 Part 3: Analytics and reporting",
    "text": "9.4 Part 3: Analytics and reporting\n\n\n\n\n\n\nNote\n\n\n\nThis part is due Friday, December 6th at 5pm. Plan ahead so you can use the Tuesday and Thursday office hours.\n\n\nYour final task for this semester is to develop an automatic reporting system. A user should be able to run the reporting system, specify a date of interest, and obtain a report or dashboard summarizing the data as of the selected date.\n\n9.4.1 Report contents\nYour reports should be graphical dashboards summarizing the data and interesting features it contains. You do not need to do any statistical modeling, although you can if you want. Descriptive tables and graphs are sufficient, since the purpose of this project is to give you experience working with real data in SQL.\n\n9.4.1.1 HHS data\nThe HHS report should summarize the COVID situation suggested by the data on hospital beds: how full are hospitals in each state? What states have the fewest open beds? Where are beds filling up the fastest?\nYour report/dashboard should include:\n\nA summary of how many hospital records were loaded in the week selected by the user, and how that compares to previous weeks.\nA table summarizing the number of adult and pediatric beds available that week, the number used, and the number used by patients with COVID, compared to the 4 most recent weeks.\nA graph or table summarizing the fraction of beds in use by hospital quality rating, so we can compare high-quality and low-quality hospitals.\nA plot of the total number of hospital beds used per week, over all time up to the selected week, split into all cases and COVID cases.\n\nAdditionally, you should include three additional interesting analyses. At least two of these should involve combining multiple tables from your database. These could be anything that can be expressed as a table or graph and that is potentially interesting. Some examples include:\n\nA map showing the number of COVID cases by state (the first two digits of a hospital FIPS code is its state)\nA table of the states in which the number of cases has increased by the most since last week\nA table of the hospitals (including names and locations) with the largest changes in COVID cases in the last week\nA table of hospitals that did not report any data in the past week, their names, and the date they most recently reported data\nGraphs of hospital utilization (the percent of available beds being used) by state, or by type of hospital (private or public), over time\n\nYou are free to think of other interesting plots, tables, and maps, as long as you think they could be useful to the Department of Health and Human Services for some health-related purpose.\nIf it helps, imagine sending this report to the Assistant to the Deputy Director of the Department of Health and Human Services. The Assistant is a doctor, not a statistician or programmer, so make your report accordingly.\n\n\n9.4.1.2 College Scorecard\nThe College Scorecard report should summarize the current data, highlighting the best- and worst-performing colleges according to financial results, and also summarize how the data has changed since the previous year. The user should be able to select a year of interest and get the report generated for that year.\nYour report/dashboard should include:\n\nSummaries of how many colleges and universities are included in the data for the selected year, by state and type of institution (private, public, for-profit, and so on).\nSummaries of current college tuition rates, by state and Carnegie Classification of institution.\nA table showing the best- and worst-performing institutions by loan repayment rates.\nGraphs showing how tuition rates and loan repayment rates have changed over time, either in aggregate (such as averages for all institutions by type) or for selected institutions (such as the most expensive).\n\nAdditionally, you should include three additional interesting analyses. At least two of these should involve combining multiple tables from your database. These could be anything that can be expressed as a table or graph and that is potentially interesting. Some examples include:\n\nMaps of tuition or loan repayment rates across the country\nGraphs comparing tuition, loan repayment rates, and faculty salaries across institution, and demonstrating their correlation\nA table of institutions that did not report data in the selected year but did previously, or that are new this year\nGraphs of total enrollment or total tuition costs over time\n\nIf it helps, imagine sending this report to the Assistant to the Deputy Director of the Department of Education. The Assistant has a degree in educational administration, not in statistics, so make your report accordingly.\n\n\n\n9.4.2 Report format\nThe report should be output in the form of a document with graphs, tables, and text. This could be an HTML file, a PDF, a Jupyter notebook (built automatically with the latest data), or even an interactive dashboard made with something like Streamlit.\n\n\n9.4.3 Mechanical requirements\nYour reporting system must be automated. This means it cannot require the user to manually edit a file or notebook every week. Ideally, it could be run with a single command:\npython weekly-report.py 2022-09-30 # for HHS data\npython education-report.py 2021 # for College Scorecard\nThat might produce an output file called report-2022-09-30.pdf or report-2021.html; for an interactive dashboard, it might open up the dashboard for that date.\nIf your report is in the form of a notebook, you can produce it from the command line using papermill, which lets you pass command-line arguments to Jupyter notebooks. For example, if you had a week parameter in your code, you could run\npapermill weekly-report.ipynb 2022-09-30-report.ipynb -p week 2022-09-30\njupyter nbconvert --no-input --to html 2022-09-30-report.ipynb \nThe first command substitutes week = \"2022-09-30\" into the parameters of the notebook, and the second executes the notebook and produces the HTML output. The --no-input argument exports only the output, not the code in the notebook. Read the papermill usage guide for details on how to accept parameters in a notebook.\nYour reporting system must be built on SQL, using a SQL connection to generate the results. The calculations must be primarily done in SQL, with a minimum of post-processing in Python to format the results for display. You should not, at any point, load the entire database table into Python so you can do your analysis in Python alone.\nYour reporting system should assume the user has already run your data loading scripts from Part 2; it does not need to load all the data.\n\n\n9.4.4 Format requirements\nThe report must be designed for an end-user: to the extent you can, it should hide the code and technical details, and present useful tables and graphs of results. Format the output nicely! Do not simply print out lists of Python output. Make graphs and tables. If you’re printing a data frame, consider the Pandas data frame formatting options that let you print it as a nice table.\n\n\n9.4.5 Submission details\nAdd your code to the GitHub repository you created in previous part. Make sure this code is committed and pushed by the deadline.\nLoad all the data files provided on Canvas and then generate a report for the final week of data. Convert this report to a PDF and submit it on Gradescope. This serves as an example of what your code produces.",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Project: Data Pipeline</span>"
    ]
  },
  {
    "objectID": "data-engineering/project.html#footnotes",
    "href": "data-engineering/project.html#footnotes",
    "title": "9  Project: Data Pipeline",
    "section": "",
    "text": "The coolest option would be to use PostGIS, which extends Postgres to understand geographic coordinate systems. But that is overkill for us here, and is not installed on our server, so you can’t use it. A couple simple columns would be fine.↩︎",
    "crumbs": [
      "Data Engineering and Distributed Environments",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Project: Data Pipeline</span>"
    ]
  }
]