[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "large-scale-data/project.html",
    "href": "large-scale-data/project.html",
    "title": "Project: Distributed Data Analysis",
    "section": "",
    "text": "In this project, you will learn to use Spark to clean a large dataset, do feature engineering, do analytics reporting, and fit machine learning models.\nWe will work with a large dataset of US airline flights. You may have used a subset of this data in a course before, but here we’ll use the full data for ten years. We’ll begin by using Spark to aggregate and summarize the data, before moving on to feature engineering and building models to predict on-time performance.\n\n\nThis project will be completed in assigned groups of 2 or 3 students. Group assignments will be posted on Canvas. There will be several milestones throughout the semester when you will turn in parts of the work; at the end of the mini, you will turn in the completed project.\n\n\n\nThe dataset is loaded in Parquet files on our Spark cluster. They live in the sampledata container of the madsdatastore storage account in Azure Data Lake Storage. There is one CSV file per year, but fortunately Spark can read them all at once:\ndelays = spark.read.csv(\"abfss://sampledata@lsd2025storage.dfs.core.windows.net/delays\",\n                        header=True, nullValue=\"NULL\")\nThis will only work in the Class Shared Cluster, since it is configured with the necessary keys to have access to the data.\nThe data includes the following columns:\n\n\n\nVariable\nMeaning\n\n\n\n\nFL_DATE\nDate of flight (YYYY-mm-dd)\n\n\nOP_CARRIER\nCode assigned by International Air Transport Association to identify each airline; Wikipedia has a full table of them.\n\n\nOP_CARRIER_FL_NUM\nFlight number assigned by this carrier for the flight.\n\n\nORIGIN\nIATA code for the airport the flight left from. Wikipedia has a list of airports by IATA code.\n\n\nDEST\nIATA code for the destination airport.\n\n\nCRS_DEP_TIME\nScheduled departure time (local time, hhmm).\n\n\nDEP_TIME\nActual departure time (local time, hhmm).\n\n\nDEP_DELAY\nDifference in minutes between scheduled and actual departure time. Early departures show negative numbers.\n\n\nTAXI_OUT\nTime taken to taxi out to the runway, in minutes.\n\n\nWHEELS_OFF\nTime the flight took off (local time, hhmm).\n\n\nWHEELS_ON\nTime the flight landed (local time, hhmm).\n\n\nTAXI_IN\nTime taken to taxi in to the gate, in minutes.\n\n\nCRS_ARR_TIME\nScheduled arrival time (local time, hhmm).\n\n\nARR_TIME\nActual arrival time (local time, hhmm).\n\n\nARR_DELAY\nDifference in minutes between scheduled and actual arrival time. Early arrivals show negative numbers.\n\n\nCANCELLED\n1 if the flight was cancelled.\n\n\nDIVERTED\n1 if the flight was diverted.\n\n\nCRS_ELAPSED_TIME\nScheduled duration of the flight, in minutes.\n\n\nACTUAL_ELAPSED_TIME\nElapsed time of flight, in minutes.\n\n\nAIR_TIME\nTime the flight was in the air, in minutes.\n\n\nDISTANCE\nDistance between origin and destination airports, in miles.\n\n\nCARRIER_DELAY\nLength of delay caused by the airline (e.g. maintenance, waiting for crew, cleaning the plane), in minutes. Note that if the flight was not delayed, these fields will be blank.\n\n\nWEATHER_DELAY\nLength of delay caused by weather, in minutes.\n\n\nNAS_DELAY\nLength of delay caused by the National Airspace System, including air traffic control, in minutes.\n\n\nSECURITY_DELAY\nLength of delay caused by a security problem, such as evacuation of a terminal or excessive security lines, in minutes.\n\n\nLATE_AIRCRAFT_DELAY\nLength of delay caused because the aircraft arrived late from its previous airport, in minutes.\n\n\n\n\n\n\nThis part is due Friday, February 7 at 5pm.\nFirst, create a notebook in the Databricks workspace for cleaning and formatting the data. Ensure you can load the data and consolidate it into one Spark DataFrame. Examine the columns and their types. Do any columns need to be converted to specific types? Review the Spark data types reference; the PySpark functions reference lists functions that can do conversion and manipulation.\nHave your notebook do all the necessary manipulation. At the end of the notebook, write your new data frame into the metastore. Choose a table name that is unique to your group, and write it with:\nyour_data_frame.write.saveAsTable(\"hive_metastore.default.your_group_table_name\")\nNext, create a new notebook in the Databricks workspace. In the notebook, calculate (using Spark) and present the following aggregates:\n\nA summary table or graph showing the number of flights per month across the history of the data, so you know you’ve loaded everything\nThe percentage of flights delayed per week, plotted over the entire length of the data\nThe number of delayed flights per week, by type of delay, plotted over time\nA table of air carriers, showing the number of flights scheduled, the number canceled, the percentage delayed, and the average delay among those delayed. Sort by total number of flights, so the biggest air carriers come first.\nA table of the top 50 airports by percentage of flights delayed, showing the airport code, percentage of flights delayed, and average number of flights per day\n\n\n\n\nThis part is due Friday, February 14 at 5pm.\nOur ultimate goal is to build a predictive model for flight delays. Given a flight and various features, your model should predict whether the flight is delayed for any reason. (We’ll count cancellation as a type of delay.)\nYou could use only the variables present in the data, but it is likely that you can derive new features that would be more useful.\nCreate a notebook that generates the following additional features for each observation:\n\nThe day of week (Monday-Sunday)\nRate of weather delays at the departure airport in the previous hour (the fraction of flights in the previous hour that were delayed due to weather)\nRate of weather delays from the arrival airport in the previous hour\nNumber of flights departing from the departure airport in the previous hour, compared the average number during this hour on the same day of the week, as a z score\nAt least two more features, calculated from the available data, that you think could be useful for your model\n\n\n\n\nThis part is due Friday, February 28 at 5pm.\nNow we aim to use Spark ML and the features you created in Part 2 to predict departure delays. Create a notebook and use your feature engineering code to augment the entire dataset with features.\nNext, split the data into training, test, and validation sets. Based on how you did the feature engineering, should your split be fully random, or do you need to do another approach? In any case, reserve at least 20% of the data for final validation, and do not use it when building and testing your models.\nNow apply Spark ML to predict departure delays. Using your training and test sets, choose the right classifier, tune its parameters, and calculate its performance.\nOnce you are done, evaluate your model’s performance on the held-out validation set. Report the accuracy, but break it down as well: produce the full confusion matrix, the true positive and false positive rates, and the sensitivity and specificity. Compare it to a baseline model that always predicts “no delay”. How much better is your model?"
  },
  {
    "objectID": "large-scale-data/project.html#logistics",
    "href": "large-scale-data/project.html#logistics",
    "title": "Project: Distributed Data Analysis",
    "section": "",
    "text": "This project will be completed in assigned groups of 2 or 3 students. Group assignments will be posted on Canvas. There will be several milestones throughout the semester when you will turn in parts of the work; at the end of the mini, you will turn in the completed project."
  },
  {
    "objectID": "large-scale-data/project.html#data-description",
    "href": "large-scale-data/project.html#data-description",
    "title": "Project: Distributed Data Analysis",
    "section": "",
    "text": "The dataset is loaded in Parquet files on our Spark cluster. They live in the sampledata container of the madsdatastore storage account in Azure Data Lake Storage. There is one CSV file per year, but fortunately Spark can read them all at once:\ndelays = spark.read.csv(\"abfss://sampledata@lsd2025storage.dfs.core.windows.net/delays\",\n                        header=True, nullValue=\"NULL\")\nThis will only work in the Class Shared Cluster, since it is configured with the necessary keys to have access to the data.\nThe data includes the following columns:\n\n\n\nVariable\nMeaning\n\n\n\n\nFL_DATE\nDate of flight (YYYY-mm-dd)\n\n\nOP_CARRIER\nCode assigned by International Air Transport Association to identify each airline; Wikipedia has a full table of them.\n\n\nOP_CARRIER_FL_NUM\nFlight number assigned by this carrier for the flight.\n\n\nORIGIN\nIATA code for the airport the flight left from. Wikipedia has a list of airports by IATA code.\n\n\nDEST\nIATA code for the destination airport.\n\n\nCRS_DEP_TIME\nScheduled departure time (local time, hhmm).\n\n\nDEP_TIME\nActual departure time (local time, hhmm).\n\n\nDEP_DELAY\nDifference in minutes between scheduled and actual departure time. Early departures show negative numbers.\n\n\nTAXI_OUT\nTime taken to taxi out to the runway, in minutes.\n\n\nWHEELS_OFF\nTime the flight took off (local time, hhmm).\n\n\nWHEELS_ON\nTime the flight landed (local time, hhmm).\n\n\nTAXI_IN\nTime taken to taxi in to the gate, in minutes.\n\n\nCRS_ARR_TIME\nScheduled arrival time (local time, hhmm).\n\n\nARR_TIME\nActual arrival time (local time, hhmm).\n\n\nARR_DELAY\nDifference in minutes between scheduled and actual arrival time. Early arrivals show negative numbers.\n\n\nCANCELLED\n1 if the flight was cancelled.\n\n\nDIVERTED\n1 if the flight was diverted.\n\n\nCRS_ELAPSED_TIME\nScheduled duration of the flight, in minutes.\n\n\nACTUAL_ELAPSED_TIME\nElapsed time of flight, in minutes.\n\n\nAIR_TIME\nTime the flight was in the air, in minutes.\n\n\nDISTANCE\nDistance between origin and destination airports, in miles.\n\n\nCARRIER_DELAY\nLength of delay caused by the airline (e.g. maintenance, waiting for crew, cleaning the plane), in minutes. Note that if the flight was not delayed, these fields will be blank.\n\n\nWEATHER_DELAY\nLength of delay caused by weather, in minutes.\n\n\nNAS_DELAY\nLength of delay caused by the National Airspace System, including air traffic control, in minutes.\n\n\nSECURITY_DELAY\nLength of delay caused by a security problem, such as evacuation of a terminal or excessive security lines, in minutes.\n\n\nLATE_AIRCRAFT_DELAY\nLength of delay caused because the aircraft arrived late from its previous airport, in minutes."
  },
  {
    "objectID": "large-scale-data/project.html#part-1-basic-data-exploration",
    "href": "large-scale-data/project.html#part-1-basic-data-exploration",
    "title": "Project: Distributed Data Analysis",
    "section": "",
    "text": "This part is due Friday, February 7 at 5pm.\nFirst, create a notebook in the Databricks workspace for cleaning and formatting the data. Ensure you can load the data and consolidate it into one Spark DataFrame. Examine the columns and their types. Do any columns need to be converted to specific types? Review the Spark data types reference; the PySpark functions reference lists functions that can do conversion and manipulation.\nHave your notebook do all the necessary manipulation. At the end of the notebook, write your new data frame into the metastore. Choose a table name that is unique to your group, and write it with:\nyour_data_frame.write.saveAsTable(\"hive_metastore.default.your_group_table_name\")\nNext, create a new notebook in the Databricks workspace. In the notebook, calculate (using Spark) and present the following aggregates:\n\nA summary table or graph showing the number of flights per month across the history of the data, so you know you’ve loaded everything\nThe percentage of flights delayed per week, plotted over the entire length of the data\nThe number of delayed flights per week, by type of delay, plotted over time\nA table of air carriers, showing the number of flights scheduled, the number canceled, the percentage delayed, and the average delay among those delayed. Sort by total number of flights, so the biggest air carriers come first.\nA table of the top 50 airports by percentage of flights delayed, showing the airport code, percentage of flights delayed, and average number of flights per day"
  },
  {
    "objectID": "large-scale-data/project.html#part-2-feature-engineering",
    "href": "large-scale-data/project.html#part-2-feature-engineering",
    "title": "Project: Distributed Data Analysis",
    "section": "",
    "text": "This part is due Friday, February 14 at 5pm.\nOur ultimate goal is to build a predictive model for flight delays. Given a flight and various features, your model should predict whether the flight is delayed for any reason. (We’ll count cancellation as a type of delay.)\nYou could use only the variables present in the data, but it is likely that you can derive new features that would be more useful.\nCreate a notebook that generates the following additional features for each observation:\n\nThe day of week (Monday-Sunday)\nRate of weather delays at the departure airport in the previous hour (the fraction of flights in the previous hour that were delayed due to weather)\nRate of weather delays from the arrival airport in the previous hour\nNumber of flights departing from the departure airport in the previous hour, compared the average number during this hour on the same day of the week, as a z score\nAt least two more features, calculated from the available data, that you think could be useful for your model"
  },
  {
    "objectID": "large-scale-data/project.html#part-3-delay-prediction",
    "href": "large-scale-data/project.html#part-3-delay-prediction",
    "title": "Project: Distributed Data Analysis",
    "section": "",
    "text": "This part is due Friday, February 28 at 5pm.\nNow we aim to use Spark ML and the features you created in Part 2 to predict departure delays. Create a notebook and use your feature engineering code to augment the entire dataset with features.\nNext, split the data into training, test, and validation sets. Based on how you did the feature engineering, should your split be fully random, or do you need to do another approach? In any case, reserve at least 20% of the data for final validation, and do not use it when building and testing your models.\nNow apply Spark ML to predict departure delays. Using your training and test sets, choose the right classifier, tune its parameters, and calculate its performance.\nOnce you are done, evaluate your model’s performance on the held-out validation set. Report the accuracy, but break it down as well: produce the full confusion matrix, the true positive and false positive rates, and the sensitivity and specificity. Compare it to a baseline model that always predicts “no delay”. How much better is your model?"
  },
  {
    "objectID": "large-scale-data/packaging.html",
    "href": "large-scale-data/packaging.html",
    "title": "Packaging Code",
    "section": "",
    "text": "In your time working with R and Python, you have installed many packages. Many of those packages depended on other packages for some of their features. Using import or library(), you loaded the packages and used functions and data from them to complete assignments.\nPackage management is a surprisingly complex topic in software engineering. As software gets increasingly complex, it has more and more dependencies; and with more dependencies come more potential problems. You may depend on specific versions of certain packages, but those packages depend on other versions of each other, causing a conflict. New versions might make incompatible changes. Corporate security policies may require new versions to be reviewed before installation. A problem with a package you’re unaware of, but which is depended upon by packages you use, might break every developer’s workflow.\nWe’re not going to get into the full complexity of package and dependency management. Instead we’ll answer a simple question: How can you create a package? We’ll focus on Python, since R works completely differently.\nOr, to put it in Python terms: How do you make your code available via pip install so anyone can import it?"
  },
  {
    "objectID": "large-scale-data/packaging.html#why-package",
    "href": "large-scale-data/packaging.html#why-package",
    "title": "Packaging Code",
    "section": "Why package?",
    "text": "Why package?\nA package is simply a way to organize code. You can, of course, simply dump a bunch of R or Python files into a folder, and use import or source() liberally to get all the functions and classes you need. Or, even simpler, you could just keep all the functions you need in one giant file.\nBut software engineering is about managing complexity. Your program will grow large. It may be maintained by a team. It may be maintained my multiple teams, each specializing in specific features. Dividing the program into pieces, each with well-defined goals makes good sense.\nAnd so we have packages. A package is a collection of code presenting an interface to users: a selection of functions and classes that can be used to achieve specific things. A large project can be divided into multiple packages that interact to achieve the goals.\n\nExample 1 (Packaging the Delphi COVIDcast pipeline) In early 2020, the Delphi Group quickly built COVIDcast, a system integrating multiple data sources to produce daily county-level measurements of signals related to COVID-19. For example, some signals were aggregated from medical insurance claim records, while others came from government-reported COVID case data.\nEach data source was different, so it required separate code to take the original data, process it, aggregate it to the county and state level, smooth it over time, and format it to be inserted into a SQL database. This code was originally developed by a separate team for each data source; some was in Python and some was in R.\nBut there were many tasks in common for each data source. For geographic aggregation, each had to be able to convert between ZIP codes, counties, metropolitan statistical areas, states, and several other geographic levels; each needed to smooth data over time; each had to output data in a specific format; and so on.\nSo eventually, after the initial rush faded, Delphi developed a single Python package to perform all these tasks. Each separate data pipeline could be reduced to a much simpler script that loaded the raw data, then used the package to format it for publication. The package could be maintained by a small team with specialized understanding of geographical aggregation and time-smoothing, while each data pipeline could be maintained by those who understood the data source best."
  },
  {
    "objectID": "large-scale-data/packaging.html#how-imports-work",
    "href": "large-scale-data/packaging.html#how-imports-work",
    "title": "Packaging Code",
    "section": "How imports work",
    "text": "How imports work\nTo understand how packages work, we need to understand imports.\nLet’s say you run\nimport spam\nThe Python interpreter must find something named spam to import. How does it do this?\n(The answer is much more complicated than I will describe here, but the basics are sufficient for now.)\nFirst, Python checks if you have already loaded spam. Then it checks if it is built into Python, like sys or other standard library packages. Then it checks sys.path. Let’s see what this variable contains:\n\nimport sys\n\nsys.path\n\n['/home/julia/Dropbox/academia_admin/teaching/DEDE/mads-computing/large-scale-data',\n '/home/julia/anaconda3/lib/python311.zip',\n '/home/julia/anaconda3/lib/python3.11',\n '/home/julia/anaconda3/lib/python3.11/lib-dynload',\n '',\n '/home/julia/anaconda3/lib/python3.11/site-packages']\n\n\nWe see a list of directories. Some of these are to the locations where packages are installed by pip; you might recall the site-packages/ name from printouts as you installed other packages. The empty string means the current working directory.\nNotice the first entry is the directory where I am writing these notes. If you run python some/script.py at the shell, the directory containing script.py will be put into the first entry of sys.path, so if that script imports anything, it will look first in the same directory. (Check the sys.path documentation for other special cases.)\nPython goes through each of the options in sys.path in order. In each it looks for something named spam to import.\nThe “something” it looks for is a module. So what are modules?"
  },
  {
    "objectID": "large-scale-data/packaging.html#modules",
    "href": "large-scale-data/packaging.html#modules",
    "title": "Packaging Code",
    "section": "Modules",
    "text": "Modules\nA “module” is a file containing Python code. A file named spam.py is the module spam. Each module has its own namespace, meaning a place where variable, function, and class names are defined.\nFor example, suppose spam.py contains:\n# spam.py\n\nmeaning_of_life = 42\n\ndef add(x, y):\n    return x * y\nThen, in eggs.py, we can write\n# eggs.py\n\nimport spam\n\nspam.meaning_of_life #=&gt; 42\n\nspam.add(2, 2) #=&gt; 4\n\nspam.__name__ #=&gt; \"spam\"\nThe import spam statement causes Python to check sys.path for the spam module. Assuming it finds it, it evaluates the code in spam.py, which defines a variable and a function. These are in the spam namespace, and so now in eggs.py we must access them with the spam. prefix. If we try to access meaning_of_life without the prefix, or define it to have a different value, we will not affect the version in spam.\nEvery module has a __name__ variable defined in its namespace. This is a string giving the module name. spam’s name is spam, of course. A program run directly from the command line has the name __main__, so if we run python eggs.py, the __name__ variable in eggs.py will be set to __main__.\nThis is useful to solve a problem. Suppose a file both defines some useful functions and also does some tasks:\n# addinator.py\n\nimport argparse\n\ndef add(x, y):\n    return x * y\n\nparser = argparse.ArgumentParser(\n    prog=\"Addinator\",\n    description=\"Adds two numbers\")\n\nparser.add_argument(\"x\")\nparser.add_argument(\"y\")\n\nargs = parser.parse_args()\n\nprint(add(args.x, args.y))\nIf I try to import addinator from another file so I can use the add function, it will not go well, because Python runs the entire contents of the file. So it will try to parse command-line arguments and print out a result.\nInstead, we can check if this script is __main__:\n# addinator.py\n\nimport argparse\n\ndef add(x, y):\n    return x * y\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        prog=\"Addinator\",\n        description=\"Adds two numbers\")\n\n    parser.add_argument(\"x\")\n    parser.add_argument(\"y\")\n\n    args = parser.parse_args()\n\n    print(add(args.x, args.y))\nIf we import addinator from another module, then addinator.__name__ == \"addinator\", and so the code guarded by the if will not run. If, on the other hand, we run python addinator.py 2 2, then __name__ == \"__main__\" and we will get the correct answer printed out.\nIt’s common to divide up programs into many modules, based on their logical purpose. Pre-processing code might go in one module, database code in another, and model-fitting in a third. This makes it easier to find"
  },
  {
    "objectID": "large-scale-data/packaging.html#packages",
    "href": "large-scale-data/packaging.html#packages",
    "title": "Packaging Code",
    "section": "Packages",
    "text": "Packages\nA package is a directory containing modules. To signal to Python that the directory is a package, it must contain a file named __init__.py.\nFor example, suppose we have a directory like this:\nspam/\n    __init__.py\n    bacon.py\n    eggs.py\nIf the spam/ directory is in sys.path, then import spam.bacon or from spam import bacon will both work fine.\nThe __init__.py can be empty. However, it can also contain Python code just like any other module. If it did, whatever functions, classes, and variables it defines would be available through import spam.\nFor example, in Pandas, you can do\nimport pandas as pd\n\npd.read_csv(\"foo.csv\")\nThe __init__.py in Pandas does not contain the code for read_csv(), but it does import it from other files within Pandas, so it is within its namespace. Hence importing pandas gives you direct access to read_csv() and various other functions, without you having to import specific modules within Pandas.\n\nRelative imports\nPackages also enable relative imports. A relative import is a way of telling Python to import from within the current package. In bacon.py in the spam package above, we could write\nfrom .eggs import over_easy\nor\nfrom . import eggs\nto indicate that we’re importing from eggs within the current package spam. (You cannot write import .eggs, since that would create a module called .eggs, and names starting with periods are not valid syntax in Python.)\nPackages can contain sub-packages, each a subdirectory with its own __init__.py, and relative imports can work within these. For instance, from .. import foo imports from the parent package containing this one."
  },
  {
    "objectID": "large-scale-data/packaging.html#sec-distribution-packages",
    "href": "large-scale-data/packaging.html#sec-distribution-packages",
    "title": "Packaging Code",
    "section": "Distribution packages",
    "text": "Distribution packages\nPackages are simple, but they still don’t answer key questions: How do you get them onto sys.path? And how do you distribute them to other people?\nA distribution package is a package plus additional information: author name, version, dependencies, and so on. A distribution package can be turned into a file that can be distributed to others, and that can be understood by programs like pip, which can install packages into directories on sys.path.\nThere have historically been many ways to make distribution packages. We’ll focus on the current one, documented in more detail in the Python Packaging User Guide.\nIf we want to make a spam distribution package, we need to add a new file to the package and reorganize slightly:\nspam/\n    spam/\n        __init__.py\n        bacon.py\n        eggs.py\n    pyproject.toml\nThe pyproject.toml file is a metadata file in TOML format specifying details of the package. For example, we might have:\n[project]\nname = \"spam\"\nversion = \"0.4.2\"\ndependencies = [\n    \"pandas\",\n    \"numpy\"\n]\ndescription = \"Spams the ham\"\nauthors = [\n    {name = \"Alex Reinhart\", email = \"areinhar@stat.cmu.edu\"},\n]\n\n[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\nNow we can build the package. By “build”, I mean we can assemble a file containing all the contents of the package, plus the metadata specifying its dependencies and such. For example, if we are in the spam/ directory, we can run\npython -m build\nThis makes distribution files containing your package code and places them in a dist/ subdirectory. These files can be uploaded to PyPI or manually installed with pip, which automatically ensures that the dependencies are installed when you do so.\nAlternately, you can use the file to install the package. For example, you might have a distribution file called spam-0.4.2-py3-none-any.whl, which is the distribution file from building spam. To install it, place the file in a convenient directory, and in that directory run\npip install spam-0.4.2-py3-none-any.whl\nThis will install the contents of the package"
  },
  {
    "objectID": "large-scale-data/packaging.html#exercises",
    "href": "large-scale-data/packaging.html#exercises",
    "title": "Packaging Code",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 (Make a toy package) Let’s make a toy Python package that can be installed using pip.\nMake a package named hard_math. It should have two modules:\n\nlinear: defines a function linear_root(a, b) that returns the solution to the equation \\(ax + b = 0\\).\nquadratic: defines a function quadratic_roots(a, b, c) that returns a tuple of the (real) roots of the polynomial equation \\(ax^2 + bx + c = 0\\).\n\nSet up __init__.py so that one can write from hard_math import quadratic_roots or from hard_math import linear_root and get the desired function. Use relative imports when doing so (Section 4.1).\nSet up pyproject.toml to contain the package name, a short description, a version number (0.0.1), and author information. Specify that the package uses setuptools.\nBuild the package and then install it using pip, as described in Section 5.\nNow make a test.py script in a completely different directory (so that the hard_math directory is not contained in it). In that script, import hard_math and use it to solve the quadratic equation \\(2x^2 + 2x - 3 = 0\\), printing the roots out.\nRun python test.py at the command line and see that it successfully runs, importing from the package you wrote and installed.\nTurn in the contents of your files. You can concatenate them into a single document, with headings indicating which file is which. Begin the file with an outline indicating the directory structure, like the ones I shows above to indicate how files are contained in the spam/ directory."
  },
  {
    "objectID": "large-scale-data/spark.html",
    "href": "large-scale-data/spark.html",
    "title": "Apache Spark",
    "section": "",
    "text": "Note\n\n\n\nA good reference source on Spark is the book by @SparkGuide, which is available for free online through the CMU Library.\n\n\nApache Spark builds on MapReduce ideas (?@sec-distributed-data) in a more flexible way.\nHere’s the idea. Your dataset (a great big data frame, for example) is immutable. There are many ways you can operate upon it, all of which produce a new dataset. For example, you could\n\nmap: apply a function to every row\nfilter: only keep the rows for which a function returns true\nsample: randomly select a subset of rows\ngroup by: if each row is a key and a value, produce a new dataset of (key, list of all the values) pairs\n\nand so on. Spark has a bunch of built-in transformation functions, including these and many more.\nWhen you load a dataset into Spark, different parts of it are loaded into memory on each machine in the cluster. When you apply a transformation to the data, each machine transforms its own chunk of the data.\nWell, hang on. Each machine doesn’t transform its data; Spark just makes a note of the transformation you made. You can do a whole sequence of transformations and Spark will only start the distributed calculations when you perform an action, like\n\nshow: print out a formatted version of the result (kind of like psql’s printed tables of results)\ncollect: return the entire result as a big array or data frame\nreduce: apply a reduction function to the data\ntake: take the first \\(n\\) rows of data\nsave: save the dataset to a file\n\nand so on.\nYou can chain together these operations: you can map, then filter, then sample, then group, then reduce, with whatever operations you want.\nBecause the calculation is only actually done when you perform an action, like collecting the results, Spark builds up a representation of the entire operation you want to perform. It knows you’re filtering the data, then grouping, then transforming, then grouping again, and so on—and just like a SQL database, it can figure out the best way to execute those operations. It can figure out how to distribute them onto different nodes, perhaps how to rearrange or combine operations to avoid redundant work, and so on.\n\n\nSpark is best run on a cluster of multiple machines. The extra work to distribute data and computations is wasted if the computation is done on a single computer—so if your data fits on your laptop, it may be more efficient to do the work in Python than to install Spark. But when the data is huge, Spark becomes necessary.\nWe’ve set up a cluster on Microsoft Azure. This uses Azure Databricks, a product that integrates Spark with online notebooks, data management, job scheduling, and many other features designed for teams working with lots of data sources at a large company.\nA link to our Databricks workspace is posted on Canvas.\nYour Workspace contains notebooks, queries, files, and other Databricks-specific features like dashboards. Computation, such as when you run code in a notebook, is run on a cluster of Azure virtual machines that we rent (see ?@sec-cloud).\n\n\n\n\n\nSpark’s architecture is rather different from what we are used to with relational databases (?@sec-database-fundamentals). In relational databases, we had a client-server architecture: the server software, such as PostgreSQL, ran on a machine and managed all the data. Clients, such as our Python scripts, could connect to the server and send queries; the server then processed the queries, figured out the appropriate response, and sent it back to the client. Many clients could be connected at once.\nThis is not how Spark works. Spark is designed to run on clusters of multiple machines, and it is designed to run applications: programs that run on the cluster, managed by Spark, and do tasks. There are several moving parts:\n\nA cluster manager. The cluster manager is software that has control over a collection of servers (nodes) on which Spark can be run. If we want to run a Spark application, we tell the cluster manager how many nodes we need and what resources (such as memory or disk) the application might use, and it will determine the appropriate nodes to run the application on.\nThe Spark driver. The driver is part of your application. It knows the job you are trying to do and determines how to split up the tasks to run on different machines.\nThe Spark executors. The executors run on the nodes assigned by the cluster manager, and do the work assigned to them by the Spark driver. If your application applies some calculation to a huge data frame, the driver will direct the executors to apply their part of the calculation to their assigned part of the data frame.\n\nSpark is written in Scala, so applications can be written in Scala; but it also can work with Java, Python, and R. In each programming language there are extensive libraries of Spark operations, so when you write an application, it can use the various Spark features. When that application is submitted to a cluster manager to run, it creates a driver, the driver runs your application, and the driver delegates the work to the executors. Every user—every person who wants to run their own calculations—gets their own driver and executors.\nThis creates the key contrast. In relational databases, we ran a Python script on our computer—or any computer we liked—and it sent SQL queries to the database, got the results, and did whatever it wanted to do with them. In Spark, we send an application to the cluster, and the application runs on the cluster, not on our own computer.\nFortunately, Spark does provide a few convenient premade applications: the Spark shells. The PySpark shell, for instance, is an application that accepts Python commands and then runs them on the cluster, so you can interactively run code and see what happens.\n\n\n\nIn Databricks, all the Spark cluster details are managed for you. When you configure a Databricks cluster (which I have done for the class), Databricks automatically sets up a cluster manager, driver, and executors. Spark provides applications: their notebooks and other features run on Spark automatically.\n\n\n\n\nOne Spark client is PySpark. Let’s start with an example derived from the PySpark documentation.\n\n\n\n\n\n\nNote\n\n\n\nPySpark uses method names like appName(), groupBy(), and getOrCreate(), which violate the PEP 8 guidelines on function naming. This is because Spark is written in Scala, which often uses mixedCase names. You should always use PEP 8 conventions when writing your own code.\n\n\n\n\nFrom Python, we need an object representing a connection to Spark, much like we had an object representing PostgreSQL connections.\nTo do this, first we make a SparkSession. This is how our application communicates with the driver and sends work to the executor.\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .getOrCreate()\nNow spark is an object we can use to ask Spark to do things. Then we write the rest of the application code in that Python script. Using a program called spark-submit, we can send the file to the cluster to be run by Spark.\nAlternately, if we’re using a Databricks notebook or the PySpark shell, a SparkSession is automatically created for us and named spark. We do not need to run the code above. We can just type and run our code interactively.\n\n\n\nA DataFrame object is a table with named columns, much like data frames in R or Pandas. We can create them manually by providing a list of rows (as tuples) and a list of column names:\ndf = spark.createDataFrame([\n    (\"Yellow-billed duck\", \"Anas\", \"Undulata\", 1839),\n    (\"Laysan duck\", \"Anas\", \"Laysanensis\", 1892),\n    (\"Mallard\", \"Anas\", \"Platyrhynchos\", 1758),\n    (\"Eurasian wigeon\", \"Mareca\", \"Penelope\", 1758)\n    ],\n    [\"Common name\", \"Genus\", \"Species\", \"Year\"]\n)\n\n\n\nYou probably won’t create most of your data frames by manually typing them into a script. Instead, the data already exists somewhere else: a data file, a SQL database, etc. You simply need Spark to load that data.\n\n\nWe can ask Spark to create a DataFrame object from a data file; it understands a whole bunch of data formats:\nppl = spark.read.load(\"dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta\")\nNow ppl is a data frame. We can view the first few rows:\nppl.show()\n\n\n\nIn Databricks, Spark is integrated with a “catalog” of data tables. This catalog is much like a PostgreSQL database, insofar as it contains a bunch of tables, and there can be complex permissions set up to control who has access to which tables.\nIn our case, the catalog is built on Apache Hive, and stores its data in distributed fashion on Azure’s cloud storage system.\nThere are sample data tables included with Databricks:\ndf = spark.read.table(\"samples.nyctaxi.trips\")\nWe’ll discuss the catalog in more detail in ?@sec-spark-catalog.\n\n\n\n\nOur Python code can do transformations, like filter:\nfiltered = df.filter(df['fare_amount'] &gt; 21)\nThis creates a new DataFrame of the results.\nNotice the magic happening here: if the data file were huge, this transformation is automatically distributed to all nodes in the cluster. But not now—later, when we try to use filtered, for example by writing filtered.show().\nSince every operation returns a new DataFrame, we can write operations like\ndf.groupBy(\"pickup_zip\").count()\nThis returns a new DataFrame with columns pickup_zip and count. A lazy DataFrame, which is only calculated when we ask for it.\nYou can think of this as a way of building up a MapReduce job. You write the sequence of transformations you need—even for loops and whatever else you want—and at the end, when you request the result, Spark rearranges and distributes the work to the executors so it can be calculated even on a massive dataset.\nYou still have to figure out how to structure your operation as a series of transformations and reductions, but it’s a bigger set of transformations and reductions, and you can compose many small ones together instead of cramming the operation into one big Map and Reduce.\n\n\n\nOnce we have a DataFrame, we can find the operations available to us in a few places:\n\nThe DataFrame class documentation lists all the methods, such as groupBy, filter, join, and so on.\nThe Spark SQL functions list gives functions that can be imported from pyspark.sql.functions and used in DataFrame operations—things like mathematical operators, date functions, and other functions you might use in data manipulation (much like PostgreSQL’s built-in functions).\n\n\n\n\n\nBecause we’re telling Spark what operations to perform, rather than performing themselves in our own code, we have to write operations in a way Spark understands. This requires some adjustment from what you’re used to with Pandas and other Python work.\nMany of these operations come from pyspark.sql.functions, so it’s common to import that module, either entirely or just for specific functions:\n# import entire module\nimport pyspark.sql.functions as F\n\n# or import just a few functions you need\nfrom pyspark.sql.functions import col, desc\nIn the examples below, we’ll use the first approach and import the entire module as F.\nYou can find a full list of functions in the PySpark documentation.\n\n\nIn Spark operations, we need some way to refer to specific columns. The col() function lets us identify columns by name as a string, e.g. F.col(\"foo\") to refer to the foo column.\nIf you have a DataFrame named df, you can also use df.foo or df[\"foo\"] to refer to columns in it. You will often see code using different ways to refer to columns. If you work at a company, they’ll probably have a standard style for Spark code and use one specific way to refer to columns.\nUsing any of these methods returns a Column object representing the column. These objects have some useful methods, such as alias(), for giving a column a new name, much like AS in SQL. For example, F.col(\"foo\").alias(\"foo2\") is like SELECT foo AS foo2.\n\n\n\nIn programming languages, expressions are operations that yield a value, such as 2 + 2 or (\"foo\", \"bar\") (whose value is a tuple containing two strings). In many Spark operations, we’ll want to write expressions that refer to entire columns, to describe how we’ll be modifying and combining columns.\nThe col() function returns a Column object, and Column objects overload many operators to turn them into Spark expressions. For example,\n# represents adding 2 to every value\nF.col(\"foo\") + 2\n\n# represents adding two columns together\nF.col(\"foo\") + F.col(\"bar\")\nSpark achieves this by overloading the basic Python operators like + and *, so instead of doing literal multiplication, they create objects representing the Spark operations being done. This means that you cannot use ordinary Python functions to operate in expressions, but have to use the Spark equivalents:\nimport math\n\n# doesn't work\nmath.sqrt(F.col(\"foo\"))\n\n# works\nF.sqrt(F.col(\"foo\"))\nLogical combinations of expressions must be expressed using the |, &, and ~ operators, not the usual or, and, and not keywords in Python:\n(F.col(\"foo\") &gt;= 18) & ~F.col(\"bar\")\nNotice the extra parentheses in this expression. The |, &, and ~ operators have higher precedence than &gt;= and other comparison operators, so without parentheses, the expression would be interpreted as F.col(\"foo\") &gt;= (18 & ~F.col(\"bar\")), which is definitely not what we want. This is an unfortunate consequence of PySpark’s operator overloading approach.\nWe can use other Column methods in expressions to obtain useful results. For example, maybe we want to make a new column whose value depends on the value of another:\nF.when(F.col(\"trip_distance\") &gt;= 10, \"Long\").otherwise(\"Short\")\nMany PySpark functions accept a string column name as well as a column expression. (The type is listed as ColumnOrName in the documentation.) For example, we can write this both ways:\nF.sqrt(F.col(\"foo\"))\nF.sqrt(\"foo\")\nBut to take the square root of the sum of two columns, we should use an expression:\nF.sqrt(F.col(\"foo\") + F.col(\"bar\"))\nYou can also use the expr() function to write expressions as strings that Spark will parse and convert into equivalent form. These are identical to the earlier examples:\nF.expr(\"foo + 2\")\n\nF.expr(\"foo + bar\")\n\nF.expr(\"sqrt(foo)\")\n\nF.expr(\"sqrt(foo + bar)\")\nThese functions all return Column objects, so you can do any of the Column operations on them, such as setting an alias.\n\n\n\nMuch of what you do with Spark DataFrames will be selecting specific columns and doing calculations with them to make new DataFrames. In SQL, that’s the role of SELECT, which can select columns but also make new ones by combining existing ones; in dplyr, that’s the role of select() and mutate().\nThe select() method on DataFrame objects is similar. It takes multiple arguments, one for each column, and the columns can be expressions giving calculations. For example, building on our examples so far, we can write:\ndf.select(F.col(\"foo\"), F.col(\"bar\"), F.col(\"foo\") + F.col(\"bar\"),\n          F.sqrt(F.col(\"foo\")))\nThe selectExpr() method does the same, but with expression strings:\ndf.selectExpr(\"foo\", \"bar\", \"foo + bar\", \"sqrt(foo)\")\nWe can also create new columns using the withColumn() method, in which we provide the column name and value:\ndf.withColumn(\"fooBarSum\", F.col(\"foo\") + F.col(\"bar\"))\nThis returns a new data frame with the additional column. If you need to calculate and add many new columns, use withColumns(), which takes a dictionary mapping column names to values.\n\n\n\nTo filter a DataFrame’s rows, we need an expression whose result has as many rows as the DataFrame, and whose values are either true or false. Rows with a True value will be kept.\nDataFrames have filter() and where() methods to do the filtering; these are identical, and the two names are provided just because both names are commonly used in different languages. Both take an argument that is the expression to use for filtering. For example:\ndf.filter(F.col(\"foo\") &gt; 10)\n\n# equivalently:\n\ndf.filter(\"foo &gt; 10\")\nPart of Spark’s optimization process is to analyze the filters in an operation and group them together, so the data is filtered before any more expensive operations are done. This means that chaining multiple filters is fine, since Spark will do them at once:\ndf.filter(\"foo &gt; 10\") \\\n    .filter(\"bar &lt; 4\") \\\n    .filter(\"sqrt(score) != 7\")\n\n\n\nTo order a DataFrame’s rows, we use its orderBy() method. This accepts one or more column names to sort by. As with every Spark feature so far, there’s more than one way to do it:\n# using desc() to specify the direction\ndf.orderBy(F.desc(\"trip_distance\"))\n\n# referring to bare column name\ndf.orderBy(\"trip_distance\", ascending=False)\n\n# using a Column object\ndf.orderBy(df.trip_distance, ascending=False)\nWe can specify multiple columns to sort on, so later columns will be used to break ties in the earlier columns:\ndf.orderBy(F.desc(\"trip_distance\"), F.asc(\"fare_amount\"))\n\nExercise 1 (Simple selections and filters) Load the sample taxi data in a Databricks notebook:\ndf = spark.read.table(\"samples.nyctaxi.trips\")\nUsing this data frame,\n\nSelect all the rows with a distance greater than 15 in with pickup in ZIP code 11422.\nCalculate a new column representing the fare amount per unit distance. Call this column fare_rate.\nOutput (with show()) a data frame containing only the trip distance, fare rate, and dropoff ZIP. Sort it by fare rate in descending order.\n\nYour answers will build on each other to produce one expression that does the whole thing.\n\n\nSolution. \n# 1\ndf.filter(\"trip_distance &gt; 15\").filter(\"pickup_zip = 11422\")\n\n# 2\ndf.filter(\"trip_distance &gt; 15\") \\\n  .filter(\"pickup_zip = 11422\") \\\n  .withColumn(\"fare_rate\", F.col(\"fare_amount\") / F.col(\"trip_distance\"))\n\n# 3\ndf.filter(\"trip_distance &gt; 15\") \\\n  .filter(\"pickup_zip = 11422\") \\\n  .withColumn(\"fare_rate\", F.col(\"fare_amount\") / F.col(\"trip_distance\")) \\\n  .select(\"trip_distance\", \"fare_rate\", \"dropoff_zip\") \\\n  .orderBy(F.desc(\"fare_rate\")) \\\n  .show()\n\n\n\n\n\n\nThe DataFrame class has a groupBy() method to group data frames, much like GROUP BY in SQL or group_by() in dplyr. This takes the columns to group by as arguments:\ndf.groupBy(\"foo\")\n\n# group by both columns, so there is one group per combination\ndf.groupBy(\"foo\", \"bar\")\n(Notice here we have used string column names, since groupBy() says it accepts arguments of type ColumnOrName. We could also use col().)\nThe result of groupBy() is a GroupedData object. This is a different object with different methods. The method you’re most likely to want is agg(), which does aggregation. Just like select(), it takes as many arguments as you want columns, each argument can be a different column expression. The expressions can use any of Spark’s built-in aggregate functions.\nFor example:\ndf.groupBy(\"foo\") \\\n    .agg(F.min(F.col(\"score\")).alias(\"min_score\"),\n         F.max(F.col(\"score\")).alias(\"max_score\"))\nIt also takes a dictionary argument. If given a dictionary, the keys should be column names, and the values should be strings naming the aggregate function to use:\ndf.groupBy(\"foo\") \\\n    .agg({\"score\": \"min\", \"latency\": \"max\"})\nIn this form, you can only do one aggregate function per column, since dictionaries can only contain each key once.\n\nExample 1 (Operating on the events data) We’ve loaded our events data (from ?@sec-example-db) into Databricks. Let’s load it:\nevents = spark.read.table(\"hive_metastore.default.events\")\nNow we can begin Spark operations. For example, to get the average score per student:\n# a Spark GroupedData object\npersona_scores = events.groupBy(\"persona\")\n\npersona_scores.avg(\"score\")\n(GroupedData objects have several methods for common aggregate functions, like avg().)\nNotice that Spark does not display the data frame—that’s because it hasn’t done the calculation yet! We can use .show() to have it print it out, or .collect() to return the results as a Python variable we can work with.\nEach operation returns a DataFrame that we can conduct further operations on, by chaining operations in sequence. For example, let’s try reproducing a query from ?@sec-grouping-aggregate:\nselect persona, avg(score) as mean_score\nfrom events\nwhere moment &gt; cast('2014-10-01 11:00:00' as timestamp)\ngroup by persona\nhaving avg(score) &gt; 300\norder by mean_score desc;\nIn Spark, we write the same operations in logical order:\nimport pyspark.sql.functions as F\nfrom datetime import datetime\n\ntop_scores = events \\\n    .filter(F.col(\"moment\") &gt; datetime(2014, 10, 1, 11)) \\\n    .groupBy(\"persona\") \\\n    .agg(F.avg(F.col(\"score\")).alias(\"mean_score\")) \\\n    .filter(F.col(\"mean_score\") &gt; 300) \\\n    .orderBy(F.desc(\"mean_score\"))\nBut when we ask for the results (with .show(), .take(), .collect(), or similar), it analyzes the operations to determine the most efficient way to execute them, and only then does it perform the calculations.\nJust like in SQL, we can also ask Spark to explain how it will run these operations on the cluster:\ntop_scores.explain()\n\n\nExercise 2 (Querying the events table) Load the events table into a Spark DataFrame as shown in Example 1. Write DataFrame operations to do the operations in ?@exr-basic-grouping. (By “DataFrame operations” I mean you should use Python methods like in Example 1, and not Spark SQL.)\nFor #4, see the datetime functions provided in pyspark.sql.functions.\n\n\nSolution. We can write the following Spark operations:\nfrom pyspark.sql import functions as F\n\n# 1\nevents.groupBy(\"persona\") \\\n    .agg({\"latency\": \"max\", \"score\": \"avg\"}) \\\n    .filter(F.col(\"max(latency)\") &gt; 565) \\\n    .orderBy(F.col(\"avg(score)\")) \\\n    .show()\n\n# 2\nevents.groupBy(\"persona\") \\\n    .avg(\"score\") \\\n    .filter(F.col(\"avg(score)\") &gt; 600) \\\n    .show()\n\n# 3\nevents.groupBy(\"element\") \\\n    .count() \\\n    .show()\n\n# 4. note that .min() only works on numeric columns, while .agg can apply min()\n# to other column types\nevents.groupBy(\"persona\") \\\n    .agg({\"moment\": \"min\"}) \\\n    .select([F.col(\"persona\"), F.date_format(F.col(\"min(moment)\"), \"MMMM d\")]) \\\n    .show()\n\n\nExercise 3 (Taxi aggregates) Using the taxi data from Exercise 1, produce a data frame giving the following summary statistics for each pickup ZIP code:\n\nThe average fare rate (as defined in Exercise 1)\nThe total number of trips from that ZIP (see the count() aggregate function)\nThe minimum and maximum distance traveled\n\nGive each column an appropriate name. Order the result by descending number of trips.\n\n\nSolution. \ndf = spark.read.table(\"samples.nyctaxi.trips\")\n\ndf.withColumn(\"fare_rate\", F.col(\"fare_amount\") / F.col(\"trip_distance\")) \\\n    .groupBy(\"pickup_zip\") \\\n    .agg(F.avg(\"fare_rate\").alias(\"avg_fare_rate\"),\n         F.min(\"trip_distance\").alias(\"min_distance\"),\n         F.max(\"trip_distance\").alias(\"max_distance\"),\n         F.count(\"fare_rate\").alias(\"num_trips\")) \\\n    .orderBy(F.desc(\"num_trips\")) \\\n    .show()\n\n\n\n\nGrouping is conceptually simple: If you group by the columns (a, b, c), the groups are defined by all unique combinations of those three columns.\nBut sometimes we’d like to calculate aggregates for groups, while also providing overall totals. The output table would hence have one row per unique combination, plus one or more summary rows. This is a rollup.\nRollups can be defined with the rollup() method on DataFrames. This creates a GroupedData object that supports all the same aggregation you can do after groupBy().\n\nExample 2 (Score rollup) Returning to the events data loaded in Example 1, let’s get the average score per element, but with a rollup:\nevents \\\n    .rollup(\"element\") \\\n    .agg(F.avg(F.col(\"score\")).alias(\"mean_score\")) \\\n    .orderBy(\"element\")\nNotice the row with a NULL for element. This row represents the mean score for all rows in the events table.\n\nJust as you can group by multiple columns, you can rollup across multiple columns. The result is hierarchical: If you rollup by (a, b, c) and aggregate, you get\n\none row per unique combination of (a, b, c),\none row per unique combination of (a, b), averaging over all c,\none row per unique value of a, averaging over all b and c,\none row averaging over all data.\n\nThe columns being averaged over are always represented with NULL. To prevent confusing problems, ensure that the columns you rollup over do not already contain NULL values, or filter them out first.\n\nExercise 4 (Roll up the taxis) Return to the taxi data from Exercise 1. Produce a table calculating average fares for every (pickup, dropoff) ZIP code pair, for every pickup ZIP code, and overall.\n\n\nSolution. \ndf = spark.read.table(\"samples.nyctaxi.trips\")\n\ndf \\\n    .rollup(\"pickup_zip\", \"dropoff_zip\") \\\n    .agg(F.avg(\"fare_amount\").alias(\"mean_fare\")) \\\n    .orderBy(\"pickup_zip\", \"dropoff_zip\")\n\n\n\n\nWe can take rollups a step further with cubes. Cubes are not hierarchical: if you make a cube of (a, b, c), you get aggregates over every subset:\n\n(a, b, c)\n(a, b)\n(b, c)\n(a, c)\na\nb\nc\noverall\n\nThe cube() method of DataFrame objects does this, and you can use aggregate functions as usual.\n\nExercise 5 (Cube the taxis) Repeat Exercise 4, but obtain average fares for every pickup ZIP, every dropoff ZIP, and every combination of them. Order by average fare in descending order. Which ZIPs or combinations have the highest average fares?\n\n\nSolution. \ndf \\\n    .cube(\"pickup_zip\", \"dropoff_zip\") \\\n    .agg(F.avg(\"fare_amount\").alias(\"mean_fare\")) \\\n    .orderBy(F.desc(\"mean_fare\"))\nNote that all the highest fares come from ZIPs or combinations with only one trip.\n\n\n\n\n\nDataFrame objects have a join() method that supports the usual joins from SQL (?@sec-joins). For example, if we have two data frames df1 and df2, we can write a join like so:\ndf1.join(df2, df1.event_id == df2.id, how=\"left\")\nRecall that df1.event_id is a way to refer to the event_id column on the df1 DataFrame, so it is a Column object; setting two Column objects equal to each other produces an object representing the join condition we are requiring. This is similar to ON in a SQL query. We can get left joins, right joins, inner joins, outer joins, and other combinations with the how argument.\nThe join() method returns a DataFrame representing the joined tables, so doing multiple joins simply requires calling the join() method repeatedly.\n\nExample 3 (Joining in TPC-H) The Transaction Processing Performance Council builds benchmarks to measure how fast different database systems are. One of their sample datasets is TPC-H, which simulates an order and inventory management system. The database contains a number of parts that can be provided by various suppliers; there are many customers who can submit orders, each order consisting of multiple parts, potentially from several suppliers. This data is split across eight tables in the samples.tpch schema:\n\n\n\nThe TPC-H table schema, from the TPC Benchmark H Standard Specification, revision 3.0.1. Column names are prefixed by the value given in parentheses, so for example all lineitem columns begin with l_. Arrows represent foreign keys. Here SF = 5, so there are 30 million line items and 50,000 suppliers.\n\n\nSuppose we’d like to find all the suppliers for parts in order #13710944. We need to find all the line items in that order (lineitem), join them with part suppliers (partsupp), and join those with suppliers (supplier). Then let’s pull out the part identifier (partkey), name of the supplier (s_name), and the available quantity (ps_availqty) for that part.\nWe begin by getting all the tables:\nlineitem = spark.read.table(\"samples.tpch.lineitem\")\npartsupp = spark.read.table(\"samples.tpch.partsupp\")\nsupplier = spark.read.table(\"samples.tpch.supplier\")\nFirst we must filter lineitem to get the order we want. Then we must do the joins. Notice how we can set multiple join conditions by providing them as a list.\nlineitem \\\n    .filter(F.col(\"l_orderkey\") == 13710944) \\\n    .join(partsupp, [lineitem.l_partkey == partsupp.ps_partkey,\n                     lineitem.l_suppkey == partsupp.ps_suppkey],\n          how=\"left\") \\\n    .join(supplier, supplier.s_suppkey == partsupp.ps_suppkey) \\\n    .select(\"l_partkey\", \"s_name\", \"ps_availqty\")\n\n\nExercise 6 (Joining harder in TPC-H)  \n\nProduce a table with one row per customer. For each customer, calculate their total expenditure (sum of the price of all orders they have submitted), the total number of line items ordered, and the total number of orders. Ensure the table includes customers who have never made any orders.\nProduce a table with one row per supplier. For each supplier, report their name and the total number of parts they have supplied to meet orders (meaning the sum of the quantity of their parts across all orders).\nProduce a summary table giving pricing information for all items shipped up to June 1, 1998 (l_shipdate). There should be one row per return flag and line status (l_returnflag and l_linestatus); each row should give:\n\nthe number of line items with this status/return flag\nthe total quantity of line items shipped\nthe average quantity of items per order\ntheir total base price (l_extendedprice)\ntheir total discounted price (l_extendedprice * (1 - l_discount))\n\n\n\n\nSolution. \n# 1\norders = spark.read.table(\"samples.tpch.orders\")\nlineitem = spark.read.table(\"samples.tpch.lineitem\")\n\n# TODO Fix to include customers who haven't made orders\norders \\\n    .join(lineitem, orders.o_orderkey == lineitem.l_orderkey) \\\n    .groupBy(\"o_custkey\") \\\n    .agg(F.sum(\"o_totalprice\").alias(\"total_spend\"),\n         F.count(\"l_linenumber\").alias(\"total_num_items\"),\n         F.count_distinct(\"o_orderkey\").alias(\"num_orders\")) \\\n    .orderBy(\"o_custkey\") \\\n    .show()\n\n# 2\npartsupp = spark.read.table(\"samples.tpch.partsupp\")\n\nlineitem \\\n    .join(partsupp, [partsupp.ps_partkey == lineitem.l_partkey,\n                     partsupp.ps_suppkey == lineitem.l_suppkey],\n          how=\"left\") \\\n    .groupBy(\"l_suppkey\") \\\n    .agg(F.sum(\"l_quantity\").alias(\"total_quantity\")) \\\n    .join(supplier, lineitem.l_suppkey == supplier.s_suppkey) \\\n    .select(\"s_suppkey\", \"s_name\", \"total_quantity\") \\\n    .show()\n\n# 3\n# Inspired by https://www.tpc.org/TPC_Documents_Current_Versions/pdf/TPC-H_v3.0.1.pdf\n# section 2.4.1\nfrom datetime import date\nlineitem \\\n    .filter(F.col(\"l_shipdate\") &lt; date(1998, 6, 1)) \\\n    .groupBy(\"l_returnflag\", \"l_linestatus\") \\\n    .agg(F.count(\"l_linenumber\").alias(\"total_items\"),\n         F.sum(\"l_quantity\").alias(\"quantity_shipped\"),\n         (F.sum(\"l_quantity\") / F.count_distinct(\"l_orderkey\")).alias(\"avg_per_order\"),\n         F.sum(\"l_extendedprice\").alias(\"total_base_price\"),\n         F.sum(F.expr(\"l_extendedprice * (1 - l_discount)\")).alias(\"total_discounted_price\")) \\\n    .show()\n\n\n\n\nIn SQL, we learned about window functions (?@sec-sql-window), which allow us to perform aggregation without summarizing the data to get one row per group. Instead, we can select data and augment it with columns that calculate aggregates or relate the row to others in a group.\nIn grouping, we use groupBy to define the groups. In windowing, we have to define which rows will be included in the window for any particular row. For example, in ?@sec-sql-window we wrote the following query of the events table:\nSELECT\n  id, persona, element, score, moment,\n  ((score - avg(score) OVER (PARTITION BY persona ORDER BY moment)) /\n   stddev_samp(score) OVER (PARTITION BY persona ORDER BY moment)) AS z_score\nFROM events\nLIMIT 10;\nHere, the window “around” each row consists of all rows with the same persona (PARTITION BY persona). But the window only considers those rows occurring before this one (ORDER BY moment).\nIn Spark, we need a way to specify this. A special Window class lets us configure the window to use:\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window \\\n    .partitionBy(\"persona\") \\\n    .orderBy(F.asc(\"moment\")) \\\n    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\nHere rowsBetween() specifies which rows of the window count: here, from the first row in the window to the current row. We can also use rangeBetween, which specifies the values to include in the window: .rangeBetween(-120, -1) means to include all rows whose ordering column value is between 120 and 1 unit less than this row. If moment were measured in seconds, this would give us a window of all events between 120 and 1 second before this row’s event, regardless of how many events that is. Meanwhile, rowsBetween(-120, -1) would give us all rows from 120 before to 1 before this one (in time), regardless of their absolute times.\nNow, any Column object—such as the output of any aggregate function—has an over() method that lets us specify the window to use. So to replicate the SQL query above, we write:\nevents = spark.read.table(\"hive_metastore.default.events\")\n\nevents.select(\n    F.col(\"id\"), F.col(\"persona\"), F.col(\"element\"), F.col(\"score\"), F.col(\"moment\"),\n    ((F.col(\"score\") - F.avg(\"score\").over(window_spec)) /\n     F.stddev_samp(\"score\").over(window_spec)).alias(\"z_score\")\n)\n\nExercise 7 (Are windows inclusive?) The rowsBetween() method of the Window class lets us specify which rows are included in the window. Above, we chose the range (Window.unboundedPreceding, Window.currentRow).\nIs this inclusive or exclusive? That is, is the current row included in the window?\nCreate a test data frame (using spark.createDataFrame()) that would let you test this, define a window, and query the data frame. Interpret your result: is it inclusive or exclusive?\n\n\nSolution. TODO Yes, they’re inclusive.\n\n\n\n\n\nYou might have noticed that the basic Spark operations, like maps, grouping, and aggregation, sound a lot like the kinds of things you can do in SQL. That’s not a coincidence: the relational model of operations on tables still holds up after all these years.\nAs a bonus, Spark understands SQL syntax. You can run operations on data stored in catalogs:\nsqlDF = spark.sql(\"\"\"\nSELECT persona, avg(score)\nFROM hive_metastore.default.events\nGROUP BY persona\"\"\")\nsqlDF.show()\nsqlDF is just another DataFrame, and so .show() prints it out. Because it’s just a DataFrame, we could also do additional operations on it in Python (like filter(), avg(), or any of the other methods we’ve used), and the entire sequence of operations will only be done when we perform an action like show().\nThere are Spark SQL functions for math, logic, dates, and other common needs (just like SQL), as well as aggregate functions and window functions. You can JOIN multiple tables just as in SQL. There are also Spark-specific extensions to give Spark hints about the most efficient way to execute a query. To find the syntax definition and list of built-in functions, see the Spark SQL reference.\nIf you have a DataFrame in Python and want to access it using SQL, regardless of whether it’s already in a catalog, you can do that too:\n# Register the DataFrame as a SQL temporary view\nsome_dataframe.createOrReplaceTempView(\"some_data\")\nThe createOrReplaceTempView() method we used above creates what Spark calls a temporary view: it lets us see the data already in a DataFrame. (And remember, a DataFrame might represent various transformations of the original data that have not been executed yet; when you write a SQL query on such a DataFrame, Spark will work out how to calculate the original transformations and the SQL operations together, in one large operation.) This view only lasts as long as our Spark session, and goes away when our application exits.\nWe can also create permanent tables, much like in Postgres. While Postgres (just like most relational databases) uses specialized file formats that support indexing, transactions, atomic updates, and the other ACID features of relational databases, Spark can use any file that it knows how to read. This is done with a specialized CREATE TABLE command that includes options specifying the file format and file location; the file could be stored on a distributed filesystem like HDFS, making Spark SQL suitable for enormous datasets.\n\nExample 4 (Operating on the events data in Spark SQL) Continuing Example 1, we can run the same query in Spark SQL.\ntop_scores = spark.sql(\"\"\"\nselect persona, avg(score) as mean_score\nfrom hive_metastore.default.events\nwhere moment &gt; cast('2014-10-01 11:00:00' as timestamp)\ngroup by persona\nhaving avg(score) &gt; 300\norder by mean_score desc\n\"\"\")\ntop_scores.show()\nThis is rather anticlimactic, since it’s exactly the same query we ran before, and it runs just fine.\n\n\nExample 5 (TPC-H data in Spark SQL) Returning to the TPC-H data from Example 3, we can conduct the same join with Spark SQL.\nspark.sql(\"\"\"\nSELECT l_partkey, s_name, ps_availqty\nFROM samples.tpch.lineitem\nLEFT JOIN samples.tpch.partsupp ON l_partkey = ps_partkey AND l_suppkey = ps_suppkey\nLEFT JOIN samples.tpch.supplier ON s_suppkey = ps_suppkey\nWHERE l_orderkey = 13710944\n\"\"\")\nNotice we have to refer to the tables by their fully qualified names, so Spark knows where to find them.\n\nIt’s even possible to have Spark to behave like a SQL server so that any language that supports SQL can connect and send SQL queries. Note, however, that while Spark is good at doing large calculations on huge datasets quickly, it is not designed to do small operations with low latency—so it is not a replacement for a standard relational database to, say, run a website that needs to respond to requests in 100 milliseconds. It is better suited for backend analysis tasks that run periodically.\n\nExercise 8 (TPC-H data in Spark SQL) Repeat Exercise 6 in Spark SQL.\n\n\nSolution. TODO\n\n\n\n\n\nExercise 9 (Reading tea leaves in the TPC-H schema) Refer to the TPC-H schema above, and reflect back on our schema design principles (?@sec-schema-design).\n\nWhy should partsupp (part suppliers) be separate from part and supplier? Couldn’t every part just have a supplier? What does this allow extra?\nName three features of the business that are implied by the schema. For example: The business can ship your order in multiple parts, because shipping information is given separately for each lineitem, rather than once per order.\n\n\n\nSolution. \n\nThis allows parts to be supplied by more than one suppler. There is a many-to-many relationship between parts and suppliers encoded in the partsupp table. Maybe some items are generic and made by many different companies.\nHere are some examples:\n\nThe company keeps separate inventory for parts supplied by each supplier, rather than inventory for the part regardless of supplier. It must care who supplies parts. (Available quantity is in partsupp, not in part.)\nEach item can be returned separately, rather than returning the entire order.\nThe company groups nations by regions, so it must aggregate its reporting by region.\nCustomers only exist in one country each—they’re not multinational.\n\n\n\n\nExercise 10 (LendingClub and Spark) LendingClub offers various types of loans to consumers and businesses. From 2007 to 2018, they offered “peer-to-peer” loans: customers could request personal loans, and individual investors could decide how to allocate their money to fund those loans. LendingClub handled payments, while the investors got a share of the loan interest.\nTo allow investors to make informed decisions, LendingClub released a large dataset of loans, including loan amounts, credit scores of borrowers, payment history, and various other financial and payment information. The data for all accepted loans is at abfss://sampledata@lsd2025storage.dfs.core.windows.net/lending-club.parquet, available from the Class Shared Cluster. A data dictionary spreadsheet describing all the variables is in the hive_metastore.default.lc_data_dictionary table.\nUse spark.read.parquet() to load the data into the variable df. Print out df to see a list of all the available columns.\nUse df.count() to get the number of rows of data.\nNow you’ll do a series of analysis tasks as if you were a data analyst at LendingClub. Your notebook should use Spark do its calculations; you cannot load the entire dataset into Python, and you cannot use Pandas.\nYour notebook should calculate, for each month (according to issue_d), the following summaries:\n\nThe total number of loans issued and, separately, the number of 36-month and 60-month loans issued (see term)\nThe total funded amount of those loans (funded_amnt) and the total remaining principal to be paid (out_prncp)\nThe percentage of loans with interest rates (int_rate) greater than 10%\nThe percentage of loans that have been fully paid by now (see loan_status): overall, for grade A loans, and for grade F loans (see grade), as separate columns\nThe percentage of loans on a hardship payment plan (hardship_flag)\n\nThe result should be a data frame with one row per month and one column per summary statistic. Ensure the columns have meaningful names. Print the first few rows of this table in your notebook so I can grade your work.\n\n\nSolution. TODO\n\n\nExercise 11 TODO exercise using TPC-H with rollups, cubes, windows"
  },
  {
    "objectID": "large-scale-data/spark.html#connecting-to-the-spark-workspace",
    "href": "large-scale-data/spark.html#connecting-to-the-spark-workspace",
    "title": "Apache Spark",
    "section": "",
    "text": "Spark is best run on a cluster of multiple machines. The extra work to distribute data and computations is wasted if the computation is done on a single computer—so if your data fits on your laptop, it may be more efficient to do the work in Python than to install Spark. But when the data is huge, Spark becomes necessary.\nWe’ve set up a cluster on Microsoft Azure. This uses Azure Databricks, a product that integrates Spark with online notebooks, data management, job scheduling, and many other features designed for teams working with lots of data sources at a large company.\nA link to our Databricks workspace is posted on Canvas.\nYour Workspace contains notebooks, queries, files, and other Databricks-specific features like dashboards. Computation, such as when you run code in a notebook, is run on a cluster of Azure virtual machines that we rent (see ?@sec-cloud)."
  },
  {
    "objectID": "large-scale-data/spark.html#sec-spark-architecture",
    "href": "large-scale-data/spark.html#sec-spark-architecture",
    "title": "Apache Spark",
    "section": "",
    "text": "Spark’s architecture is rather different from what we are used to with relational databases (?@sec-database-fundamentals). In relational databases, we had a client-server architecture: the server software, such as PostgreSQL, ran on a machine and managed all the data. Clients, such as our Python scripts, could connect to the server and send queries; the server then processed the queries, figured out the appropriate response, and sent it back to the client. Many clients could be connected at once.\nThis is not how Spark works. Spark is designed to run on clusters of multiple machines, and it is designed to run applications: programs that run on the cluster, managed by Spark, and do tasks. There are several moving parts:\n\nA cluster manager. The cluster manager is software that has control over a collection of servers (nodes) on which Spark can be run. If we want to run a Spark application, we tell the cluster manager how many nodes we need and what resources (such as memory or disk) the application might use, and it will determine the appropriate nodes to run the application on.\nThe Spark driver. The driver is part of your application. It knows the job you are trying to do and determines how to split up the tasks to run on different machines.\nThe Spark executors. The executors run on the nodes assigned by the cluster manager, and do the work assigned to them by the Spark driver. If your application applies some calculation to a huge data frame, the driver will direct the executors to apply their part of the calculation to their assigned part of the data frame.\n\nSpark is written in Scala, so applications can be written in Scala; but it also can work with Java, Python, and R. In each programming language there are extensive libraries of Spark operations, so when you write an application, it can use the various Spark features. When that application is submitted to a cluster manager to run, it creates a driver, the driver runs your application, and the driver delegates the work to the executors. Every user—every person who wants to run their own calculations—gets their own driver and executors.\nThis creates the key contrast. In relational databases, we ran a Python script on our computer—or any computer we liked—and it sent SQL queries to the database, got the results, and did whatever it wanted to do with them. In Spark, we send an application to the cluster, and the application runs on the cluster, not on our own computer.\nFortunately, Spark does provide a few convenient premade applications: the Spark shells. The PySpark shell, for instance, is an application that accepts Python commands and then runs them on the cluster, so you can interactively run code and see what happens.\n\n\n\nIn Databricks, all the Spark cluster details are managed for you. When you configure a Databricks cluster (which I have done for the class), Databricks automatically sets up a cluster manager, driver, and executors. Spark provides applications: their notebooks and other features run on Spark automatically."
  },
  {
    "objectID": "large-scale-data/spark.html#spark-in-python",
    "href": "large-scale-data/spark.html#spark-in-python",
    "title": "Apache Spark",
    "section": "",
    "text": "One Spark client is PySpark. Let’s start with an example derived from the PySpark documentation.\n\n\n\n\n\n\nNote\n\n\n\nPySpark uses method names like appName(), groupBy(), and getOrCreate(), which violate the PEP 8 guidelines on function naming. This is because Spark is written in Scala, which often uses mixedCase names. You should always use PEP 8 conventions when writing your own code.\n\n\n\n\nFrom Python, we need an object representing a connection to Spark, much like we had an object representing PostgreSQL connections.\nTo do this, first we make a SparkSession. This is how our application communicates with the driver and sends work to the executor.\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .getOrCreate()\nNow spark is an object we can use to ask Spark to do things. Then we write the rest of the application code in that Python script. Using a program called spark-submit, we can send the file to the cluster to be run by Spark.\nAlternately, if we’re using a Databricks notebook or the PySpark shell, a SparkSession is automatically created for us and named spark. We do not need to run the code above. We can just type and run our code interactively.\n\n\n\nA DataFrame object is a table with named columns, much like data frames in R or Pandas. We can create them manually by providing a list of rows (as tuples) and a list of column names:\ndf = spark.createDataFrame([\n    (\"Yellow-billed duck\", \"Anas\", \"Undulata\", 1839),\n    (\"Laysan duck\", \"Anas\", \"Laysanensis\", 1892),\n    (\"Mallard\", \"Anas\", \"Platyrhynchos\", 1758),\n    (\"Eurasian wigeon\", \"Mareca\", \"Penelope\", 1758)\n    ],\n    [\"Common name\", \"Genus\", \"Species\", \"Year\"]\n)\n\n\n\nYou probably won’t create most of your data frames by manually typing them into a script. Instead, the data already exists somewhere else: a data file, a SQL database, etc. You simply need Spark to load that data.\n\n\nWe can ask Spark to create a DataFrame object from a data file; it understands a whole bunch of data formats:\nppl = spark.read.load(\"dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta\")\nNow ppl is a data frame. We can view the first few rows:\nppl.show()\n\n\n\nIn Databricks, Spark is integrated with a “catalog” of data tables. This catalog is much like a PostgreSQL database, insofar as it contains a bunch of tables, and there can be complex permissions set up to control who has access to which tables.\nIn our case, the catalog is built on Apache Hive, and stores its data in distributed fashion on Azure’s cloud storage system.\nThere are sample data tables included with Databricks:\ndf = spark.read.table(\"samples.nyctaxi.trips\")\nWe’ll discuss the catalog in more detail in ?@sec-spark-catalog.\n\n\n\n\nOur Python code can do transformations, like filter:\nfiltered = df.filter(df['fare_amount'] &gt; 21)\nThis creates a new DataFrame of the results.\nNotice the magic happening here: if the data file were huge, this transformation is automatically distributed to all nodes in the cluster. But not now—later, when we try to use filtered, for example by writing filtered.show().\nSince every operation returns a new DataFrame, we can write operations like\ndf.groupBy(\"pickup_zip\").count()\nThis returns a new DataFrame with columns pickup_zip and count. A lazy DataFrame, which is only calculated when we ask for it.\nYou can think of this as a way of building up a MapReduce job. You write the sequence of transformations you need—even for loops and whatever else you want—and at the end, when you request the result, Spark rearranges and distributes the work to the executors so it can be calculated even on a massive dataset.\nYou still have to figure out how to structure your operation as a series of transformations and reductions, but it’s a bigger set of transformations and reductions, and you can compose many small ones together instead of cramming the operation into one big Map and Reduce.\n\n\n\nOnce we have a DataFrame, we can find the operations available to us in a few places:\n\nThe DataFrame class documentation lists all the methods, such as groupBy, filter, join, and so on.\nThe Spark SQL functions list gives functions that can be imported from pyspark.sql.functions and used in DataFrame operations—things like mathematical operators, date functions, and other functions you might use in data manipulation (much like PostgreSQL’s built-in functions)."
  },
  {
    "objectID": "large-scale-data/spark.html#manipulating-dataframes",
    "href": "large-scale-data/spark.html#manipulating-dataframes",
    "title": "Apache Spark",
    "section": "",
    "text": "Because we’re telling Spark what operations to perform, rather than performing themselves in our own code, we have to write operations in a way Spark understands. This requires some adjustment from what you’re used to with Pandas and other Python work.\nMany of these operations come from pyspark.sql.functions, so it’s common to import that module, either entirely or just for specific functions:\n# import entire module\nimport pyspark.sql.functions as F\n\n# or import just a few functions you need\nfrom pyspark.sql.functions import col, desc\nIn the examples below, we’ll use the first approach and import the entire module as F.\nYou can find a full list of functions in the PySpark documentation.\n\n\nIn Spark operations, we need some way to refer to specific columns. The col() function lets us identify columns by name as a string, e.g. F.col(\"foo\") to refer to the foo column.\nIf you have a DataFrame named df, you can also use df.foo or df[\"foo\"] to refer to columns in it. You will often see code using different ways to refer to columns. If you work at a company, they’ll probably have a standard style for Spark code and use one specific way to refer to columns.\nUsing any of these methods returns a Column object representing the column. These objects have some useful methods, such as alias(), for giving a column a new name, much like AS in SQL. For example, F.col(\"foo\").alias(\"foo2\") is like SELECT foo AS foo2.\n\n\n\nIn programming languages, expressions are operations that yield a value, such as 2 + 2 or (\"foo\", \"bar\") (whose value is a tuple containing two strings). In many Spark operations, we’ll want to write expressions that refer to entire columns, to describe how we’ll be modifying and combining columns.\nThe col() function returns a Column object, and Column objects overload many operators to turn them into Spark expressions. For example,\n# represents adding 2 to every value\nF.col(\"foo\") + 2\n\n# represents adding two columns together\nF.col(\"foo\") + F.col(\"bar\")\nSpark achieves this by overloading the basic Python operators like + and *, so instead of doing literal multiplication, they create objects representing the Spark operations being done. This means that you cannot use ordinary Python functions to operate in expressions, but have to use the Spark equivalents:\nimport math\n\n# doesn't work\nmath.sqrt(F.col(\"foo\"))\n\n# works\nF.sqrt(F.col(\"foo\"))\nLogical combinations of expressions must be expressed using the |, &, and ~ operators, not the usual or, and, and not keywords in Python:\n(F.col(\"foo\") &gt;= 18) & ~F.col(\"bar\")\nNotice the extra parentheses in this expression. The |, &, and ~ operators have higher precedence than &gt;= and other comparison operators, so without parentheses, the expression would be interpreted as F.col(\"foo\") &gt;= (18 & ~F.col(\"bar\")), which is definitely not what we want. This is an unfortunate consequence of PySpark’s operator overloading approach.\nWe can use other Column methods in expressions to obtain useful results. For example, maybe we want to make a new column whose value depends on the value of another:\nF.when(F.col(\"trip_distance\") &gt;= 10, \"Long\").otherwise(\"Short\")\nMany PySpark functions accept a string column name as well as a column expression. (The type is listed as ColumnOrName in the documentation.) For example, we can write this both ways:\nF.sqrt(F.col(\"foo\"))\nF.sqrt(\"foo\")\nBut to take the square root of the sum of two columns, we should use an expression:\nF.sqrt(F.col(\"foo\") + F.col(\"bar\"))\nYou can also use the expr() function to write expressions as strings that Spark will parse and convert into equivalent form. These are identical to the earlier examples:\nF.expr(\"foo + 2\")\n\nF.expr(\"foo + bar\")\n\nF.expr(\"sqrt(foo)\")\n\nF.expr(\"sqrt(foo + bar)\")\nThese functions all return Column objects, so you can do any of the Column operations on them, such as setting an alias.\n\n\n\nMuch of what you do with Spark DataFrames will be selecting specific columns and doing calculations with them to make new DataFrames. In SQL, that’s the role of SELECT, which can select columns but also make new ones by combining existing ones; in dplyr, that’s the role of select() and mutate().\nThe select() method on DataFrame objects is similar. It takes multiple arguments, one for each column, and the columns can be expressions giving calculations. For example, building on our examples so far, we can write:\ndf.select(F.col(\"foo\"), F.col(\"bar\"), F.col(\"foo\") + F.col(\"bar\"),\n          F.sqrt(F.col(\"foo\")))\nThe selectExpr() method does the same, but with expression strings:\ndf.selectExpr(\"foo\", \"bar\", \"foo + bar\", \"sqrt(foo)\")\nWe can also create new columns using the withColumn() method, in which we provide the column name and value:\ndf.withColumn(\"fooBarSum\", F.col(\"foo\") + F.col(\"bar\"))\nThis returns a new data frame with the additional column. If you need to calculate and add many new columns, use withColumns(), which takes a dictionary mapping column names to values.\n\n\n\nTo filter a DataFrame’s rows, we need an expression whose result has as many rows as the DataFrame, and whose values are either true or false. Rows with a True value will be kept.\nDataFrames have filter() and where() methods to do the filtering; these are identical, and the two names are provided just because both names are commonly used in different languages. Both take an argument that is the expression to use for filtering. For example:\ndf.filter(F.col(\"foo\") &gt; 10)\n\n# equivalently:\n\ndf.filter(\"foo &gt; 10\")\nPart of Spark’s optimization process is to analyze the filters in an operation and group them together, so the data is filtered before any more expensive operations are done. This means that chaining multiple filters is fine, since Spark will do them at once:\ndf.filter(\"foo &gt; 10\") \\\n    .filter(\"bar &lt; 4\") \\\n    .filter(\"sqrt(score) != 7\")\n\n\n\nTo order a DataFrame’s rows, we use its orderBy() method. This accepts one or more column names to sort by. As with every Spark feature so far, there’s more than one way to do it:\n# using desc() to specify the direction\ndf.orderBy(F.desc(\"trip_distance\"))\n\n# referring to bare column name\ndf.orderBy(\"trip_distance\", ascending=False)\n\n# using a Column object\ndf.orderBy(df.trip_distance, ascending=False)\nWe can specify multiple columns to sort on, so later columns will be used to break ties in the earlier columns:\ndf.orderBy(F.desc(\"trip_distance\"), F.asc(\"fare_amount\"))\n\nExercise 1 (Simple selections and filters) Load the sample taxi data in a Databricks notebook:\ndf = spark.read.table(\"samples.nyctaxi.trips\")\nUsing this data frame,\n\nSelect all the rows with a distance greater than 15 in with pickup in ZIP code 11422.\nCalculate a new column representing the fare amount per unit distance. Call this column fare_rate.\nOutput (with show()) a data frame containing only the trip distance, fare rate, and dropoff ZIP. Sort it by fare rate in descending order.\n\nYour answers will build on each other to produce one expression that does the whole thing.\n\n\nSolution. \n# 1\ndf.filter(\"trip_distance &gt; 15\").filter(\"pickup_zip = 11422\")\n\n# 2\ndf.filter(\"trip_distance &gt; 15\") \\\n  .filter(\"pickup_zip = 11422\") \\\n  .withColumn(\"fare_rate\", F.col(\"fare_amount\") / F.col(\"trip_distance\"))\n\n# 3\ndf.filter(\"trip_distance &gt; 15\") \\\n  .filter(\"pickup_zip = 11422\") \\\n  .withColumn(\"fare_rate\", F.col(\"fare_amount\") / F.col(\"trip_distance\")) \\\n  .select(\"trip_distance\", \"fare_rate\", \"dropoff_zip\") \\\n  .orderBy(F.desc(\"fare_rate\")) \\\n  .show()\n\n\n\n\n\n\nThe DataFrame class has a groupBy() method to group data frames, much like GROUP BY in SQL or group_by() in dplyr. This takes the columns to group by as arguments:\ndf.groupBy(\"foo\")\n\n# group by both columns, so there is one group per combination\ndf.groupBy(\"foo\", \"bar\")\n(Notice here we have used string column names, since groupBy() says it accepts arguments of type ColumnOrName. We could also use col().)\nThe result of groupBy() is a GroupedData object. This is a different object with different methods. The method you’re most likely to want is agg(), which does aggregation. Just like select(), it takes as many arguments as you want columns, each argument can be a different column expression. The expressions can use any of Spark’s built-in aggregate functions.\nFor example:\ndf.groupBy(\"foo\") \\\n    .agg(F.min(F.col(\"score\")).alias(\"min_score\"),\n         F.max(F.col(\"score\")).alias(\"max_score\"))\nIt also takes a dictionary argument. If given a dictionary, the keys should be column names, and the values should be strings naming the aggregate function to use:\ndf.groupBy(\"foo\") \\\n    .agg({\"score\": \"min\", \"latency\": \"max\"})\nIn this form, you can only do one aggregate function per column, since dictionaries can only contain each key once.\n\nExample 1 (Operating on the events data) We’ve loaded our events data (from ?@sec-example-db) into Databricks. Let’s load it:\nevents = spark.read.table(\"hive_metastore.default.events\")\nNow we can begin Spark operations. For example, to get the average score per student:\n# a Spark GroupedData object\npersona_scores = events.groupBy(\"persona\")\n\npersona_scores.avg(\"score\")\n(GroupedData objects have several methods for common aggregate functions, like avg().)\nNotice that Spark does not display the data frame—that’s because it hasn’t done the calculation yet! We can use .show() to have it print it out, or .collect() to return the results as a Python variable we can work with.\nEach operation returns a DataFrame that we can conduct further operations on, by chaining operations in sequence. For example, let’s try reproducing a query from ?@sec-grouping-aggregate:\nselect persona, avg(score) as mean_score\nfrom events\nwhere moment &gt; cast('2014-10-01 11:00:00' as timestamp)\ngroup by persona\nhaving avg(score) &gt; 300\norder by mean_score desc;\nIn Spark, we write the same operations in logical order:\nimport pyspark.sql.functions as F\nfrom datetime import datetime\n\ntop_scores = events \\\n    .filter(F.col(\"moment\") &gt; datetime(2014, 10, 1, 11)) \\\n    .groupBy(\"persona\") \\\n    .agg(F.avg(F.col(\"score\")).alias(\"mean_score\")) \\\n    .filter(F.col(\"mean_score\") &gt; 300) \\\n    .orderBy(F.desc(\"mean_score\"))\nBut when we ask for the results (with .show(), .take(), .collect(), or similar), it analyzes the operations to determine the most efficient way to execute them, and only then does it perform the calculations.\nJust like in SQL, we can also ask Spark to explain how it will run these operations on the cluster:\ntop_scores.explain()\n\n\nExercise 2 (Querying the events table) Load the events table into a Spark DataFrame as shown in Example 1. Write DataFrame operations to do the operations in ?@exr-basic-grouping. (By “DataFrame operations” I mean you should use Python methods like in Example 1, and not Spark SQL.)\nFor #4, see the datetime functions provided in pyspark.sql.functions.\n\n\nSolution. We can write the following Spark operations:\nfrom pyspark.sql import functions as F\n\n# 1\nevents.groupBy(\"persona\") \\\n    .agg({\"latency\": \"max\", \"score\": \"avg\"}) \\\n    .filter(F.col(\"max(latency)\") &gt; 565) \\\n    .orderBy(F.col(\"avg(score)\")) \\\n    .show()\n\n# 2\nevents.groupBy(\"persona\") \\\n    .avg(\"score\") \\\n    .filter(F.col(\"avg(score)\") &gt; 600) \\\n    .show()\n\n# 3\nevents.groupBy(\"element\") \\\n    .count() \\\n    .show()\n\n# 4. note that .min() only works on numeric columns, while .agg can apply min()\n# to other column types\nevents.groupBy(\"persona\") \\\n    .agg({\"moment\": \"min\"}) \\\n    .select([F.col(\"persona\"), F.date_format(F.col(\"min(moment)\"), \"MMMM d\")]) \\\n    .show()\n\n\nExercise 3 (Taxi aggregates) Using the taxi data from Exercise 1, produce a data frame giving the following summary statistics for each pickup ZIP code:\n\nThe average fare rate (as defined in Exercise 1)\nThe total number of trips from that ZIP (see the count() aggregate function)\nThe minimum and maximum distance traveled\n\nGive each column an appropriate name. Order the result by descending number of trips.\n\n\nSolution. \ndf = spark.read.table(\"samples.nyctaxi.trips\")\n\ndf.withColumn(\"fare_rate\", F.col(\"fare_amount\") / F.col(\"trip_distance\")) \\\n    .groupBy(\"pickup_zip\") \\\n    .agg(F.avg(\"fare_rate\").alias(\"avg_fare_rate\"),\n         F.min(\"trip_distance\").alias(\"min_distance\"),\n         F.max(\"trip_distance\").alias(\"max_distance\"),\n         F.count(\"fare_rate\").alias(\"num_trips\")) \\\n    .orderBy(F.desc(\"num_trips\")) \\\n    .show()\n\n\n\n\nGrouping is conceptually simple: If you group by the columns (a, b, c), the groups are defined by all unique combinations of those three columns.\nBut sometimes we’d like to calculate aggregates for groups, while also providing overall totals. The output table would hence have one row per unique combination, plus one or more summary rows. This is a rollup.\nRollups can be defined with the rollup() method on DataFrames. This creates a GroupedData object that supports all the same aggregation you can do after groupBy().\n\nExample 2 (Score rollup) Returning to the events data loaded in Example 1, let’s get the average score per element, but with a rollup:\nevents \\\n    .rollup(\"element\") \\\n    .agg(F.avg(F.col(\"score\")).alias(\"mean_score\")) \\\n    .orderBy(\"element\")\nNotice the row with a NULL for element. This row represents the mean score for all rows in the events table.\n\nJust as you can group by multiple columns, you can rollup across multiple columns. The result is hierarchical: If you rollup by (a, b, c) and aggregate, you get\n\none row per unique combination of (a, b, c),\none row per unique combination of (a, b), averaging over all c,\none row per unique value of a, averaging over all b and c,\none row averaging over all data.\n\nThe columns being averaged over are always represented with NULL. To prevent confusing problems, ensure that the columns you rollup over do not already contain NULL values, or filter them out first.\n\nExercise 4 (Roll up the taxis) Return to the taxi data from Exercise 1. Produce a table calculating average fares for every (pickup, dropoff) ZIP code pair, for every pickup ZIP code, and overall.\n\n\nSolution. \ndf = spark.read.table(\"samples.nyctaxi.trips\")\n\ndf \\\n    .rollup(\"pickup_zip\", \"dropoff_zip\") \\\n    .agg(F.avg(\"fare_amount\").alias(\"mean_fare\")) \\\n    .orderBy(\"pickup_zip\", \"dropoff_zip\")\n\n\n\n\nWe can take rollups a step further with cubes. Cubes are not hierarchical: if you make a cube of (a, b, c), you get aggregates over every subset:\n\n(a, b, c)\n(a, b)\n(b, c)\n(a, c)\na\nb\nc\noverall\n\nThe cube() method of DataFrame objects does this, and you can use aggregate functions as usual.\n\nExercise 5 (Cube the taxis) Repeat Exercise 4, but obtain average fares for every pickup ZIP, every dropoff ZIP, and every combination of them. Order by average fare in descending order. Which ZIPs or combinations have the highest average fares?\n\n\nSolution. \ndf \\\n    .cube(\"pickup_zip\", \"dropoff_zip\") \\\n    .agg(F.avg(\"fare_amount\").alias(\"mean_fare\")) \\\n    .orderBy(F.desc(\"mean_fare\"))\nNote that all the highest fares come from ZIPs or combinations with only one trip.\n\n\n\n\n\nDataFrame objects have a join() method that supports the usual joins from SQL (?@sec-joins). For example, if we have two data frames df1 and df2, we can write a join like so:\ndf1.join(df2, df1.event_id == df2.id, how=\"left\")\nRecall that df1.event_id is a way to refer to the event_id column on the df1 DataFrame, so it is a Column object; setting two Column objects equal to each other produces an object representing the join condition we are requiring. This is similar to ON in a SQL query. We can get left joins, right joins, inner joins, outer joins, and other combinations with the how argument.\nThe join() method returns a DataFrame representing the joined tables, so doing multiple joins simply requires calling the join() method repeatedly.\n\nExample 3 (Joining in TPC-H) The Transaction Processing Performance Council builds benchmarks to measure how fast different database systems are. One of their sample datasets is TPC-H, which simulates an order and inventory management system. The database contains a number of parts that can be provided by various suppliers; there are many customers who can submit orders, each order consisting of multiple parts, potentially from several suppliers. This data is split across eight tables in the samples.tpch schema:\n\n\n\nThe TPC-H table schema, from the TPC Benchmark H Standard Specification, revision 3.0.1. Column names are prefixed by the value given in parentheses, so for example all lineitem columns begin with l_. Arrows represent foreign keys. Here SF = 5, so there are 30 million line items and 50,000 suppliers.\n\n\nSuppose we’d like to find all the suppliers for parts in order #13710944. We need to find all the line items in that order (lineitem), join them with part suppliers (partsupp), and join those with suppliers (supplier). Then let’s pull out the part identifier (partkey), name of the supplier (s_name), and the available quantity (ps_availqty) for that part.\nWe begin by getting all the tables:\nlineitem = spark.read.table(\"samples.tpch.lineitem\")\npartsupp = spark.read.table(\"samples.tpch.partsupp\")\nsupplier = spark.read.table(\"samples.tpch.supplier\")\nFirst we must filter lineitem to get the order we want. Then we must do the joins. Notice how we can set multiple join conditions by providing them as a list.\nlineitem \\\n    .filter(F.col(\"l_orderkey\") == 13710944) \\\n    .join(partsupp, [lineitem.l_partkey == partsupp.ps_partkey,\n                     lineitem.l_suppkey == partsupp.ps_suppkey],\n          how=\"left\") \\\n    .join(supplier, supplier.s_suppkey == partsupp.ps_suppkey) \\\n    .select(\"l_partkey\", \"s_name\", \"ps_availqty\")\n\n\nExercise 6 (Joining harder in TPC-H)  \n\nProduce a table with one row per customer. For each customer, calculate their total expenditure (sum of the price of all orders they have submitted), the total number of line items ordered, and the total number of orders. Ensure the table includes customers who have never made any orders.\nProduce a table with one row per supplier. For each supplier, report their name and the total number of parts they have supplied to meet orders (meaning the sum of the quantity of their parts across all orders).\nProduce a summary table giving pricing information for all items shipped up to June 1, 1998 (l_shipdate). There should be one row per return flag and line status (l_returnflag and l_linestatus); each row should give:\n\nthe number of line items with this status/return flag\nthe total quantity of line items shipped\nthe average quantity of items per order\ntheir total base price (l_extendedprice)\ntheir total discounted price (l_extendedprice * (1 - l_discount))\n\n\n\n\nSolution. \n# 1\norders = spark.read.table(\"samples.tpch.orders\")\nlineitem = spark.read.table(\"samples.tpch.lineitem\")\n\n# TODO Fix to include customers who haven't made orders\norders \\\n    .join(lineitem, orders.o_orderkey == lineitem.l_orderkey) \\\n    .groupBy(\"o_custkey\") \\\n    .agg(F.sum(\"o_totalprice\").alias(\"total_spend\"),\n         F.count(\"l_linenumber\").alias(\"total_num_items\"),\n         F.count_distinct(\"o_orderkey\").alias(\"num_orders\")) \\\n    .orderBy(\"o_custkey\") \\\n    .show()\n\n# 2\npartsupp = spark.read.table(\"samples.tpch.partsupp\")\n\nlineitem \\\n    .join(partsupp, [partsupp.ps_partkey == lineitem.l_partkey,\n                     partsupp.ps_suppkey == lineitem.l_suppkey],\n          how=\"left\") \\\n    .groupBy(\"l_suppkey\") \\\n    .agg(F.sum(\"l_quantity\").alias(\"total_quantity\")) \\\n    .join(supplier, lineitem.l_suppkey == supplier.s_suppkey) \\\n    .select(\"s_suppkey\", \"s_name\", \"total_quantity\") \\\n    .show()\n\n# 3\n# Inspired by https://www.tpc.org/TPC_Documents_Current_Versions/pdf/TPC-H_v3.0.1.pdf\n# section 2.4.1\nfrom datetime import date\nlineitem \\\n    .filter(F.col(\"l_shipdate\") &lt; date(1998, 6, 1)) \\\n    .groupBy(\"l_returnflag\", \"l_linestatus\") \\\n    .agg(F.count(\"l_linenumber\").alias(\"total_items\"),\n         F.sum(\"l_quantity\").alias(\"quantity_shipped\"),\n         (F.sum(\"l_quantity\") / F.count_distinct(\"l_orderkey\")).alias(\"avg_per_order\"),\n         F.sum(\"l_extendedprice\").alias(\"total_base_price\"),\n         F.sum(F.expr(\"l_extendedprice * (1 - l_discount)\")).alias(\"total_discounted_price\")) \\\n    .show()\n\n\n\n\nIn SQL, we learned about window functions (?@sec-sql-window), which allow us to perform aggregation without summarizing the data to get one row per group. Instead, we can select data and augment it with columns that calculate aggregates or relate the row to others in a group.\nIn grouping, we use groupBy to define the groups. In windowing, we have to define which rows will be included in the window for any particular row. For example, in ?@sec-sql-window we wrote the following query of the events table:\nSELECT\n  id, persona, element, score, moment,\n  ((score - avg(score) OVER (PARTITION BY persona ORDER BY moment)) /\n   stddev_samp(score) OVER (PARTITION BY persona ORDER BY moment)) AS z_score\nFROM events\nLIMIT 10;\nHere, the window “around” each row consists of all rows with the same persona (PARTITION BY persona). But the window only considers those rows occurring before this one (ORDER BY moment).\nIn Spark, we need a way to specify this. A special Window class lets us configure the window to use:\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window \\\n    .partitionBy(\"persona\") \\\n    .orderBy(F.asc(\"moment\")) \\\n    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\nHere rowsBetween() specifies which rows of the window count: here, from the first row in the window to the current row. We can also use rangeBetween, which specifies the values to include in the window: .rangeBetween(-120, -1) means to include all rows whose ordering column value is between 120 and 1 unit less than this row. If moment were measured in seconds, this would give us a window of all events between 120 and 1 second before this row’s event, regardless of how many events that is. Meanwhile, rowsBetween(-120, -1) would give us all rows from 120 before to 1 before this one (in time), regardless of their absolute times.\nNow, any Column object—such as the output of any aggregate function—has an over() method that lets us specify the window to use. So to replicate the SQL query above, we write:\nevents = spark.read.table(\"hive_metastore.default.events\")\n\nevents.select(\n    F.col(\"id\"), F.col(\"persona\"), F.col(\"element\"), F.col(\"score\"), F.col(\"moment\"),\n    ((F.col(\"score\") - F.avg(\"score\").over(window_spec)) /\n     F.stddev_samp(\"score\").over(window_spec)).alias(\"z_score\")\n)\n\nExercise 7 (Are windows inclusive?) The rowsBetween() method of the Window class lets us specify which rows are included in the window. Above, we chose the range (Window.unboundedPreceding, Window.currentRow).\nIs this inclusive or exclusive? That is, is the current row included in the window?\nCreate a test data frame (using spark.createDataFrame()) that would let you test this, define a window, and query the data frame. Interpret your result: is it inclusive or exclusive?\n\n\nSolution. TODO Yes, they’re inclusive."
  },
  {
    "objectID": "large-scale-data/spark.html#spark-sql",
    "href": "large-scale-data/spark.html#spark-sql",
    "title": "Apache Spark",
    "section": "",
    "text": "You might have noticed that the basic Spark operations, like maps, grouping, and aggregation, sound a lot like the kinds of things you can do in SQL. That’s not a coincidence: the relational model of operations on tables still holds up after all these years.\nAs a bonus, Spark understands SQL syntax. You can run operations on data stored in catalogs:\nsqlDF = spark.sql(\"\"\"\nSELECT persona, avg(score)\nFROM hive_metastore.default.events\nGROUP BY persona\"\"\")\nsqlDF.show()\nsqlDF is just another DataFrame, and so .show() prints it out. Because it’s just a DataFrame, we could also do additional operations on it in Python (like filter(), avg(), or any of the other methods we’ve used), and the entire sequence of operations will only be done when we perform an action like show().\nThere are Spark SQL functions for math, logic, dates, and other common needs (just like SQL), as well as aggregate functions and window functions. You can JOIN multiple tables just as in SQL. There are also Spark-specific extensions to give Spark hints about the most efficient way to execute a query. To find the syntax definition and list of built-in functions, see the Spark SQL reference.\nIf you have a DataFrame in Python and want to access it using SQL, regardless of whether it’s already in a catalog, you can do that too:\n# Register the DataFrame as a SQL temporary view\nsome_dataframe.createOrReplaceTempView(\"some_data\")\nThe createOrReplaceTempView() method we used above creates what Spark calls a temporary view: it lets us see the data already in a DataFrame. (And remember, a DataFrame might represent various transformations of the original data that have not been executed yet; when you write a SQL query on such a DataFrame, Spark will work out how to calculate the original transformations and the SQL operations together, in one large operation.) This view only lasts as long as our Spark session, and goes away when our application exits.\nWe can also create permanent tables, much like in Postgres. While Postgres (just like most relational databases) uses specialized file formats that support indexing, transactions, atomic updates, and the other ACID features of relational databases, Spark can use any file that it knows how to read. This is done with a specialized CREATE TABLE command that includes options specifying the file format and file location; the file could be stored on a distributed filesystem like HDFS, making Spark SQL suitable for enormous datasets.\n\nExample 4 (Operating on the events data in Spark SQL) Continuing Example 1, we can run the same query in Spark SQL.\ntop_scores = spark.sql(\"\"\"\nselect persona, avg(score) as mean_score\nfrom hive_metastore.default.events\nwhere moment &gt; cast('2014-10-01 11:00:00' as timestamp)\ngroup by persona\nhaving avg(score) &gt; 300\norder by mean_score desc\n\"\"\")\ntop_scores.show()\nThis is rather anticlimactic, since it’s exactly the same query we ran before, and it runs just fine.\n\n\nExample 5 (TPC-H data in Spark SQL) Returning to the TPC-H data from Example 3, we can conduct the same join with Spark SQL.\nspark.sql(\"\"\"\nSELECT l_partkey, s_name, ps_availqty\nFROM samples.tpch.lineitem\nLEFT JOIN samples.tpch.partsupp ON l_partkey = ps_partkey AND l_suppkey = ps_suppkey\nLEFT JOIN samples.tpch.supplier ON s_suppkey = ps_suppkey\nWHERE l_orderkey = 13710944\n\"\"\")\nNotice we have to refer to the tables by their fully qualified names, so Spark knows where to find them.\n\nIt’s even possible to have Spark to behave like a SQL server so that any language that supports SQL can connect and send SQL queries. Note, however, that while Spark is good at doing large calculations on huge datasets quickly, it is not designed to do small operations with low latency—so it is not a replacement for a standard relational database to, say, run a website that needs to respond to requests in 100 milliseconds. It is better suited for backend analysis tasks that run periodically.\n\nExercise 8 (TPC-H data in Spark SQL) Repeat Exercise 6 in Spark SQL.\n\n\nSolution. TODO"
  },
  {
    "objectID": "large-scale-data/spark.html#exercises",
    "href": "large-scale-data/spark.html#exercises",
    "title": "Apache Spark",
    "section": "",
    "text": "Exercise 9 (Reading tea leaves in the TPC-H schema) Refer to the TPC-H schema above, and reflect back on our schema design principles (?@sec-schema-design).\n\nWhy should partsupp (part suppliers) be separate from part and supplier? Couldn’t every part just have a supplier? What does this allow extra?\nName three features of the business that are implied by the schema. For example: The business can ship your order in multiple parts, because shipping information is given separately for each lineitem, rather than once per order.\n\n\n\nSolution. \n\nThis allows parts to be supplied by more than one suppler. There is a many-to-many relationship between parts and suppliers encoded in the partsupp table. Maybe some items are generic and made by many different companies.\nHere are some examples:\n\nThe company keeps separate inventory for parts supplied by each supplier, rather than inventory for the part regardless of supplier. It must care who supplies parts. (Available quantity is in partsupp, not in part.)\nEach item can be returned separately, rather than returning the entire order.\nThe company groups nations by regions, so it must aggregate its reporting by region.\nCustomers only exist in one country each—they’re not multinational.\n\n\n\n\nExercise 10 (LendingClub and Spark) LendingClub offers various types of loans to consumers and businesses. From 2007 to 2018, they offered “peer-to-peer” loans: customers could request personal loans, and individual investors could decide how to allocate their money to fund those loans. LendingClub handled payments, while the investors got a share of the loan interest.\nTo allow investors to make informed decisions, LendingClub released a large dataset of loans, including loan amounts, credit scores of borrowers, payment history, and various other financial and payment information. The data for all accepted loans is at abfss://sampledata@lsd2025storage.dfs.core.windows.net/lending-club.parquet, available from the Class Shared Cluster. A data dictionary spreadsheet describing all the variables is in the hive_metastore.default.lc_data_dictionary table.\nUse spark.read.parquet() to load the data into the variable df. Print out df to see a list of all the available columns.\nUse df.count() to get the number of rows of data.\nNow you’ll do a series of analysis tasks as if you were a data analyst at LendingClub. Your notebook should use Spark do its calculations; you cannot load the entire dataset into Python, and you cannot use Pandas.\nYour notebook should calculate, for each month (according to issue_d), the following summaries:\n\nThe total number of loans issued and, separately, the number of 36-month and 60-month loans issued (see term)\nThe total funded amount of those loans (funded_amnt) and the total remaining principal to be paid (out_prncp)\nThe percentage of loans with interest rates (int_rate) greater than 10%\nThe percentage of loans that have been fully paid by now (see loan_status): overall, for grade A loans, and for grade F loans (see grade), as separate columns\nThe percentage of loans on a hardship payment plan (hardship_flag)\n\nThe result should be a data frame with one row per month and one column per summary statistic. Ensure the columns have meaningful names. Print the first few rows of this table in your notebook so I can grade your work.\n\n\nSolution. TODO\n\n\nExercise 11 TODO exercise using TPC-H with rollups, cubes, windows"
  },
  {
    "objectID": "large-scale-data/spark-data.html",
    "href": "large-scale-data/spark-data.html",
    "title": "Spark Data Management",
    "section": "",
    "text": "So far, I have suggested that distributed data systems like Spark (or MapReduce/Hadoop) are best suited for problems where your data is so enormous that it cannot conveniently fit on one machine. This is for a few reasons:\n\nAny system that runs on multiple machines will be more complicated, and harder to use, than a system that runs on just one machine.\nCoordinating and transmitting data between machines is almost always slower than using the data on one machine.\nSingle-machine database systems have been in use for decades, so their ecosystems are very mature. You can find detailed books, packages for every conceivable programming languages, thousands of online tutorials, example projects, and so on. Distributed systems are much newer and change rapidly, so it can be hard to find up-to-date information.\n\nBut sometimes you do need distributed data. Given your statistical skills, the distribution of this data is likely to be handled by someone else: other teams at your company will be running the systems that produce and store the data, and your job will be to analyze it to answer business questions. So let’s talk about how data would be stored and how you get it into Spark.\n\n\nBack in ?@sec-distributing-data, we discussed distributed file systems. These run on multiple servers and automatically store files redundantly. To store a petabyte of data, we might have 100 machines, each with 40 TB of hard drives installed—allowing the distributed file system enough space to store multiple copies of each file, on multiple machines, so that the failure of a few machines does not mean the loss of any data.\nApache Spark supports several distributed file systems, including HDFS, natively. It can load data directly from HDFS, and also from S3 and several other popular storage systems.\n\n\n\nSpark supports a whole bunch of data formats—a confusingly large number, actually. You can read text files, CSVs, JSON, Parquet, ORC, Avro, HBase, and various other things with connectors and libraries.\nThe SparkSession object you get when initializing PySpark has a read attribute that returns a DataFrameReader object with methods for loading numerous different data formats. Let’s look at two common formats, CSV and Parquet.\n\n\nCSV files can be read with the csv() method on the DataFrameReader:\nspark.read.csv(\"some-csv-file.csv\")\nThis returns a DataFrame. However, you may need some additional steps to get the DataFrame you want. By default, Spark does not read the first line to get column headers, so the columns will have names like _c0, _c1, _c2, etc. (much like R uses V1, V2, and so on). Use the header=True option to have it read column headers.\nSecond, CSV files do not specify the types of their columns. The type must be inferred: if you read a column and it only contains numbers, you can guess it’s a numeric column. But that requires reading the entire file to check its contents, a costly operation for a large dataset. The inferSchema option can be used to turn this on.\nAlternately, the schema argument can be used to specify the column types, using the same syntax you’d use inside a SQL CREATE TABLE command:\nspark.read.csv(\"some-csv-file.csv\",\n               schema=\"id INTEGER, name STRING, post_date DATE\")\nA table of supported types (partway down the page) is given in the Spark documentation.\nFinally, you may need to tell Spark how null (missing) values are represented. For example, if they’re stored as NA in the CSV file, use the nullValue=\"NA\" argument.\nBesides CSVs, many other formats are supported. So what should you choose? When you’re scraping your data and preparing it for Spark, what file format is best?\nFor perfectly ordinary data files, CSVs are fine. But as the data file gets large—tens or hundreds of megabytes—reading the CSV can become slow. To access a single column or a small subset of data you usually have to parse the entire file. On huge datasets that defeats the purpose of having a distributed system.\n\n\n\nParquet is designed as a columnar data store. Instead of storing one row at a time, it stores one column at a time, with metadata indicating where to find each column in the file. It’s very efficient to load a subset of columns. Parquet files can also be partitioned, meaning the data is split into multiple files according to the value of a certain column (e.g., the year of the entry). This is nice for huge datasets, and on a distributed file system lets different chunks exist on different servers.\nReading a Parquet file is similar to reading a CSV:\nspark.read.parquet(\"some-file.parquet\")\nThis produces a DataFrame. Parquet files support all the data types of Spark, so things like timestamps, booleans, and arrays are automatically read and converted, unlike a CSV where you might have to manually parse a date or specify a schema.\nArrow is the name of the in-memory format for Parquet, i.e. how the data is laid out in memory once it is loaded. This is standardized so you can load Parquet data into Arrow and pass it between libraries in programming languages without any conversion.\nThere is a Python library for Parquet and Arrow, and an R package that can read and write Parquet.\n\n\n\n\nThe whole point of a distributed file system, and of distributed data analysis, is that data might be too big to fit on a machine. If we have an enormous dataset, it can be broken into pieces for storage. That also provides an advantage in analysis with Spark: each Spark executor can read in pieces of the file instead of the whole thing.\nThese pieces are called partitions. When reading a large data file from a block file system like HDFS, Spark will create one partition per block, meaning each executor will hold only part of the DataFrame.\nDepending on the operations you do to the data, Spark might have to move the partitioned data between executors: if you sort the DataFrame, for instance, it will need to collect it all into one place in sorted order. This will cost time as the data is sent between machines. But it does suggest we could choose how to partition the data for maximum efficiency. Spark allows for data files to themselves be partitioned or bucketed, meaning the data is split into multiple files when stored.\n\n\nSuppose you are working with account data from a bank. Each transaction in a bank account has a date, and most of your analyses use data within a certain date range: you produce monthly reports, quarterly reports, and annual reports, each doing complicated analysis in that date range, but rarely need to work with the entire dataset from all dates.\nIf your data has a column for month or year, you could choose to partition the data manually by that column. That means storing the data in a special folder where every month or year has its own data file whose name includes the date.\nIf we are writing the data from Spark, we can use the partitionBy() method on DataFrameWriter to do this:\ndf.write.partitionBy(\"year\").parquet(\"foo.parquet\")\nInstead of a single file foo.parquet, we’ll instead get a folder named foo.parquet/, containing folders with names like year=2022/, themselves containing Parquet data files.\nWe can read the file normally:\ndf_new = spark.read.parquet(\"foo.parquet\")\nSpark automatically recognizes this is a partitioned file that has been split by year. If we write a Spark operation like\ndf_new.filter(col(\"year\") &gt;= 2020) \\\n    .groupBy(\"accountno\") \\\n    .agg(...) \\\n    ... # more Spark stuff here\nthen when we execute an action like .collect() or .show(), Spark will know that its executors need only read the files for 2020 and later. The rest of the data will never be read into memory. And if we write an operation like\ndf_new.filter(col(\"year\") &gt;= 2020) \\\n    .groupBy(\"year\") \\\n    .agg(...) \\\n    ... # more Spark stuff here\nthen each file can be read on one executor, and the grouping and aggregating can be performed on the same executor—requiring no shuffling of data between machines.\nExplicitly partitioning by a specific column works best for columns that have dozens of unique values, not millions; you want partitions that are large enough that each can be comfortably managed by an executor, but are not so small that Spark must read millions of tiny files. (Many distributed file systems are not good at managing many small files, and are better at working with fewer larger files.)\n\n\n\nIn bucketing, we can organize the data in a more flexible way than partitioning. We can specify multiple columns to use in bucketing, and the result is something like a hash table: the columns are combined together and hashed into a single value, and that value determines which bucket the data is stored in. Multiple unique values can be stored in the same bucket, unlike in partitioning, where every partition is a separate file.\nFor example,\ndf.write.bucketBy(10, \"year\", \"month\", \"account\").parquet(\"foo.parquet\")\ntells Spark to use the year, month, and account columns to split the data into 10 buckets. This ensures that all records for a particular account in a particular month will be placed into the same bucket, and hence the same file. Again, foo.parquet will be a directory of multiple files, and Spark will automatically understand the bucketing when we try to read the data.\nIf we load this data into a Spark DataFrame, it will only read the buckets necessary to fulfill a query. This is useful for filtering—if we want a particular account’s records in a particular month—but also is useful for joins. Suppose we want to join the entire table with another data frame, and the join is on the columns used in bucketing. Each executor can read in one bucket and join it with the matching rows in the other table, and the results can be concatenated together at the end. If the other table is also bucketed, each executor may only need to read certain buckets from each table, and do the joining within those buckets, rather than needing to read the entire tables into memory.\n\n\n\n\nSo far we have seen two ways of accessing data from Spark. The boring way is to read a data file directly from the file system. Spark can read from the local filesystem (if the data is available on all executors), but it can also read from distributed filesystems like HDFS, or from cloud data stores like Amazon S3 and Azure Blob Storage.\nThe more interesting way is through a metastore. A metastore is essentially a registry of available data files, their types, their locations, and additional metadata—such as who is allowed to access and modify them. This becomes useful in a large organization, where you need data governance: control over who is officially in charge of certain data sets and who is allowed to access them, records showing when they were used, a search system to find datasets from across the organization, and so on.\nFor this reason, metastores seem pointless when you are just noodling around with data on your own. And they are useless in that context. But think of them as a technical solution to the organizational problems that arise when your company is big and complex enough to require a data processing system as big and complex as Spark.\nTraditionally, Spark uses the Apache Hive metastore. This is still very common.\nDatabricks provides its own proprietary system, Unity Catalog, that includes more advanced features. (It is unfortunately not available on our class Azure Databricks setup.) But the basic principles are the same.\n\n\nWe have already used the Hive metastore when we write Spark commands like this:\nevents = spark.read.table(\"hive_metastore.default.events\")\nThe string hive_metastore.default.events is the name of a managed table. Here “managed” means that Hive is responsible for its storage, and all access goes through Hive (via Spark, in our case). The name has three parts:\n\nThe catalog: hive_metastore. A catalog contains schemas. Different users can have access permissions to different catalogs.\nThe schema: default. A schema contains tables. Different users can have access permissions to different schemas.\nThe table: events. A table refers to an actual data table with rows and columns. Different users can have access permissions to different tables.\n\nThis creates a hierarchy. If I want to access hive_metastore.default.events, I need access to the hive_metastore catalog, its default schema, and the events table contained therein.\nYou can’t put tables directly inside catalogs, or put schemas inside tables, or anything like that: you are restricted to a three-level hierarchy.1\nWith managed tables, the data is stored wherever Hive is configured to store it. In systems like Azure Databricks or Amazon’s EMR, it’s usually configured to store its data in the cloud’s object storage system.\n\n\n\nData frames can be written to managed tables using the write attribute. This returns a DataFrameWriter, which has further options to control partitioning and bucketing. For example:\ndf.write.partitionBy(\"some_column\") \\\n    .saveAsTable(\"hive_metastore.default.example_table\")\nSimilarly, bucketing can be done with the bucketBy() method.\n\n\n\n\nOne strength of Spark is the ability to load data from many different sources and work with them in identical ways within Spark. One alternative data source is a relational database that speaks SQL, like PostgreSQL.\nOf course, if all your data fits in a SQL database on one large server, it probably makes sense to do your analysis on that server in SQL. But sometimes you must use data from a SQL database as part of a larger analysis. For example, your company might have log data stored on HDFS in Parquet files, but your analysis needs to match up those logs to user accounts stored in PostgreSQL.\nSpark supports accessing any SQL database that supports JDBC (Java Database Connectivity), a standard Java interface for connecting to databases. Because both SQL and Java are extremely popular, almost every relational database has a JDBC package available. Using it is pretty easy:\ndf = spark.read.format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:postgresql://sculptor.stat.cmu.edu/databasename/\") \\\n    .option(\"user\", YOURUSERNAME) \\\n    .option(\"password\", YOURPASSWORD) \\\n    .option(\"dbtable\", \"name_of_table\") \\\n    .load()\nAs usual, Spark will try to be smart about how it reads the database table, and it will not simply do SELECT * FROM name_of_table to read all the data from memory. For example, if you do a filter() in Spark, it will try to turn this into a WHERE clause for the database. It will also make connections from each executor that try to read different parts of the data in parallel. You can thus benefit from the SQL server’s ability to use indexes to make queries faster, while still benefiting from Spark’s distributed data analysis. You might join the table with other large datasets you’ve loaded into Spark.\n\n\n\n\nExercise 1 (Partioned LendingClub data) Return to the LendingClub data you loaded in ?@exr-spark-lendingclub.\nSpark does not provide a direct way to tell what variable a DataFrame was partitioned by, if any. But DataFrame objects do have an inputFiles() method that lists the one or more data files used as their input.\nGet the inputFiles() list for the LendingClub data file. Examine the file names. What variable is the Parquet file partitioned by?\n\n\nExercise 2 (Fiscally standardized cities) A dataset of information on “fiscally standardized” cities is available at abfss://sampledata@lsd2025storage.dfs.core.windows.net/fisc_full_dataset_2020_update.csv. You can read the full dataset description for details on what it contains; the short version is that it gives revenue, expenses, and tax information on over 200 American cities for over 40 years, adjusting for differences in government structure across states.\n\nLoad the data using spark.read.csv(). Check the schema with the printSchema() method. Make any necessary tweaks to have the column names and schema read correctly (see Section 1.2.1).\nAdd a net_income column to the data, calculated as the difference between its total revenue (rev_general) and total spending (spending_total).\nOnce you have the data correctly formatted, save it as a table in the Hive metastore. Name it something like fisc_andrewid, with your Andrew ID, so it does not collide with any other student’s table. Partition it by the variable that seems most useful to partition by in the remaining steps of this exercise.\nComplete the following steps in Spark SQL using the table you created. First, produce one row per city, giving the year it had the worst net income – the most negative net income – and its revenue, spending, and net income for that year. Order by city and state.\nNow produce one row per city, listing its name, average net income from 1977 to 2020, and standard deviation of net income over that time period."
  },
  {
    "objectID": "large-scale-data/spark-data.html#distributed-file-systems",
    "href": "large-scale-data/spark-data.html#distributed-file-systems",
    "title": "Spark Data Management",
    "section": "",
    "text": "Back in ?@sec-distributing-data, we discussed distributed file systems. These run on multiple servers and automatically store files redundantly. To store a petabyte of data, we might have 100 machines, each with 40 TB of hard drives installed—allowing the distributed file system enough space to store multiple copies of each file, on multiple machines, so that the failure of a few machines does not mean the loss of any data.\nApache Spark supports several distributed file systems, including HDFS, natively. It can load data directly from HDFS, and also from S3 and several other popular storage systems."
  },
  {
    "objectID": "large-scale-data/spark-data.html#loading-data-from-files",
    "href": "large-scale-data/spark-data.html#loading-data-from-files",
    "title": "Spark Data Management",
    "section": "",
    "text": "Spark supports a whole bunch of data formats—a confusingly large number, actually. You can read text files, CSVs, JSON, Parquet, ORC, Avro, HBase, and various other things with connectors and libraries.\nThe SparkSession object you get when initializing PySpark has a read attribute that returns a DataFrameReader object with methods for loading numerous different data formats. Let’s look at two common formats, CSV and Parquet.\n\n\nCSV files can be read with the csv() method on the DataFrameReader:\nspark.read.csv(\"some-csv-file.csv\")\nThis returns a DataFrame. However, you may need some additional steps to get the DataFrame you want. By default, Spark does not read the first line to get column headers, so the columns will have names like _c0, _c1, _c2, etc. (much like R uses V1, V2, and so on). Use the header=True option to have it read column headers.\nSecond, CSV files do not specify the types of their columns. The type must be inferred: if you read a column and it only contains numbers, you can guess it’s a numeric column. But that requires reading the entire file to check its contents, a costly operation for a large dataset. The inferSchema option can be used to turn this on.\nAlternately, the schema argument can be used to specify the column types, using the same syntax you’d use inside a SQL CREATE TABLE command:\nspark.read.csv(\"some-csv-file.csv\",\n               schema=\"id INTEGER, name STRING, post_date DATE\")\nA table of supported types (partway down the page) is given in the Spark documentation.\nFinally, you may need to tell Spark how null (missing) values are represented. For example, if they’re stored as NA in the CSV file, use the nullValue=\"NA\" argument.\nBesides CSVs, many other formats are supported. So what should you choose? When you’re scraping your data and preparing it for Spark, what file format is best?\nFor perfectly ordinary data files, CSVs are fine. But as the data file gets large—tens or hundreds of megabytes—reading the CSV can become slow. To access a single column or a small subset of data you usually have to parse the entire file. On huge datasets that defeats the purpose of having a distributed system.\n\n\n\nParquet is designed as a columnar data store. Instead of storing one row at a time, it stores one column at a time, with metadata indicating where to find each column in the file. It’s very efficient to load a subset of columns. Parquet files can also be partitioned, meaning the data is split into multiple files according to the value of a certain column (e.g., the year of the entry). This is nice for huge datasets, and on a distributed file system lets different chunks exist on different servers.\nReading a Parquet file is similar to reading a CSV:\nspark.read.parquet(\"some-file.parquet\")\nThis produces a DataFrame. Parquet files support all the data types of Spark, so things like timestamps, booleans, and arrays are automatically read and converted, unlike a CSV where you might have to manually parse a date or specify a schema.\nArrow is the name of the in-memory format for Parquet, i.e. how the data is laid out in memory once it is loaded. This is standardized so you can load Parquet data into Arrow and pass it between libraries in programming languages without any conversion.\nThere is a Python library for Parquet and Arrow, and an R package that can read and write Parquet."
  },
  {
    "objectID": "large-scale-data/spark-data.html#sec-partitioning-bucketing",
    "href": "large-scale-data/spark-data.html#sec-partitioning-bucketing",
    "title": "Spark Data Management",
    "section": "",
    "text": "The whole point of a distributed file system, and of distributed data analysis, is that data might be too big to fit on a machine. If we have an enormous dataset, it can be broken into pieces for storage. That also provides an advantage in analysis with Spark: each Spark executor can read in pieces of the file instead of the whole thing.\nThese pieces are called partitions. When reading a large data file from a block file system like HDFS, Spark will create one partition per block, meaning each executor will hold only part of the DataFrame.\nDepending on the operations you do to the data, Spark might have to move the partitioned data between executors: if you sort the DataFrame, for instance, it will need to collect it all into one place in sorted order. This will cost time as the data is sent between machines. But it does suggest we could choose how to partition the data for maximum efficiency. Spark allows for data files to themselves be partitioned or bucketed, meaning the data is split into multiple files when stored.\n\n\nSuppose you are working with account data from a bank. Each transaction in a bank account has a date, and most of your analyses use data within a certain date range: you produce monthly reports, quarterly reports, and annual reports, each doing complicated analysis in that date range, but rarely need to work with the entire dataset from all dates.\nIf your data has a column for month or year, you could choose to partition the data manually by that column. That means storing the data in a special folder where every month or year has its own data file whose name includes the date.\nIf we are writing the data from Spark, we can use the partitionBy() method on DataFrameWriter to do this:\ndf.write.partitionBy(\"year\").parquet(\"foo.parquet\")\nInstead of a single file foo.parquet, we’ll instead get a folder named foo.parquet/, containing folders with names like year=2022/, themselves containing Parquet data files.\nWe can read the file normally:\ndf_new = spark.read.parquet(\"foo.parquet\")\nSpark automatically recognizes this is a partitioned file that has been split by year. If we write a Spark operation like\ndf_new.filter(col(\"year\") &gt;= 2020) \\\n    .groupBy(\"accountno\") \\\n    .agg(...) \\\n    ... # more Spark stuff here\nthen when we execute an action like .collect() or .show(), Spark will know that its executors need only read the files for 2020 and later. The rest of the data will never be read into memory. And if we write an operation like\ndf_new.filter(col(\"year\") &gt;= 2020) \\\n    .groupBy(\"year\") \\\n    .agg(...) \\\n    ... # more Spark stuff here\nthen each file can be read on one executor, and the grouping and aggregating can be performed on the same executor—requiring no shuffling of data between machines.\nExplicitly partitioning by a specific column works best for columns that have dozens of unique values, not millions; you want partitions that are large enough that each can be comfortably managed by an executor, but are not so small that Spark must read millions of tiny files. (Many distributed file systems are not good at managing many small files, and are better at working with fewer larger files.)\n\n\n\nIn bucketing, we can organize the data in a more flexible way than partitioning. We can specify multiple columns to use in bucketing, and the result is something like a hash table: the columns are combined together and hashed into a single value, and that value determines which bucket the data is stored in. Multiple unique values can be stored in the same bucket, unlike in partitioning, where every partition is a separate file.\nFor example,\ndf.write.bucketBy(10, \"year\", \"month\", \"account\").parquet(\"foo.parquet\")\ntells Spark to use the year, month, and account columns to split the data into 10 buckets. This ensures that all records for a particular account in a particular month will be placed into the same bucket, and hence the same file. Again, foo.parquet will be a directory of multiple files, and Spark will automatically understand the bucketing when we try to read the data.\nIf we load this data into a Spark DataFrame, it will only read the buckets necessary to fulfill a query. This is useful for filtering—if we want a particular account’s records in a particular month—but also is useful for joins. Suppose we want to join the entire table with another data frame, and the join is on the columns used in bucketing. Each executor can read in one bucket and join it with the matching rows in the other table, and the results can be concatenated together at the end. If the other table is also bucketed, each executor may only need to read certain buckets from each table, and do the joining within those buckets, rather than needing to read the entire tables into memory."
  },
  {
    "objectID": "large-scale-data/spark-data.html#sec-spark-catalog",
    "href": "large-scale-data/spark-data.html#sec-spark-catalog",
    "title": "Spark Data Management",
    "section": "",
    "text": "So far we have seen two ways of accessing data from Spark. The boring way is to read a data file directly from the file system. Spark can read from the local filesystem (if the data is available on all executors), but it can also read from distributed filesystems like HDFS, or from cloud data stores like Amazon S3 and Azure Blob Storage.\nThe more interesting way is through a metastore. A metastore is essentially a registry of available data files, their types, their locations, and additional metadata—such as who is allowed to access and modify them. This becomes useful in a large organization, where you need data governance: control over who is officially in charge of certain data sets and who is allowed to access them, records showing when they were used, a search system to find datasets from across the organization, and so on.\nFor this reason, metastores seem pointless when you are just noodling around with data on your own. And they are useless in that context. But think of them as a technical solution to the organizational problems that arise when your company is big and complex enough to require a data processing system as big and complex as Spark.\nTraditionally, Spark uses the Apache Hive metastore. This is still very common.\nDatabricks provides its own proprietary system, Unity Catalog, that includes more advanced features. (It is unfortunately not available on our class Azure Databricks setup.) But the basic principles are the same.\n\n\nWe have already used the Hive metastore when we write Spark commands like this:\nevents = spark.read.table(\"hive_metastore.default.events\")\nThe string hive_metastore.default.events is the name of a managed table. Here “managed” means that Hive is responsible for its storage, and all access goes through Hive (via Spark, in our case). The name has three parts:\n\nThe catalog: hive_metastore. A catalog contains schemas. Different users can have access permissions to different catalogs.\nThe schema: default. A schema contains tables. Different users can have access permissions to different schemas.\nThe table: events. A table refers to an actual data table with rows and columns. Different users can have access permissions to different tables.\n\nThis creates a hierarchy. If I want to access hive_metastore.default.events, I need access to the hive_metastore catalog, its default schema, and the events table contained therein.\nYou can’t put tables directly inside catalogs, or put schemas inside tables, or anything like that: you are restricted to a three-level hierarchy.1\nWith managed tables, the data is stored wherever Hive is configured to store it. In systems like Azure Databricks or Amazon’s EMR, it’s usually configured to store its data in the cloud’s object storage system.\n\n\n\nData frames can be written to managed tables using the write attribute. This returns a DataFrameWriter, which has further options to control partitioning and bucketing. For example:\ndf.write.partitionBy(\"some_column\") \\\n    .saveAsTable(\"hive_metastore.default.example_table\")\nSimilarly, bucketing can be done with the bucketBy() method."
  },
  {
    "objectID": "large-scale-data/spark-data.html#using-data-from-sql-databases",
    "href": "large-scale-data/spark-data.html#using-data-from-sql-databases",
    "title": "Spark Data Management",
    "section": "",
    "text": "One strength of Spark is the ability to load data from many different sources and work with them in identical ways within Spark. One alternative data source is a relational database that speaks SQL, like PostgreSQL.\nOf course, if all your data fits in a SQL database on one large server, it probably makes sense to do your analysis on that server in SQL. But sometimes you must use data from a SQL database as part of a larger analysis. For example, your company might have log data stored on HDFS in Parquet files, but your analysis needs to match up those logs to user accounts stored in PostgreSQL.\nSpark supports accessing any SQL database that supports JDBC (Java Database Connectivity), a standard Java interface for connecting to databases. Because both SQL and Java are extremely popular, almost every relational database has a JDBC package available. Using it is pretty easy:\ndf = spark.read.format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:postgresql://sculptor.stat.cmu.edu/databasename/\") \\\n    .option(\"user\", YOURUSERNAME) \\\n    .option(\"password\", YOURPASSWORD) \\\n    .option(\"dbtable\", \"name_of_table\") \\\n    .load()\nAs usual, Spark will try to be smart about how it reads the database table, and it will not simply do SELECT * FROM name_of_table to read all the data from memory. For example, if you do a filter() in Spark, it will try to turn this into a WHERE clause for the database. It will also make connections from each executor that try to read different parts of the data in parallel. You can thus benefit from the SQL server’s ability to use indexes to make queries faster, while still benefiting from Spark’s distributed data analysis. You might join the table with other large datasets you’ve loaded into Spark."
  },
  {
    "objectID": "large-scale-data/spark-data.html#exercises",
    "href": "large-scale-data/spark-data.html#exercises",
    "title": "Spark Data Management",
    "section": "",
    "text": "Exercise 1 (Partioned LendingClub data) Return to the LendingClub data you loaded in ?@exr-spark-lendingclub.\nSpark does not provide a direct way to tell what variable a DataFrame was partitioned by, if any. But DataFrame objects do have an inputFiles() method that lists the one or more data files used as their input.\nGet the inputFiles() list for the LendingClub data file. Examine the file names. What variable is the Parquet file partitioned by?\n\n\nExercise 2 (Fiscally standardized cities) A dataset of information on “fiscally standardized” cities is available at abfss://sampledata@lsd2025storage.dfs.core.windows.net/fisc_full_dataset_2020_update.csv. You can read the full dataset description for details on what it contains; the short version is that it gives revenue, expenses, and tax information on over 200 American cities for over 40 years, adjusting for differences in government structure across states.\n\nLoad the data using spark.read.csv(). Check the schema with the printSchema() method. Make any necessary tweaks to have the column names and schema read correctly (see Section 1.2.1).\nAdd a net_income column to the data, calculated as the difference between its total revenue (rev_general) and total spending (spending_total).\nOnce you have the data correctly formatted, save it as a table in the Hive metastore. Name it something like fisc_andrewid, with your Andrew ID, so it does not collide with any other student’s table. Partition it by the variable that seems most useful to partition by in the remaining steps of this exercise.\nComplete the following steps in Spark SQL using the table you created. First, produce one row per city, giving the year it had the worst net income – the most negative net income – and its revenue, spending, and net income for that year. Order by city and state.\nNow produce one row per city, listing its name, average net income from 1977 to 2020, and standard deviation of net income over that time period."
  },
  {
    "objectID": "large-scale-data/spark-data.html#footnotes",
    "href": "large-scale-data/spark-data.html#footnotes",
    "title": "Spark Data Management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI don’t know why Hive (and Unity Catalog) are restricted to three-level hierarchies, instead of allowing a general hierarchy of any depth. It’s like if every file on your computer had to be stored exactly two folders deep.↩︎"
  },
  {
    "objectID": "data-engineering/project.html",
    "href": "data-engineering/project.html",
    "title": "Project: Data Pipeline",
    "section": "",
    "text": "In this project, you will develop a data pipeline for ingesting messy, unstructured data, producing a nice and structured SQL database, and automatically generating reports.\nThere are two datasets to choose between. For each, the source data is somewhat messy, so you will work to organize it into a convenient format to facilitate reporting and querying. The project is meant to imitate a common situation you will face in industry: your product or business will be spewing a firehose of messy data, and it’ll be your job to turn it into a sprinkler of insight.\nThe two datasets:\n\nData from the US Department of Health and Human Services (HHS) about hospitals throughout the US, with weekly updates on how many patients have been admitted with COVID, how many hospital beds are currently available, how many staff are vaccinated, and related factors. You can find the data and descriptions here on HealthData.gov.\nData from the Department of Education’s College Scorecard about the performance of colleges and universities throughout the US. This data is updated annually. The Department of Education provides detailed data dictionaries.\n\n\n\nThis project will be completed in assigned groups of 2 or 3 students. Group assignments will be posted on Canvas. There will be several milestones throughout the semester when you will turn in parts of the work; at the end of the semester, you will turn in the completed project.\nThe groups will be paired: your group is matched with another group. You must use a different dataset from your matched group.\nWhile completing the project, you will also be working on a knowledge transfer activity in 36-611 based on this project. Just as in 36-613, I’ll be grading your technical work, and the knowledge transfer activity will be part of your 36-611 grade. The knowledge transfer will be with your paired group.\n\n\n\nThis part is due Wednesday, November 6 at 5pm.\nIn Part 1 of the project, your team must design a table schema for the data. By “table schema” I mean the CREATE TABLE statements necessary to create database tables that fit the data. You should follow the design principles in ?@sec-schema-design to build a normalized structure for the database that minimizes redundant information. Include primary keys, foreign keys, column types, and any appropriate constraints. It is up to you to decide how many tables you need, their names, and their contents.\nWrite your CREATE TABLE statements in a notebook. Test them out on Azure to ensure they work correctly. You do not need to load any real data into the database yet.\nIn the notebook, write comments explaining the following: What are the basic entities in your schema? (In ?@exm-songs-schema, entities were things like songs, record labels, and albums, that each had their own database table.) How did you choose them and what did you do to ensure there is not redundant information in your database?\nYou may find it useful to make a Git repository to share with your project team, though this is not required. Instead you will turn in your schema and explanation on Gradescope.\nThe rest of the instructions are split up by the dataset your group is using:\n\n\nThe HHS data files have many columns; we won’t be interested in all of them here. The data contains the following information:\n\nA unique ID for each hospital (hospital_pk, a string)\nThe state the hospital is in (state, as a two-letter abbreviation, like PA)\nThe hospital’s name (hospital_name), street address (address), city (city), ZIP code (zip), and FIPS code (fips_code, a unique identifier for counties)\nThe latitude and longitude of the hospital (geocoded_hospital_address), formatted as a string like POINT(-91 30), where the first number is the longitude and the second is the latitude. When you load data, you will need to convert this to a format you can use1\nThe week this observation is for (collection_week)\nThe total number of hospital beds available each week, broken down into adult and pediatric (children) beds (all_adult_hospital_beds_7_day_avg, all_pediatric_inpatient_beds_7_day_avg). This can change weekly depending on staffing and facilities.\nThe number of hospital beds that are in use each week (all_adult_hospital_inpatient_bed_occupied_7_day_avg, all_pediatric_inpatient_bed_occupied_7_day_avg)\nThe number of ICU (intensive care unit) beds available and the number in use (total_icu_beds_7_day_avg and icu_beds_used_7_day_avg)\nThe number of patients hospitalized who have confirmed COVID (inpatient_beds_used_covid_7_day_avg)\nThe number of adult ICU patients who have confirmed COVID (staffed_icu_adult_patients_confirmed_covid_7_day_avg)\n\nThe data is updated weekly. In each weekly file I will provide you, each row will be one hospital, and all of the columns above will be present—so each hospital’s address, location, and so on will appear every week.\nThere are several thousand hospitals in the United States, and this data has been updated weekly for much of the pandemic, so the data contains about 580,000 rows. In raw form, with dozens of columns, it is 257 MB.\nYou will also be using a hospital quality dataset from the Centers for Medicare and Medicaid Services (CMS). We are interested in the following information in this data:\n\nA facility ID, which matches the hospital_pk in the HHS data\nThe facility’s name, address, city, state, ZIP code, and county\nThe type of hospital and its type of ownership (government, private, non-profit, etc.)\nWhether the hospital provides emergency services\nThe hospital’s overall quality rating. This quality rating is updated several times a year, and we want to be able to track each version of the quality rating. For instance, we might ask “What was the quality rating of this hospital in 2020?” and compare it to the rating in 2022.\n\n\n\n\nThe College Scorecard data files cover dozens of variables per institution; we won’t be interested in all of them here. The Data Dictionary lists all variables, their column names (VARIABLE NAME), a human readable description, and the meaning of each value. We care about the following columns:\n\nUNITID, the institution ID\nACCREDAGENCY\nPREDDEG\nHIGHDEG\nCONTROL\nREGION\nCCBASIC (TODO: this is missing before 2022; prefer IPEDS version?)\nADM_RATE\nTUITIONFEE_IN, TUITIONFEE_OUT, and TUITIONFEE_PROG\nTUITFTE\nAVGFACSAL\nCDR2 and CDR3\nScroll through the list (there are nearly 3,500 variables!) and pick some additional variables that could be interesting to analyze.\n\nThe data is updated annually. Your database should be able to store each year’s data for each university, so you can quickly look up statistics for a university in any particular year.\nYou will also be using supplementary data from the Integrated Postsecondary Education Data System (IPEDS); specifically, the directory information files. These provide annual directory information and other statistics based on surveys of institutions. Again, since this is updated annually, you must be able to track each version of the data and the dates it applies to.\nFrom the IPEDS data, obtain:\n\nAll information about the institution’s name, location, address, and similar\nAll Carnegie Classification 2021 variables\nThe Census identifiers that apply to it: Core Based Statistical Area (CBSA) and its type, the Combined Statistical Area (CSA), and the county FIPS code\nLatitude and longitude of the institution.\n\nMost of these values will not change from year to year, but they can, since colleges can change names, move, grow, or change.\nThe College Scorecard data also includes crosswalk files giving links between OPEIDs (in the College Scorecard data) and UNITIDs (in the IPEDS) data, so you can match between datasets.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis part is due Friday, November 15th at 5pm. Plan ahead so you can use the Tuesday and Friday office hours.\n\n\nNow we need to write Python code to load data each week. This code will handle any conversions, cleaning, and reformatting of the data, and then run the necessary INSERT commands.\nThe necessary data files will be posted on Canvas. Do not use the original files from the sources; I am providing a few specific subsets to make your task easier.\n\n\n\n\nI will provide the HHS data to you in the form of files containing new data for each week. Each CSV file will cover one week of data. Your first script will load one file into your SQL database.\nThe script should be run from the command line like this:\npython load-hhs.py 2022-01-04-hhs-data.csv\nwhere the argument is the name of the file to load from. The script should then:\n\nLoad the CSV file.\nDo any necessary processing (such as converting -999 to None or NULL, parsing dates into Python date objects, etc.).\nExecute a series of INSERT commands to insert the relevant data into the database.\n\n\n\n\nSimilarly, you will write a script to load the quality data whenever it may be updated. I’ll provide you several example files you will load. The script should run from the command line like this:\npython load-quality.py 2021-07-01 Hospital_General_Information-2021-07.csv\nHere the first argument is the date of this quality data (YYYY-mm-dd) and the second argument is the CSV file name. Again, your Python script will do any necessary work and then INSERT the data.\nIf you chose to have a separate hospitals table, referenced by the tables storing the weekly updates or quality data, it will need to be updated whenever there’s a new hospital. Your scripts should automatically insert new hospitals whenever they appear. If the new quality data changes key hospital metadata (like its location), you should update your hospital data.\n\n\n\n\nI will provide you College Scorecard and IPEDS data files containing data for each year.\nYou should create two Python scripts that can be run from the command line, like so:\npython load-scorecard.py TODO-name-of-file.csv\npython load-ipeds.py TODO-name-of-file.csv\nwhere the arguments are the names of the files to load from. The scripts should then:\n\nLoad the CSV file.\nDo any necessary processing (such as converting -999 to None or NULL, parsing dates into Python date objects, etc.).\nExecute a series of INSERT commands to insert the relevant data into the database.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese requirements apply to both project datasets.\n\n\nYour scripts should have the following common features:\n\nThey should print out a summary of how much data has been read from the CSV and how much has been successfully inserted into the database. This should allow the user to determine whether the amount of data loaded matches what they expect.\nIf the script gets partway through and crashes with an error, either all the data should be inserted or none of it. You should not be able to insert only half the data, then have to manually remove it before you can run the script again to reload the rest of the data.\nIf a row is invalid and rejected by Postgres (because a constraint failed, for instance), your script should print an error message identifying which row failed and giving information about it (such as the hospital or college affected) that helps the user figure out what’s wrong. Then your script should stop so the user can fix the issue in the data before re-running the script.\n\n\n\n\nAdditionally, this work will be part of your knowledge transfer activity in 611. To facilitate transfer, you should ensure that:\n\nyour code is clearly written, separated into small understandable functions, as discussed in 36-650\nyou follow standard PEP 8 style rules for naming and formatting\nyou have docstrings for each function explaining what it does, following PEP 257 conventions\nyour code also includes a README file describing how to use your scripts to load the data\n\nIf you choose, you may create additional Python files containing functions and classes used by your data loading scripts.\n\n\n\nTo submit your code, create a private GitHub repository shared with your classmates. We will ask you to add us (capnrefsmmat and aerosengart) as collaborators, and you will submit a link to the repository as your Canvas submission for this part of the project.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis part is due Friday, December 6th at 5pm. Plan ahead so you can use the Tuesday and Thursday office hours.\n\n\nYour final task for this semester is to develop an automatic reporting system. A user should be able to run the reporting system, specify a date of interest, and obtain a report or dashboard summarizing the data as of the selected date.\n\n\nYour reports should be graphical dashboards summarizing the data and interesting features it contains. You do not need to do any statistical modeling, although you can if you want. Descriptive tables and graphs are sufficient, since the purpose of this project is to give you experience working with real data in SQL.\n\n\nThe HHS report should summarize the COVID situation suggested by the data on hospital beds: how full are hospitals in each state? What states have the fewest open beds? Where are beds filling up the fastest?\nYour report/dashboard should include:\n\nA summary of how many hospital records were loaded in the week selected by the user, and how that compares to previous weeks.\nA table summarizing the number of adult and pediatric beds available that week, the number used, and the number used by patients with COVID, compared to the 4 most recent weeks.\nA graph or table summarizing the fraction of beds in use by hospital quality rating, so we can compare high-quality and low-quality hospitals.\nA plot of the total number of hospital beds used per week, over all time up to the selected week, split into all cases and COVID cases.\n\nAdditionally, you should include three additional interesting analyses. At least two of these should involve combining multiple tables from your database. These could be anything that can be expressed as a table or graph and that is potentially interesting. Some examples include:\n\nA map showing the number of COVID cases by state (the first two digits of a hospital FIPS code is its state)\nA table of the states in which the number of cases has increased by the most since last week\nA table of the hospitals (including names and locations) with the largest changes in COVID cases in the last week\nA table of hospitals that did not report any data in the past week, their names, and the date they most recently reported data\nGraphs of hospital utilization (the percent of available beds being used) by state, or by type of hospital (private or public), over time\n\nYou are free to think of other interesting plots, tables, and maps, as long as you think they could be useful to the Department of Health and Human Services for some health-related purpose.\nIf it helps, imagine sending this report to the Assistant to the Deputy Director of the Department of Health and Human Services. The Assistant is a doctor, not a statistician or programmer, so make your report accordingly.\n\n\n\nThe College Scorecard report should summarize the current data, highlighting the best- and worst-performing colleges according to financial results, and also summarize how the data has changed since the previous year. The user should be able to select a year of interest and get the report generated for that year.\nYour report/dashboard should include:\n\nSummaries of how many colleges and universities are included in the data for the selected year, by state and type of institution (private, public, for-profit, and so on).\nSummaries of current college tuition rates, by state and Carnegie Classification of institution.\nA table showing the best- and worst-performing institutions by loan repayment rates.\nGraphs showing how tuition rates and loan repayment rates have changed over time, either in aggregate (such as averages for all institutions by type) or for selected institutions (such as the most expensive).\n\nAdditionally, you should include three additional interesting analyses. At least two of these should involve combining multiple tables from your database. These could be anything that can be expressed as a table or graph and that is potentially interesting. Some examples include:\n\nMaps of tuition or loan repayment rates across the country\nGraphs comparing tuition, loan repayment rates, and faculty salaries across institution, and demonstrating their correlation\nA table of institutions that did not report data in the selected year but did previously, or that are new this year\nGraphs of total enrollment or total tuition costs over time\n\nIf it helps, imagine sending this report to the Assistant to the Deputy Director of the Department of Education. The Assistant has a degree in educational administration, not in statistics, so make your report accordingly.\n\n\n\n\nThe report should be output in the form of a document with graphs, tables, and text. This could be an HTML file, a PDF, a Jupyter notebook (built automatically with the latest data), or even an interactive dashboard made with something like Streamlit.\n\n\n\nYour reporting system must be automated. This means it cannot require the user to manually edit a file or notebook every week. Ideally, it could be run with a single command:\npython weekly-report.py 2022-09-30 # for HHS data\npython education-report.py 2021 # for College Scorecard\nThat might produce an output file called report-2022-09-30.pdf or report-2021.html; for an interactive dashboard, it might open up the dashboard for that date.\nIf your report is in the form of a notebook, you can produce it from the command line using papermill, which lets you pass command-line arguments to Jupyter notebooks. For example, if you had a week parameter in your code, you could run\npapermill weekly-report.ipynb 2022-09-30-report.ipynb -p week 2022-09-30\njupyter nbconvert --no-input --to html 2022-09-30-report.ipynb \nThe first command substitutes week = \"2022-09-30\" into the parameters of the notebook, and the second executes the notebook and produces the HTML output. The --no-input argument exports only the output, not the code in the notebook. Read the papermill usage guide for details on how to accept parameters in a notebook.\nYour reporting system must be built on SQL, using a SQL connection to generate the results. The calculations must be primarily done in SQL, with a minimum of post-processing in Python to format the results for display. You should not, at any point, load the entire database table into Python so you can do your analysis in Python alone.\nYour reporting system should assume the user has already run your data loading scripts from Part 2; it does not need to load all the data.\n\n\n\nThe report must be designed for an end-user: to the extent you can, it should hide the code and technical details, and present useful tables and graphs of results. Format the output nicely! Do not simply print out lists of Python output. Make graphs and tables. If you’re printing a data frame, consider the Pandas data frame formatting options that let you print it as a nice table.\n\n\n\nAdd your code to the GitHub repository you created in previous part. Make sure this code is committed and pushed by the deadline.\nLoad all the data files provided on Canvas and then generate a report for the final week of data. Convert this report to a PDF and submit it on Gradescope. This serves as an example of what your code produces."
  },
  {
    "objectID": "data-engineering/project.html#logistics",
    "href": "data-engineering/project.html#logistics",
    "title": "Project: Data Pipeline",
    "section": "",
    "text": "This project will be completed in assigned groups of 2 or 3 students. Group assignments will be posted on Canvas. There will be several milestones throughout the semester when you will turn in parts of the work; at the end of the semester, you will turn in the completed project.\nThe groups will be paired: your group is matched with another group. You must use a different dataset from your matched group.\nWhile completing the project, you will also be working on a knowledge transfer activity in 36-611 based on this project. Just as in 36-613, I’ll be grading your technical work, and the knowledge transfer activity will be part of your 36-611 grade. The knowledge transfer will be with your paired group."
  },
  {
    "objectID": "data-engineering/project.html#part-1-designing-the-database",
    "href": "data-engineering/project.html#part-1-designing-the-database",
    "title": "Project: Data Pipeline",
    "section": "",
    "text": "This part is due Wednesday, November 6 at 5pm.\nIn Part 1 of the project, your team must design a table schema for the data. By “table schema” I mean the CREATE TABLE statements necessary to create database tables that fit the data. You should follow the design principles in ?@sec-schema-design to build a normalized structure for the database that minimizes redundant information. Include primary keys, foreign keys, column types, and any appropriate constraints. It is up to you to decide how many tables you need, their names, and their contents.\nWrite your CREATE TABLE statements in a notebook. Test them out on Azure to ensure they work correctly. You do not need to load any real data into the database yet.\nIn the notebook, write comments explaining the following: What are the basic entities in your schema? (In ?@exm-songs-schema, entities were things like songs, record labels, and albums, that each had their own database table.) How did you choose them and what did you do to ensure there is not redundant information in your database?\nYou may find it useful to make a Git repository to share with your project team, though this is not required. Instead you will turn in your schema and explanation on Gradescope.\nThe rest of the instructions are split up by the dataset your group is using:\n\n\nThe HHS data files have many columns; we won’t be interested in all of them here. The data contains the following information:\n\nA unique ID for each hospital (hospital_pk, a string)\nThe state the hospital is in (state, as a two-letter abbreviation, like PA)\nThe hospital’s name (hospital_name), street address (address), city (city), ZIP code (zip), and FIPS code (fips_code, a unique identifier for counties)\nThe latitude and longitude of the hospital (geocoded_hospital_address), formatted as a string like POINT(-91 30), where the first number is the longitude and the second is the latitude. When you load data, you will need to convert this to a format you can use1\nThe week this observation is for (collection_week)\nThe total number of hospital beds available each week, broken down into adult and pediatric (children) beds (all_adult_hospital_beds_7_day_avg, all_pediatric_inpatient_beds_7_day_avg). This can change weekly depending on staffing and facilities.\nThe number of hospital beds that are in use each week (all_adult_hospital_inpatient_bed_occupied_7_day_avg, all_pediatric_inpatient_bed_occupied_7_day_avg)\nThe number of ICU (intensive care unit) beds available and the number in use (total_icu_beds_7_day_avg and icu_beds_used_7_day_avg)\nThe number of patients hospitalized who have confirmed COVID (inpatient_beds_used_covid_7_day_avg)\nThe number of adult ICU patients who have confirmed COVID (staffed_icu_adult_patients_confirmed_covid_7_day_avg)\n\nThe data is updated weekly. In each weekly file I will provide you, each row will be one hospital, and all of the columns above will be present—so each hospital’s address, location, and so on will appear every week.\nThere are several thousand hospitals in the United States, and this data has been updated weekly for much of the pandemic, so the data contains about 580,000 rows. In raw form, with dozens of columns, it is 257 MB.\nYou will also be using a hospital quality dataset from the Centers for Medicare and Medicaid Services (CMS). We are interested in the following information in this data:\n\nA facility ID, which matches the hospital_pk in the HHS data\nThe facility’s name, address, city, state, ZIP code, and county\nThe type of hospital and its type of ownership (government, private, non-profit, etc.)\nWhether the hospital provides emergency services\nThe hospital’s overall quality rating. This quality rating is updated several times a year, and we want to be able to track each version of the quality rating. For instance, we might ask “What was the quality rating of this hospital in 2020?” and compare it to the rating in 2022.\n\n\n\n\nThe College Scorecard data files cover dozens of variables per institution; we won’t be interested in all of them here. The Data Dictionary lists all variables, their column names (VARIABLE NAME), a human readable description, and the meaning of each value. We care about the following columns:\n\nUNITID, the institution ID\nACCREDAGENCY\nPREDDEG\nHIGHDEG\nCONTROL\nREGION\nCCBASIC (TODO: this is missing before 2022; prefer IPEDS version?)\nADM_RATE\nTUITIONFEE_IN, TUITIONFEE_OUT, and TUITIONFEE_PROG\nTUITFTE\nAVGFACSAL\nCDR2 and CDR3\nScroll through the list (there are nearly 3,500 variables!) and pick some additional variables that could be interesting to analyze.\n\nThe data is updated annually. Your database should be able to store each year’s data for each university, so you can quickly look up statistics for a university in any particular year.\nYou will also be using supplementary data from the Integrated Postsecondary Education Data System (IPEDS); specifically, the directory information files. These provide annual directory information and other statistics based on surveys of institutions. Again, since this is updated annually, you must be able to track each version of the data and the dates it applies to.\nFrom the IPEDS data, obtain:\n\nAll information about the institution’s name, location, address, and similar\nAll Carnegie Classification 2021 variables\nThe Census identifiers that apply to it: Core Based Statistical Area (CBSA) and its type, the Combined Statistical Area (CSA), and the county FIPS code\nLatitude and longitude of the institution.\n\nMost of these values will not change from year to year, but they can, since colleges can change names, move, grow, or change.\nThe College Scorecard data also includes crosswalk files giving links between OPEIDs (in the College Scorecard data) and UNITIDs (in the IPEDS) data, so you can match between datasets."
  },
  {
    "objectID": "data-engineering/project.html#part-2-loading-data",
    "href": "data-engineering/project.html#part-2-loading-data",
    "title": "Project: Data Pipeline",
    "section": "",
    "text": "Note\n\n\n\nThis part is due Friday, November 15th at 5pm. Plan ahead so you can use the Tuesday and Friday office hours.\n\n\nNow we need to write Python code to load data each week. This code will handle any conversions, cleaning, and reformatting of the data, and then run the necessary INSERT commands.\nThe necessary data files will be posted on Canvas. Do not use the original files from the sources; I am providing a few specific subsets to make your task easier.\n\n\n\n\nI will provide the HHS data to you in the form of files containing new data for each week. Each CSV file will cover one week of data. Your first script will load one file into your SQL database.\nThe script should be run from the command line like this:\npython load-hhs.py 2022-01-04-hhs-data.csv\nwhere the argument is the name of the file to load from. The script should then:\n\nLoad the CSV file.\nDo any necessary processing (such as converting -999 to None or NULL, parsing dates into Python date objects, etc.).\nExecute a series of INSERT commands to insert the relevant data into the database.\n\n\n\n\nSimilarly, you will write a script to load the quality data whenever it may be updated. I’ll provide you several example files you will load. The script should run from the command line like this:\npython load-quality.py 2021-07-01 Hospital_General_Information-2021-07.csv\nHere the first argument is the date of this quality data (YYYY-mm-dd) and the second argument is the CSV file name. Again, your Python script will do any necessary work and then INSERT the data.\nIf you chose to have a separate hospitals table, referenced by the tables storing the weekly updates or quality data, it will need to be updated whenever there’s a new hospital. Your scripts should automatically insert new hospitals whenever they appear. If the new quality data changes key hospital metadata (like its location), you should update your hospital data.\n\n\n\n\nI will provide you College Scorecard and IPEDS data files containing data for each year.\nYou should create two Python scripts that can be run from the command line, like so:\npython load-scorecard.py TODO-name-of-file.csv\npython load-ipeds.py TODO-name-of-file.csv\nwhere the arguments are the names of the files to load from. The scripts should then:\n\nLoad the CSV file.\nDo any necessary processing (such as converting -999 to None or NULL, parsing dates into Python date objects, etc.).\nExecute a series of INSERT commands to insert the relevant data into the database.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese requirements apply to both project datasets.\n\n\nYour scripts should have the following common features:\n\nThey should print out a summary of how much data has been read from the CSV and how much has been successfully inserted into the database. This should allow the user to determine whether the amount of data loaded matches what they expect.\nIf the script gets partway through and crashes with an error, either all the data should be inserted or none of it. You should not be able to insert only half the data, then have to manually remove it before you can run the script again to reload the rest of the data.\nIf a row is invalid and rejected by Postgres (because a constraint failed, for instance), your script should print an error message identifying which row failed and giving information about it (such as the hospital or college affected) that helps the user figure out what’s wrong. Then your script should stop so the user can fix the issue in the data before re-running the script.\n\n\n\n\nAdditionally, this work will be part of your knowledge transfer activity in 611. To facilitate transfer, you should ensure that:\n\nyour code is clearly written, separated into small understandable functions, as discussed in 36-650\nyou follow standard PEP 8 style rules for naming and formatting\nyou have docstrings for each function explaining what it does, following PEP 257 conventions\nyour code also includes a README file describing how to use your scripts to load the data\n\nIf you choose, you may create additional Python files containing functions and classes used by your data loading scripts.\n\n\n\nTo submit your code, create a private GitHub repository shared with your classmates. We will ask you to add us (capnrefsmmat and aerosengart) as collaborators, and you will submit a link to the repository as your Canvas submission for this part of the project."
  },
  {
    "objectID": "data-engineering/project.html#part-3-analytics-and-reporting",
    "href": "data-engineering/project.html#part-3-analytics-and-reporting",
    "title": "Project: Data Pipeline",
    "section": "",
    "text": "Note\n\n\n\nThis part is due Friday, December 6th at 5pm. Plan ahead so you can use the Tuesday and Thursday office hours.\n\n\nYour final task for this semester is to develop an automatic reporting system. A user should be able to run the reporting system, specify a date of interest, and obtain a report or dashboard summarizing the data as of the selected date.\n\n\nYour reports should be graphical dashboards summarizing the data and interesting features it contains. You do not need to do any statistical modeling, although you can if you want. Descriptive tables and graphs are sufficient, since the purpose of this project is to give you experience working with real data in SQL.\n\n\nThe HHS report should summarize the COVID situation suggested by the data on hospital beds: how full are hospitals in each state? What states have the fewest open beds? Where are beds filling up the fastest?\nYour report/dashboard should include:\n\nA summary of how many hospital records were loaded in the week selected by the user, and how that compares to previous weeks.\nA table summarizing the number of adult and pediatric beds available that week, the number used, and the number used by patients with COVID, compared to the 4 most recent weeks.\nA graph or table summarizing the fraction of beds in use by hospital quality rating, so we can compare high-quality and low-quality hospitals.\nA plot of the total number of hospital beds used per week, over all time up to the selected week, split into all cases and COVID cases.\n\nAdditionally, you should include three additional interesting analyses. At least two of these should involve combining multiple tables from your database. These could be anything that can be expressed as a table or graph and that is potentially interesting. Some examples include:\n\nA map showing the number of COVID cases by state (the first two digits of a hospital FIPS code is its state)\nA table of the states in which the number of cases has increased by the most since last week\nA table of the hospitals (including names and locations) with the largest changes in COVID cases in the last week\nA table of hospitals that did not report any data in the past week, their names, and the date they most recently reported data\nGraphs of hospital utilization (the percent of available beds being used) by state, or by type of hospital (private or public), over time\n\nYou are free to think of other interesting plots, tables, and maps, as long as you think they could be useful to the Department of Health and Human Services for some health-related purpose.\nIf it helps, imagine sending this report to the Assistant to the Deputy Director of the Department of Health and Human Services. The Assistant is a doctor, not a statistician or programmer, so make your report accordingly.\n\n\n\nThe College Scorecard report should summarize the current data, highlighting the best- and worst-performing colleges according to financial results, and also summarize how the data has changed since the previous year. The user should be able to select a year of interest and get the report generated for that year.\nYour report/dashboard should include:\n\nSummaries of how many colleges and universities are included in the data for the selected year, by state and type of institution (private, public, for-profit, and so on).\nSummaries of current college tuition rates, by state and Carnegie Classification of institution.\nA table showing the best- and worst-performing institutions by loan repayment rates.\nGraphs showing how tuition rates and loan repayment rates have changed over time, either in aggregate (such as averages for all institutions by type) or for selected institutions (such as the most expensive).\n\nAdditionally, you should include three additional interesting analyses. At least two of these should involve combining multiple tables from your database. These could be anything that can be expressed as a table or graph and that is potentially interesting. Some examples include:\n\nMaps of tuition or loan repayment rates across the country\nGraphs comparing tuition, loan repayment rates, and faculty salaries across institution, and demonstrating their correlation\nA table of institutions that did not report data in the selected year but did previously, or that are new this year\nGraphs of total enrollment or total tuition costs over time\n\nIf it helps, imagine sending this report to the Assistant to the Deputy Director of the Department of Education. The Assistant has a degree in educational administration, not in statistics, so make your report accordingly.\n\n\n\n\nThe report should be output in the form of a document with graphs, tables, and text. This could be an HTML file, a PDF, a Jupyter notebook (built automatically with the latest data), or even an interactive dashboard made with something like Streamlit.\n\n\n\nYour reporting system must be automated. This means it cannot require the user to manually edit a file or notebook every week. Ideally, it could be run with a single command:\npython weekly-report.py 2022-09-30 # for HHS data\npython education-report.py 2021 # for College Scorecard\nThat might produce an output file called report-2022-09-30.pdf or report-2021.html; for an interactive dashboard, it might open up the dashboard for that date.\nIf your report is in the form of a notebook, you can produce it from the command line using papermill, which lets you pass command-line arguments to Jupyter notebooks. For example, if you had a week parameter in your code, you could run\npapermill weekly-report.ipynb 2022-09-30-report.ipynb -p week 2022-09-30\njupyter nbconvert --no-input --to html 2022-09-30-report.ipynb \nThe first command substitutes week = \"2022-09-30\" into the parameters of the notebook, and the second executes the notebook and produces the HTML output. The --no-input argument exports only the output, not the code in the notebook. Read the papermill usage guide for details on how to accept parameters in a notebook.\nYour reporting system must be built on SQL, using a SQL connection to generate the results. The calculations must be primarily done in SQL, with a minimum of post-processing in Python to format the results for display. You should not, at any point, load the entire database table into Python so you can do your analysis in Python alone.\nYour reporting system should assume the user has already run your data loading scripts from Part 2; it does not need to load all the data.\n\n\n\nThe report must be designed for an end-user: to the extent you can, it should hide the code and technical details, and present useful tables and graphs of results. Format the output nicely! Do not simply print out lists of Python output. Make graphs and tables. If you’re printing a data frame, consider the Pandas data frame formatting options that let you print it as a nice table.\n\n\n\nAdd your code to the GitHub repository you created in previous part. Make sure this code is committed and pushed by the deadline.\nLoad all the data files provided on Canvas and then generate a report for the final week of data. Convert this report to a PDF and submit it on Gradescope. This serves as an example of what your code produces."
  },
  {
    "objectID": "data-engineering/project.html#footnotes",
    "href": "data-engineering/project.html#footnotes",
    "title": "Project: Data Pipeline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe coolest option would be to use PostGIS, which extends Postgres to understand geographic coordinate systems. But that is overkill for us here, and is not installed on our server, so you can’t use it. A couple simple columns would be fine.↩︎"
  },
  {
    "objectID": "data-engineering/cloud-intro.html",
    "href": "data-engineering/cloud-intro.html",
    "title": "The Cloud",
    "section": "",
    "text": "The Cloud\n“The cloud” is just somebody else’s computer."
  },
  {
    "objectID": "data-engineering/distributed-data.html",
    "href": "data-engineering/distributed-data.html",
    "title": "Distributed Data and Computation",
    "section": "",
    "text": "In large companies or large scientific projects, there’s problem of scale: the amount of data we want to process is very large, so we divide it up to process on multiple machines. This is Big Data.\nThink of it this way: if you have one server with loads of RAM, a big hard drive, and a top-of-the-line CPU with many cores, and your data fits easily on that server, you’re probably best off writing parallel code and using that server. But if the data is going to be too large—or you’re receiving new data all the time—instead of buying an even bigger server, it might make more sense to buy a fleet of many smaller servers.\nComputing on many independent machines is distributed computing. It brings many challenges over just using a single machine:\n\nHow do you distribute the data across multiple machines?\nHow do you ingest new data that might be arriving all the time?\nHow do you split up the computing task so it can be performed on separate machines with a minimum of communication between machines?\nHow do you collect the results from each machine and combine them together to get the final answer?\nWith more machines, the chance of one crashing or having a hardware failure increases–how do you recover from this?\n\nEach of these is a difficult problem. There is an ecosystem of software built to solve these problems, from the Hadoop Distributed File System to Spark to Hive to Mahout. But before we learn what the buzzwords mean, let’s talk about the structure of the problems.\n\n\nStatisticians tend to think of datasets as simple things: maybe a few CSV files, a folder full of text files, or a big pile of images. Usually our biggest storage challenge is that we have to pay for extra Dropbox storage space to share the dataset with our collaborators. If datasets change, we usually just get some new data files or a few corrections.\nBut this is not what happens in industry. Consider a few examples:\n\nA web search engine. Search engines run “crawlers,” programs that browse web pages, extract text and links from the HTML, and process the data to form a search index. There may be thousands of crawlers running on different machines at once (the Web is huge!), each producing vast quantities of data, and the search index must be updated with the new data very quickly to stay relevant.\nA shopping site. A large shopping site might take many customer orders per minute, and have thousands of people browsing products all the time. To measure demand, adjust search results, recommend relevant products, and decide how much of each product to order for the warehouse, all this activity must be processed at least daily.\nAn ad network. Advertisements online are weird. When you visit a webpage containing ads, a tiny auction happens instantaneously: data on the webpage and about you is sent to different ad companies, running programs that submit bids on how much they’re willing to pay to show you an ad. An advertiser collects vast data on consumers, what pages they visit, and what ad campaigns are most profitable, and must crunch this data so it can instantaneously decide how much a particular pair of eyeballs is worth.\nA retailer. A big chain like Walmart has hundreds or thousands of stores selling tens of thousands of different products. New sales happen every second, and data arrives about warehouse supplies, pricing changes from suppliers, online searches, and so on. Pricing needs to be adjusted, products ordered, ad campaigns mailed to relevant customers, and executives keep asking analysts to make charts justifying their latest big ideas.\n\nThere are a few common features here:\n\nNew data arrives regularly or continuously.\nNew data is being produced from many sources simultaneously.\nThe analysis must be updated regularly or it will no longer be useful.\nThe analysis may involve large amounts of computation that can only be done in parallel for it to be finished in time.\nThe incoming data is vast, and so is the archive.\n\nSaving to a big CSV file simply is not going to scale. Large companies may have terabytes or petabytes of data stored, with gigabytes more arriving each day. We’re also fundamentally limited by how quickly we can write data to a single hard drive (and afraid of what would happen if that drive fails).\nThis is where a distributed file system is useful.\nIn a distributed file system, data is\n\nSpread across multiple machines (often dozens or hundreds). Each server has its own large and fast hard drives containing a subset of the full data.\nStored redundantly. Each new data file is stored on several different servers, to prevent it from being lost if a hard drive fails or a power supply lets out its magic smoke.\nStored near where it is useful. Ideally, the machine where the data is processed is near the particular machine where it is stored, to avoid congesting the network with data files being sent back and forth.\nCoherent. It shouldn’t be possible to change a file and have the change appear in one copy but not another redundant copy.\nAvailable in quantity. It’s often assumed that rather than wanting small parts of many files, applications will want to receive huge chunks of single files; distributed systems may not be very fast if you try accessing many small files very quickly.\n\nTo achieve this, distributed file systems usually have several parts:\n\nA bunch of servers with hard drives. Each server stores some chunk of files.\nA coordination server (or servers) that tracks the data servers and knows where every file is stored, so users can fetch data from the right servers.\nSome kind of library or client software for connecting to the distributed file system and accessing its data.\n\nThere are two common strategies in the distributed file systems you might use:\n\nBlock-based file systems: In HDFS, each file is split up into blocks, and the blocks are sent to different machines for storage. A 1 GB file might be split into 10 blocks of about 128 MB each, and those blocks might live on the same machines or different machines, based on HDFS settings and automatic criteria.\nObject-based file systems: Amazon S3 is a cloud service that stores files as objects, meaning each file is one unit that is stored and can be read. Files are not split up into blocks.\n\nFortunately for users, these distributed file systems are designed to look like any other file system you might use, and work in much the same way.\nA common block-based file system is the Hadoop Distributed File System, HDFS, though there are many others. Amazon S3 (Simple Storage Service) is a distributed object-based file system as a service, letting you send arbitrary objects to be stored on Amazon’s servers and retrieved whenever you want.\n\n\n\nReading the above, you might think: Why not use a big relational database? As we discussed, these are designed to keep large amounts of data coherent and consistent, and to make the data available quickly on request.\nThe simplest answer is that sometimes, the data volume is more than any one database server can handle.1 But the better answer is that sometimes the computation is more than any one server can handle.\nAs we have seen, relational databases can do surprisingly complex calculations. But those calculations are limited by the size of the server (try writing a SQL query that aggregates 100GB of data on a server with 16GB of RAM!), and of course are limited by what can be expressed in the SQL language. If you want to write a calculation that works on several terabytes of data, be prepared for your relational database server to grind to a halt for hours while it processes the query.\nBut some computational tasks can be split into pieces. We can do the calculations for each piece separately, then combine the results together when we’re done. And all those calculations can be done at the same time, if we have enough computers with access to the right chunks of data.\nHence distributed computing and distributed data fit together. If we’ve built a system for spreading data across many machines, and we also have analytic tasks that can operate on chunks of data and then combine their results, we can use a distributed data system to do large-scale calculations that couldn’t be done on a single machine.\nSo how do we divide up computing tasks to be run on multiple machines, loading their input data from a distributed file system?\nThere are several conceptual models we could use. The first is MapReduce.\n\n\n\nMapReduce dates to 2004, when Google engineers published a paper advertising the MapReduce concept as “Simplified Data Processing on Large Clusters”. By 2006 the Hadoop project existed to make an open-source implementation of the MapReduce ideas, and Hadoop soon exploded: it was the single largest buzzword of the early 2010s, being the canonical Big Data system.\nLet’s talk about the conceptual ideas before we discuss the implementation details.\nMapReduce uses the Map and Reduce operations you may have learned in functional programming, but with an extra twist: Shuffle. A MapReduce system has many “nodes”—different servers running the software—that are connected to some kind of distributed file system.\n\nMap. Each node loads one chunk of data from the distributed file system, applies some function to that data, and writes the result to an output file. The result can have a “key”, some arbitrary identify.\nShuffle. The output files are moved around so that all data with the same key is on the same node. (Or, keys are assigned to nodes, and they fetch the data with that key from the distributed file system.)\nReduce. Each node applies another function to each chunk of output data, processing all chunks with the same key.\n\nThe output of all the reduction functions is aggregated into one big list.\nThe Map and Reduce steps are parallelized: each node processes the Map function simultaneously, and each node processes the Reduce function on each key simultaneously.\nThe Shuffle step lets us do reductions that aren’t completely associative.\n\nExample 1 (Counting words) The standard MapReduce example is counting words in a huge set of documents. Stealing pseudocode from Wikipedia,\nfunction map(String name, String document):\n    // name: document name\n    // document: document contents\n    for each word w in document:\n        emit (w, 1)\n\nfunction reduce(String word, Iterator partialCounts):\n    // word: a word\n    // partialCounts: a list of aggregated partial counts\n    sum = 0\n    for each pc in partialCounts:\n        sum += pc\n    emit (word, sum)\nHere emit (w, 1) means w is the key for the value 1.\nIf we supply these to a MapReduce system that has many documents on many nodes, each node can apply map() to each document it stores. The emitted results are shuffled so each node contains all the output for specific words. Then each node runs reduce() for each word, and gets a word count.\n\n\nExercise 1 (k-means clustering) Word counting doesn’t really give you a sense of the scale of MapReduce, so let’s consider a more statistical example: parallel K-means clustering.\nFirst, a brief review of K-means. We have \\(n\\) points (in some \\(d\\)-dimensional space) we want to cluster into \\(k\\) clusters. We randomly select \\(k\\) points and use their locations as cluster centers. Then:\n\nAssign each point to a cluster, based on the distance between it and the cluster centers.\nTake all the points in each cluster and set that cluster’s center to be the mean of the points.\nReturn to step 1.\n\nThe distance calculations are the scaling problem here: each iteration requires calculating the distance between all \\(n\\) points and all \\(k\\) cluster centers.\nHow could we do K-means as a sequence of MapReduce operations?\n\n\nSolution. TODO\n\n\n\nApache Hadoop is an implementation of the MapReduce idea, in the same way that PostgreSQL or SQLite are implementations of the SQL idea. It’s built in Java.\nHadoop handles the details: it splits up the data, assigns data chunks to compute nodes, calls the Map function with the right data chunks, collects together output with the same key, provides output to the Reduce function, and handles all the communication between nodes. You need only write the Map and Reduce functions and let Hadoop do the hard work.\nIt’s easiest to write map and reduce functions in Java, but Hadoop also provides ways to specify “run this arbitrary program to do the Map”, so you can have your favorite R or Python code do the analysis.\nHadoop can be used to coordinate clusters of hundreds or thousands of servers running the Hadoop software, to which MapReduce tasks will be distributed.\n\n\n\nHadoop was incredibly popular for quite a while, but it has its limitations.\n\nHadoop relies heavily on the distributed file system, storing all its intermediate results on disk. That can make it slow.\nThe MapReduce paradigm is quite restricted. There are extra features—like a Combiner step that operates on the Map’s output before it’s Reduced—but it can still be tricky to turn your task into a MapReduce operation, and it forces everything to be done in a certain order.\nMapReduce isn’t very good for interactive use, when you just want to query your data and try stuff. It works in batches, reading data files from disk and spitting out new results to disk.\nHadoop was cool for long enough that its buzzword value was wearing off.\n\nIt would be nice to have something more flexible: something where we can write complicated programs and automatically have the work split up between nodes, but without having to structure our problem as a series of Map and Reduce steps. And that’s where Spark comes in."
  },
  {
    "objectID": "data-engineering/distributed-data.html#sec-distributing-data",
    "href": "data-engineering/distributed-data.html#sec-distributing-data",
    "title": "Distributed Data and Computation",
    "section": "",
    "text": "Statisticians tend to think of datasets as simple things: maybe a few CSV files, a folder full of text files, or a big pile of images. Usually our biggest storage challenge is that we have to pay for extra Dropbox storage space to share the dataset with our collaborators. If datasets change, we usually just get some new data files or a few corrections.\nBut this is not what happens in industry. Consider a few examples:\n\nA web search engine. Search engines run “crawlers,” programs that browse web pages, extract text and links from the HTML, and process the data to form a search index. There may be thousands of crawlers running on different machines at once (the Web is huge!), each producing vast quantities of data, and the search index must be updated with the new data very quickly to stay relevant.\nA shopping site. A large shopping site might take many customer orders per minute, and have thousands of people browsing products all the time. To measure demand, adjust search results, recommend relevant products, and decide how much of each product to order for the warehouse, all this activity must be processed at least daily.\nAn ad network. Advertisements online are weird. When you visit a webpage containing ads, a tiny auction happens instantaneously: data on the webpage and about you is sent to different ad companies, running programs that submit bids on how much they’re willing to pay to show you an ad. An advertiser collects vast data on consumers, what pages they visit, and what ad campaigns are most profitable, and must crunch this data so it can instantaneously decide how much a particular pair of eyeballs is worth.\nA retailer. A big chain like Walmart has hundreds or thousands of stores selling tens of thousands of different products. New sales happen every second, and data arrives about warehouse supplies, pricing changes from suppliers, online searches, and so on. Pricing needs to be adjusted, products ordered, ad campaigns mailed to relevant customers, and executives keep asking analysts to make charts justifying their latest big ideas.\n\nThere are a few common features here:\n\nNew data arrives regularly or continuously.\nNew data is being produced from many sources simultaneously.\nThe analysis must be updated regularly or it will no longer be useful.\nThe analysis may involve large amounts of computation that can only be done in parallel for it to be finished in time.\nThe incoming data is vast, and so is the archive.\n\nSaving to a big CSV file simply is not going to scale. Large companies may have terabytes or petabytes of data stored, with gigabytes more arriving each day. We’re also fundamentally limited by how quickly we can write data to a single hard drive (and afraid of what would happen if that drive fails).\nThis is where a distributed file system is useful.\nIn a distributed file system, data is\n\nSpread across multiple machines (often dozens or hundreds). Each server has its own large and fast hard drives containing a subset of the full data.\nStored redundantly. Each new data file is stored on several different servers, to prevent it from being lost if a hard drive fails or a power supply lets out its magic smoke.\nStored near where it is useful. Ideally, the machine where the data is processed is near the particular machine where it is stored, to avoid congesting the network with data files being sent back and forth.\nCoherent. It shouldn’t be possible to change a file and have the change appear in one copy but not another redundant copy.\nAvailable in quantity. It’s often assumed that rather than wanting small parts of many files, applications will want to receive huge chunks of single files; distributed systems may not be very fast if you try accessing many small files very quickly.\n\nTo achieve this, distributed file systems usually have several parts:\n\nA bunch of servers with hard drives. Each server stores some chunk of files.\nA coordination server (or servers) that tracks the data servers and knows where every file is stored, so users can fetch data from the right servers.\nSome kind of library or client software for connecting to the distributed file system and accessing its data.\n\nThere are two common strategies in the distributed file systems you might use:\n\nBlock-based file systems: In HDFS, each file is split up into blocks, and the blocks are sent to different machines for storage. A 1 GB file might be split into 10 blocks of about 128 MB each, and those blocks might live on the same machines or different machines, based on HDFS settings and automatic criteria.\nObject-based file systems: Amazon S3 is a cloud service that stores files as objects, meaning each file is one unit that is stored and can be read. Files are not split up into blocks.\n\nFortunately for users, these distributed file systems are designed to look like any other file system you might use, and work in much the same way.\nA common block-based file system is the Hadoop Distributed File System, HDFS, though there are many others. Amazon S3 (Simple Storage Service) is a distributed object-based file system as a service, letting you send arbitrary objects to be stored on Amazon’s servers and retrieved whenever you want."
  },
  {
    "objectID": "data-engineering/distributed-data.html#distributing-computation",
    "href": "data-engineering/distributed-data.html#distributing-computation",
    "title": "Distributed Data and Computation",
    "section": "",
    "text": "Reading the above, you might think: Why not use a big relational database? As we discussed, these are designed to keep large amounts of data coherent and consistent, and to make the data available quickly on request.\nThe simplest answer is that sometimes, the data volume is more than any one database server can handle.1 But the better answer is that sometimes the computation is more than any one server can handle.\nAs we have seen, relational databases can do surprisingly complex calculations. But those calculations are limited by the size of the server (try writing a SQL query that aggregates 100GB of data on a server with 16GB of RAM!), and of course are limited by what can be expressed in the SQL language. If you want to write a calculation that works on several terabytes of data, be prepared for your relational database server to grind to a halt for hours while it processes the query.\nBut some computational tasks can be split into pieces. We can do the calculations for each piece separately, then combine the results together when we’re done. And all those calculations can be done at the same time, if we have enough computers with access to the right chunks of data.\nHence distributed computing and distributed data fit together. If we’ve built a system for spreading data across many machines, and we also have analytic tasks that can operate on chunks of data and then combine their results, we can use a distributed data system to do large-scale calculations that couldn’t be done on a single machine.\nSo how do we divide up computing tasks to be run on multiple machines, loading their input data from a distributed file system?\nThere are several conceptual models we could use. The first is MapReduce."
  },
  {
    "objectID": "data-engineering/distributed-data.html#mapreduce",
    "href": "data-engineering/distributed-data.html#mapreduce",
    "title": "Distributed Data and Computation",
    "section": "",
    "text": "MapReduce dates to 2004, when Google engineers published a paper advertising the MapReduce concept as “Simplified Data Processing on Large Clusters”. By 2006 the Hadoop project existed to make an open-source implementation of the MapReduce ideas, and Hadoop soon exploded: it was the single largest buzzword of the early 2010s, being the canonical Big Data system.\nLet’s talk about the conceptual ideas before we discuss the implementation details.\nMapReduce uses the Map and Reduce operations you may have learned in functional programming, but with an extra twist: Shuffle. A MapReduce system has many “nodes”—different servers running the software—that are connected to some kind of distributed file system.\n\nMap. Each node loads one chunk of data from the distributed file system, applies some function to that data, and writes the result to an output file. The result can have a “key”, some arbitrary identify.\nShuffle. The output files are moved around so that all data with the same key is on the same node. (Or, keys are assigned to nodes, and they fetch the data with that key from the distributed file system.)\nReduce. Each node applies another function to each chunk of output data, processing all chunks with the same key.\n\nThe output of all the reduction functions is aggregated into one big list.\nThe Map and Reduce steps are parallelized: each node processes the Map function simultaneously, and each node processes the Reduce function on each key simultaneously.\nThe Shuffle step lets us do reductions that aren’t completely associative.\n\nExample 1 (Counting words) The standard MapReduce example is counting words in a huge set of documents. Stealing pseudocode from Wikipedia,\nfunction map(String name, String document):\n    // name: document name\n    // document: document contents\n    for each word w in document:\n        emit (w, 1)\n\nfunction reduce(String word, Iterator partialCounts):\n    // word: a word\n    // partialCounts: a list of aggregated partial counts\n    sum = 0\n    for each pc in partialCounts:\n        sum += pc\n    emit (word, sum)\nHere emit (w, 1) means w is the key for the value 1.\nIf we supply these to a MapReduce system that has many documents on many nodes, each node can apply map() to each document it stores. The emitted results are shuffled so each node contains all the output for specific words. Then each node runs reduce() for each word, and gets a word count.\n\n\nExercise 1 (k-means clustering) Word counting doesn’t really give you a sense of the scale of MapReduce, so let’s consider a more statistical example: parallel K-means clustering.\nFirst, a brief review of K-means. We have \\(n\\) points (in some \\(d\\)-dimensional space) we want to cluster into \\(k\\) clusters. We randomly select \\(k\\) points and use their locations as cluster centers. Then:\n\nAssign each point to a cluster, based on the distance between it and the cluster centers.\nTake all the points in each cluster and set that cluster’s center to be the mean of the points.\nReturn to step 1.\n\nThe distance calculations are the scaling problem here: each iteration requires calculating the distance between all \\(n\\) points and all \\(k\\) cluster centers.\nHow could we do K-means as a sequence of MapReduce operations?\n\n\nSolution. TODO\n\n\n\nApache Hadoop is an implementation of the MapReduce idea, in the same way that PostgreSQL or SQLite are implementations of the SQL idea. It’s built in Java.\nHadoop handles the details: it splits up the data, assigns data chunks to compute nodes, calls the Map function with the right data chunks, collects together output with the same key, provides output to the Reduce function, and handles all the communication between nodes. You need only write the Map and Reduce functions and let Hadoop do the hard work.\nIt’s easiest to write map and reduce functions in Java, but Hadoop also provides ways to specify “run this arbitrary program to do the Map”, so you can have your favorite R or Python code do the analysis.\nHadoop can be used to coordinate clusters of hundreds or thousands of servers running the Hadoop software, to which MapReduce tasks will be distributed.\n\n\n\nHadoop was incredibly popular for quite a while, but it has its limitations.\n\nHadoop relies heavily on the distributed file system, storing all its intermediate results on disk. That can make it slow.\nThe MapReduce paradigm is quite restricted. There are extra features—like a Combiner step that operates on the Map’s output before it’s Reduced—but it can still be tricky to turn your task into a MapReduce operation, and it forces everything to be done in a certain order.\nMapReduce isn’t very good for interactive use, when you just want to query your data and try stuff. It works in batches, reading data files from disk and spitting out new results to disk.\nHadoop was cool for long enough that its buzzword value was wearing off.\n\nIt would be nice to have something more flexible: something where we can write complicated programs and automatically have the work split up between nodes, but without having to structure our problem as a series of Map and Reduce steps. And that’s where Spark comes in."
  },
  {
    "objectID": "data-engineering/distributed-data.html#footnotes",
    "href": "data-engineering/distributed-data.html#footnotes",
    "title": "Distributed Data and Computation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe cynical answer is that software engineers need to put more things on their résumés, so they adopt new technologies every month.↩︎"
  },
  {
    "objectID": "data-engineering/data-pipeline.html",
    "href": "data-engineering/data-pipeline.html",
    "title": "The Data Pipeline",
    "section": "",
    "text": "\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\R}{\\mathbb{R}}\n\\]$\n\nIn this mini and the following one, we’ll discuss several pieces of how to work with large datasets in practice:\n\nStoring data, using tools like relational databases\nExtracting data for a specific task, using SQL or Spark\nUsing data to fit models, with neural networks or Spark ML\n\nBut first, we must look at the entire process end-to-end: the data pipeline, following the data from when it arrives at the company to its storage, use, and reuse. As statisticians, we often gloss over most steps of the pipeline, and start our analysis with a nicely curated CSV containing all the relevant information. But in practice, someone must do the curation—and more importantly, someone must design the data pipeline, and think about the life of the data from beginning to end. What data should be collected, where should it be stored, and how can it be put to use?\nAs we discuss data pipeline design, let’s consider a motivating example.\n\nExample 1 (HamsterCard) Credit card companies work with large volumes of data: data about their customers, data about transactions made with their cards, data for marketing, data for fraud detection, and so on. There are many credit cards for specific audiences, like cards with rewards for travel, cards with special discounts for gas or groceries, and even luxury cards that come in titanium and help you flaunt your wealth.\nSo let’s consider the HamsterCard, a credit card for people who like hamsters. Benefits include hamster-themed card designs, 5% cash-back rewards at pet supply stores, and a quarterly magazine for hamster aficionados.\n\n\n\nHamsterCard, the card for hamster lovers. Generated by Bing Image Creator.\n\n\nTo successfully operate the HamsterCard, the company must do many things:\n\nFind potential customers to market HamsterCards to, through postal mail, email, and online advertisements\nDetect and block fraudulent transactions\nOffer special promotions and coupons to current customers based on their interests\nSet and update credit limits for customers based on their creditworthiness and the company’s willingness to take financial risks\nTrack the productivity of customer support staff who answer phone calls and online messages about HamsterCard, and identify high and low performers for management\n\nAll of these will involve data.\n\n\n\nThe first part of the process is to get the data. This is not as easy as it sounds. The data you need comes from many sources in many different formats, and you must work to get what you need.\nThe usual acronym for this process is ETL: extract, transform, and load. You must extract the relevant data from various sources, transform it to be consistent and match whatever schema you use to store it, and load it into your choice of database or storage system. This might happen repeatedly as you obtain updated data, or when you decide you need to change what data is extracted and stored.\n\nExercise 1 (Lead generation for HamsterCard) Credit card companies need to market their cards to prospective customers, since each customer is a potential source of revenue. That means finding new people who don’t have the card but could be interested in getting one. Prospective customers are called leads, and companies try to acquire leads so they can send them advertisements and convince them to sign up.\n\n\n\nOne of HamsterCard’s partner companies generating leads. Liberty Meadows by Frank Cho.\n\n\nHamsterCard acquires leads in several ways:\n\nThey buy customer lists from partner banks, with customer names, addresses, and basic financial information. These are provided as tab-delimited text files.\nThey buy email lists from marketing companies (that themselves bought the email addresses from other online services). The lists include email addresses, variables giving interests and demographic information learned or inferred by the marketing companies, and perhaps information about other products that email is known to be associated with. The lists are given as CSVs.\nThey buy subscriber lists from Hamsters Monthly, Hamster Fancy, and other top newsletters and magazines. These lists give email addresses, subscription dates, and click-through rates measuring how many times the subscriber clicked on links in the email newsletters. The lists are in a shared SQL database that HamsterFacts has read-only access to.\nThey buy subscription lists from Hamster Facts, a text message service that sends hourly hamster facts texts. The subscription list has phone numbers, but no other information about the subscribers. The list is provided as a text file with one phone number per line.\n\nSuppose you’ve been assigned to build a database of leads by combining these sources, so the marketing team can use them to send emails, postcards, and texts.\nFor each step in the extract, transform, and load process, what decisions do you need to make? What problems might occur?\n\nWe can see that ETL can be very complicated in its own right. Data always seems to come in the wrong format, it’s frequently updated, and people want to use it for many different things. ETL involves designing a core storage system for the parts you want to keep, and writing all the necessary code to load data into that storage system. For this class, that storage system will often be a SQL database.\n\n\n\nWhen the ETL process is done, hopefully you have all the data you need in a convenient format. Now you need to solve some business questions using your data, and since you’re a statistician, you’ll probably have to fit some models.\nThe variables we use in our models don’t always match the variables stored in our databases. Some differences are trivial: a categorical variable might be turned into a one-hot encoded vector, for instance. But some differences are more substantial.\nFeature engineering refers to the process of manipulating data to extract new features (predictors) that can be used in fitted models. Let’s consider an example.\n\nExample 2 (Fraud detection for HamsterCard) Credit card fraud is a major problem, and every credit card issuer must check transactions for signs of fraud so fraudulent transactions can be rejected or reversed. HamsterCard thus has an automated system that must make classifications: when each transaction is received, it gets features about that transaction, and it must classify it as “fraudulent”, “questionable”, or “valid”. Fraudulent transactions will be blocked, questionable ones may get checked by sending a text or email to the cardholder, and valid transactions will go through.\nFor each transaction, HamsterCard receives the following information:\n\nThe date and time\nThe vendor (such as the store the card is being used at)\nThe geographic location of the purchase, if in person, or a code indicating if the purchase was made online or over the phone\nThe card number\nThe purchase amount\n\nWith only these features, a classifier would have a very hard time. So before giving the information to the classifier, HamsterCard’s system looks up additional information, such as\n\nthe number of times this customer has made purchases with the same vendor previously\nthe average amount this customer spends per purchase\nthe number of past fraudulent transactions for this customer\n\nGetting this information may require pulling data from multiple databases inside HamsterCard, but it would make the fraud detector much more accurate.\n\nPart of being a data scientist in a large company, then, is feature engineering: identifying data that may be relevant for a statistical problem, and pulling out interesting features from it. These features may not be stored in the original data but instead are calculated from it. The challenge is knowing what to calculate, and this is why a good data scientist must understand their data sources and the problems they are trying to solve.\nIn this course, a lot of feature engineering will involve writing SQL queries to fetch appropriate data from our database.\n\nExercise 2 (Feature engineering for fraud) Suggest three additional features that HamsterCard may want to extract from its data sources to assist in the fraud detection task in Example 2.\n\n\n\n\nYou’ll spend plenty of time on this in your other classes, so we won’t discuss it here.\n\n\n\nOnce you’ve built a model, it needs to be put to use. Sometimes this means you use the model to answer some research questions, write a report, and give a presentation about what you’ve learned, since the project goals were to answer some questions. This is what you’ve been learning to do in other courses, like when you write data analysis reports for 36-617.\nBut in other cases, the model needs to be used in a product. Your model for recommending movies to streaming service subscribers has to be used to make recommendations; your model for fraud detection needs to be used to detect fraud in new data. This usually means applying your model to new data beyond the training dataset, and that means trouble.\n\n\nA model put into production use may be used for many months or years. Over that time, additional training data may be collected. It may be useful to update the model with the new data as it’s collected, so the model is always trained with the largest and most up-to-date possible dataset.\nThat implies you must choose: How often should the model be updated?\n\nExercise 3 (Re-training a HamsterCard fraud detector) Consider HamsterCard’s fraud detector in Example 2. Each day, HamsterCard processes millions of new transactions, and in principle it could feed these transactions into its fraud detection model to retrain it.\nWhat reasons might there be to retrain the model as quickly as possible, say every day?\nWhy might it be a problem to use the previous day’s data to train the fraud detector for the next day?\n\nNow, it’s important not to automate this too much. If you retrain the model automatically every day or week, then immediately start using the new model, you may run into problems. What if the new data causes the model to perform worse? What if a batch of unusual data arrives and biases the model? You’re going to need a process to check the model every time it is updated, and only put it to use if it passes the checks. In fact, we’ll need to monitor the model’s performance even if we’re not updating it, as we’ll see in Section 1.5.\n\n\n\nThere are different ways to put a predictive model into production.\nOne approach is batch prediction. Predictions are made in batches, say every week or every month, and in large quantity. That batch of predictions is then used to do various things until the next batch is produced. The predictions are produced by some automated system that runs periodically, and perhaps we can afford a slow system: if a batch takes a full day to produce, that’s fine if we’re only doing it once a month.\nThe opposite is online prediction: predictions are made in real time as new observations come in. The predictions are immediately used in the product, and so it is often important for the predictions to be made very quickly. We can’t afford a slow, complicated model that takes many minutes to look up necessary data and make its prediction; we might deliberately use a simpler but less accurate model because we do not have time for a more accurate but slower model.\nThe word “latency” is often used to describe the time between something being requested and it being delivered, and so we could define a model’s latency as the time between providing an observation and receiving a prediction for that observation. In batch predictions, we can tolerate high latency; in online predictions, we may need the latency to be as small as possible.\n\nExercise 4 In Example 1, we listed four tasks HamsterCard must do with data. Each of these would involve a model (or several).\nFor each task, identify if predictions would be made in batches or online, and how much latency can be tolerated.\n\n\n\n\n\nWe statisticians tend to think of models as static objects. We spend time carefully constructing a model, then we fit the final version, and then we use it. But models used for prediction on new data must face the real world, and the real world sucks.\n\n\nEver since you first learned to fit regression models, you were taught not to extrapolate beyond the range of your data. But whenever you use a model trained on the past to predict observations received in the present, you are implicitly extrapolating: you are assuming that the same relationship between \\(X\\) and \\(Y\\) is present now as was present when you fit the model [@Hume:1748]. You are also assuming the population distribution of \\(X\\) is roughly the same as it was when you trained the model.\nLet’s examine each of these in turn.\n\nDefinition 1 (Concept drift) Regression models typically try to approximate \\(\\E[Y \\mid X = x]\\), where \\(Y \\in\n\\R\\) and \\(x \\in \\R^p\\); classifiers try to approximate \\(\\Pr(Y = k \\mid X = x)\\) for a class \\(k\\), or try to pick the class with the highest probability. When we train models using a sample, we are hoping that the estimate we obtain from that sample is a good approximation in the population.\nBut what if the true relationship \\(\\E[Y \\mid X = x]\\) changes? Our model is now an approximation to the wrong relationship. Depending on the change, our predictions could become much less accurate. The same applies for a change in the true classification probability \\(\\Pr(Y = k \\mid X = x)\\).\nThis problem is called concept drift.\n\nAnother possibility is that the distribution of \\(X\\) may change even while \\(\\E[Y\n\\mid X = x]\\) stays the same.\n\nDefinition 2 (Covariate shift) Suppose we obtained our training sample from some distribution \\(F_\\text{train}\\), but over time, new observations begin to come from some different distribution \\(F_\\text{new}\\).\nThis problem is called covariate shift.\n\nThis is much more akin to the extrapolation you’re familiar with. If \\(F_\\text{new}\\) produces values of \\(X\\) that are very different from those drawn from \\(F_\\text{train}\\), then your model will have to extrapolate beyond the range of \\(X\\) it was trained on. This problem is more severe when \\(X\\) is multidimensional (i.e. you are predicting using many covariates), because a new distribution could produce combinations of features your model was never trained on, even if individually they are within the same range.\nBoth concept drift and covariate shift can cause the performance of a predictive model to decline over time.\n\nExample 3 (Predicting house prices) Consider a model that predicts a house’s value using features like its size, age, number of bedrooms, and so on—the kind of model that sites like Zillow and Redfin use to estimate values of houses that are not for sale.\nIf house prices suddenly go up in an area due to increased demand, this can cause concept drift: you have the same houses with the same \\(X\\) values, but the value \\(Y\\) is suddenly much higher. If you do not update your model, it will continue predicting the old prices.\nSimilarly, if preferences change and people suddenly hate large, spacious homes because of how much time they have to spend sweeping the floors, this would be concept drift.\nOn the other hand, if the model were trained on an area with mostly single-family homes, but then a developer came in and built hundreds of units of townhomes and condos, that could cause covariate shift: the new housing units are very different than those the model was trained on, and the model will have to extrapolate to predict their values.\n\n\nExercise 5 Give examples of how concept drift and covariate shift could affect HamsterCard’s fraud detection system.\n\n\n\n\nWe statisticians are used to think of our models as being separate from the world being modeled. The models live in our computer; the data comes from the world out there. Nothing we do in R or Python affects the real world.\nBut if you’re building a product that uses your models to make decisions that do something in the real world—send advertising, block a user, purchase a stock, or whatever—those decisions may affect the world and affect future data you collect.\nA simple example: Many police departments keep extensive data on reported crimes and arrests, and use this data to predict locations where high crime or arrest rates are expected. They then send additional police patrols to those areas. If those patrols are more likely to conduct traffic stops, search suspicious pedestrians, and issue tickets for loitering, they are creating more reported events in the data—and hence contribute to the area being predicted to have more events in the future. Police may be sent to areas based on their own work in those areas, not because they have an intrinsically higher crime rate than other areas. This is a feedback loop: the model’s output is creating data that will affect its outputs.\n\nExercise 6 Give an example of how HamsterCard’s marketing models—which choose potential customers to target with advertising—could create a feedback loop.\n\nPreventing feedback loops can be difficult. After all, the point of the model is often to make changes in the real world. There is no one strategy to prevent them, but in particular situations we may be able to.\nFor example, recommender systems can easily have feedback. If you recommend movies to viewers based on what you think they’ll like, but estimate whether they’ll like movies based on how many other people watch and enjoy them, you may tend to make popular movies more popular while rarely recommending movies that people haven’t viewed yet. This creates a “rich get richer” situation: nobody watches unknown movies, so you don’t know if anyone will like them, so you don’t recommend them to anyone. This can be avoided by deliberately recommending a mix of movies highly rated by your algorithm and less well-known movies with high uncertainty.\n\n\n\nTogether, these problems suggest an important fact: We cannot simply train a model, deploy it, and never look back. We must periodically check that the model is still performing well and serving its intended purpose.\nWhat you monitor depends on what is available to you. If you eventually observe outcomes for your predictions, you can measure their accuracy or success rate and track it over time. You can also track metrics about the typical inputs to your model, the distribution of covariates, or other features relevant to your prediction.\n\nExercise 7 Give some examples of metrics HamsterCard might track for its marketing models."
  },
  {
    "objectID": "data-engineering/data-pipeline.html#etl",
    "href": "data-engineering/data-pipeline.html#etl",
    "title": "The Data Pipeline",
    "section": "",
    "text": "The first part of the process is to get the data. This is not as easy as it sounds. The data you need comes from many sources in many different formats, and you must work to get what you need.\nThe usual acronym for this process is ETL: extract, transform, and load. You must extract the relevant data from various sources, transform it to be consistent and match whatever schema you use to store it, and load it into your choice of database or storage system. This might happen repeatedly as you obtain updated data, or when you decide you need to change what data is extracted and stored.\n\nExercise 1 (Lead generation for HamsterCard) Credit card companies need to market their cards to prospective customers, since each customer is a potential source of revenue. That means finding new people who don’t have the card but could be interested in getting one. Prospective customers are called leads, and companies try to acquire leads so they can send them advertisements and convince them to sign up.\n\n\n\nOne of HamsterCard’s partner companies generating leads. Liberty Meadows by Frank Cho.\n\n\nHamsterCard acquires leads in several ways:\n\nThey buy customer lists from partner banks, with customer names, addresses, and basic financial information. These are provided as tab-delimited text files.\nThey buy email lists from marketing companies (that themselves bought the email addresses from other online services). The lists include email addresses, variables giving interests and demographic information learned or inferred by the marketing companies, and perhaps information about other products that email is known to be associated with. The lists are given as CSVs.\nThey buy subscriber lists from Hamsters Monthly, Hamster Fancy, and other top newsletters and magazines. These lists give email addresses, subscription dates, and click-through rates measuring how many times the subscriber clicked on links in the email newsletters. The lists are in a shared SQL database that HamsterFacts has read-only access to.\nThey buy subscription lists from Hamster Facts, a text message service that sends hourly hamster facts texts. The subscription list has phone numbers, but no other information about the subscribers. The list is provided as a text file with one phone number per line.\n\nSuppose you’ve been assigned to build a database of leads by combining these sources, so the marketing team can use them to send emails, postcards, and texts.\nFor each step in the extract, transform, and load process, what decisions do you need to make? What problems might occur?\n\nWe can see that ETL can be very complicated in its own right. Data always seems to come in the wrong format, it’s frequently updated, and people want to use it for many different things. ETL involves designing a core storage system for the parts you want to keep, and writing all the necessary code to load data into that storage system. For this class, that storage system will often be a SQL database."
  },
  {
    "objectID": "data-engineering/data-pipeline.html#feature-engineering",
    "href": "data-engineering/data-pipeline.html#feature-engineering",
    "title": "The Data Pipeline",
    "section": "",
    "text": "When the ETL process is done, hopefully you have all the data you need in a convenient format. Now you need to solve some business questions using your data, and since you’re a statistician, you’ll probably have to fit some models.\nThe variables we use in our models don’t always match the variables stored in our databases. Some differences are trivial: a categorical variable might be turned into a one-hot encoded vector, for instance. But some differences are more substantial.\nFeature engineering refers to the process of manipulating data to extract new features (predictors) that can be used in fitted models. Let’s consider an example.\n\nExample 2 (Fraud detection for HamsterCard) Credit card fraud is a major problem, and every credit card issuer must check transactions for signs of fraud so fraudulent transactions can be rejected or reversed. HamsterCard thus has an automated system that must make classifications: when each transaction is received, it gets features about that transaction, and it must classify it as “fraudulent”, “questionable”, or “valid”. Fraudulent transactions will be blocked, questionable ones may get checked by sending a text or email to the cardholder, and valid transactions will go through.\nFor each transaction, HamsterCard receives the following information:\n\nThe date and time\nThe vendor (such as the store the card is being used at)\nThe geographic location of the purchase, if in person, or a code indicating if the purchase was made online or over the phone\nThe card number\nThe purchase amount\n\nWith only these features, a classifier would have a very hard time. So before giving the information to the classifier, HamsterCard’s system looks up additional information, such as\n\nthe number of times this customer has made purchases with the same vendor previously\nthe average amount this customer spends per purchase\nthe number of past fraudulent transactions for this customer\n\nGetting this information may require pulling data from multiple databases inside HamsterCard, but it would make the fraud detector much more accurate.\n\nPart of being a data scientist in a large company, then, is feature engineering: identifying data that may be relevant for a statistical problem, and pulling out interesting features from it. These features may not be stored in the original data but instead are calculated from it. The challenge is knowing what to calculate, and this is why a good data scientist must understand their data sources and the problems they are trying to solve.\nIn this course, a lot of feature engineering will involve writing SQL queries to fetch appropriate data from our database.\n\nExercise 2 (Feature engineering for fraud) Suggest three additional features that HamsterCard may want to extract from its data sources to assist in the fraud detection task in Example 2."
  },
  {
    "objectID": "data-engineering/data-pipeline.html#model-building",
    "href": "data-engineering/data-pipeline.html#model-building",
    "title": "The Data Pipeline",
    "section": "",
    "text": "You’ll spend plenty of time on this in your other classes, so we won’t discuss it here."
  },
  {
    "objectID": "data-engineering/data-pipeline.html#model-deployment",
    "href": "data-engineering/data-pipeline.html#model-deployment",
    "title": "The Data Pipeline",
    "section": "",
    "text": "Once you’ve built a model, it needs to be put to use. Sometimes this means you use the model to answer some research questions, write a report, and give a presentation about what you’ve learned, since the project goals were to answer some questions. This is what you’ve been learning to do in other courses, like when you write data analysis reports for 36-617.\nBut in other cases, the model needs to be used in a product. Your model for recommending movies to streaming service subscribers has to be used to make recommendations; your model for fraud detection needs to be used to detect fraud in new data. This usually means applying your model to new data beyond the training dataset, and that means trouble.\n\n\nA model put into production use may be used for many months or years. Over that time, additional training data may be collected. It may be useful to update the model with the new data as it’s collected, so the model is always trained with the largest and most up-to-date possible dataset.\nThat implies you must choose: How often should the model be updated?\n\nExercise 3 (Re-training a HamsterCard fraud detector) Consider HamsterCard’s fraud detector in Example 2. Each day, HamsterCard processes millions of new transactions, and in principle it could feed these transactions into its fraud detection model to retrain it.\nWhat reasons might there be to retrain the model as quickly as possible, say every day?\nWhy might it be a problem to use the previous day’s data to train the fraud detector for the next day?\n\nNow, it’s important not to automate this too much. If you retrain the model automatically every day or week, then immediately start using the new model, you may run into problems. What if the new data causes the model to perform worse? What if a batch of unusual data arrives and biases the model? You’re going to need a process to check the model every time it is updated, and only put it to use if it passes the checks. In fact, we’ll need to monitor the model’s performance even if we’re not updating it, as we’ll see in Section 1.5.\n\n\n\nThere are different ways to put a predictive model into production.\nOne approach is batch prediction. Predictions are made in batches, say every week or every month, and in large quantity. That batch of predictions is then used to do various things until the next batch is produced. The predictions are produced by some automated system that runs periodically, and perhaps we can afford a slow system: if a batch takes a full day to produce, that’s fine if we’re only doing it once a month.\nThe opposite is online prediction: predictions are made in real time as new observations come in. The predictions are immediately used in the product, and so it is often important for the predictions to be made very quickly. We can’t afford a slow, complicated model that takes many minutes to look up necessary data and make its prediction; we might deliberately use a simpler but less accurate model because we do not have time for a more accurate but slower model.\nThe word “latency” is often used to describe the time between something being requested and it being delivered, and so we could define a model’s latency as the time between providing an observation and receiving a prediction for that observation. In batch predictions, we can tolerate high latency; in online predictions, we may need the latency to be as small as possible.\n\nExercise 4 In Example 1, we listed four tasks HamsterCard must do with data. Each of these would involve a model (or several).\nFor each task, identify if predictions would be made in batches or online, and how much latency can be tolerated."
  },
  {
    "objectID": "data-engineering/data-pipeline.html#sec-pipeline-monitoring",
    "href": "data-engineering/data-pipeline.html#sec-pipeline-monitoring",
    "title": "The Data Pipeline",
    "section": "",
    "text": "We statisticians tend to think of models as static objects. We spend time carefully constructing a model, then we fit the final version, and then we use it. But models used for prediction on new data must face the real world, and the real world sucks.\n\n\nEver since you first learned to fit regression models, you were taught not to extrapolate beyond the range of your data. But whenever you use a model trained on the past to predict observations received in the present, you are implicitly extrapolating: you are assuming that the same relationship between \\(X\\) and \\(Y\\) is present now as was present when you fit the model [@Hume:1748]. You are also assuming the population distribution of \\(X\\) is roughly the same as it was when you trained the model.\nLet’s examine each of these in turn.\n\nDefinition 1 (Concept drift) Regression models typically try to approximate \\(\\E[Y \\mid X = x]\\), where \\(Y \\in\n\\R\\) and \\(x \\in \\R^p\\); classifiers try to approximate \\(\\Pr(Y = k \\mid X = x)\\) for a class \\(k\\), or try to pick the class with the highest probability. When we train models using a sample, we are hoping that the estimate we obtain from that sample is a good approximation in the population.\nBut what if the true relationship \\(\\E[Y \\mid X = x]\\) changes? Our model is now an approximation to the wrong relationship. Depending on the change, our predictions could become much less accurate. The same applies for a change in the true classification probability \\(\\Pr(Y = k \\mid X = x)\\).\nThis problem is called concept drift.\n\nAnother possibility is that the distribution of \\(X\\) may change even while \\(\\E[Y\n\\mid X = x]\\) stays the same.\n\nDefinition 2 (Covariate shift) Suppose we obtained our training sample from some distribution \\(F_\\text{train}\\), but over time, new observations begin to come from some different distribution \\(F_\\text{new}\\).\nThis problem is called covariate shift.\n\nThis is much more akin to the extrapolation you’re familiar with. If \\(F_\\text{new}\\) produces values of \\(X\\) that are very different from those drawn from \\(F_\\text{train}\\), then your model will have to extrapolate beyond the range of \\(X\\) it was trained on. This problem is more severe when \\(X\\) is multidimensional (i.e. you are predicting using many covariates), because a new distribution could produce combinations of features your model was never trained on, even if individually they are within the same range.\nBoth concept drift and covariate shift can cause the performance of a predictive model to decline over time.\n\nExample 3 (Predicting house prices) Consider a model that predicts a house’s value using features like its size, age, number of bedrooms, and so on—the kind of model that sites like Zillow and Redfin use to estimate values of houses that are not for sale.\nIf house prices suddenly go up in an area due to increased demand, this can cause concept drift: you have the same houses with the same \\(X\\) values, but the value \\(Y\\) is suddenly much higher. If you do not update your model, it will continue predicting the old prices.\nSimilarly, if preferences change and people suddenly hate large, spacious homes because of how much time they have to spend sweeping the floors, this would be concept drift.\nOn the other hand, if the model were trained on an area with mostly single-family homes, but then a developer came in and built hundreds of units of townhomes and condos, that could cause covariate shift: the new housing units are very different than those the model was trained on, and the model will have to extrapolate to predict their values.\n\n\nExercise 5 Give examples of how concept drift and covariate shift could affect HamsterCard’s fraud detection system.\n\n\n\n\nWe statisticians are used to think of our models as being separate from the world being modeled. The models live in our computer; the data comes from the world out there. Nothing we do in R or Python affects the real world.\nBut if you’re building a product that uses your models to make decisions that do something in the real world—send advertising, block a user, purchase a stock, or whatever—those decisions may affect the world and affect future data you collect.\nA simple example: Many police departments keep extensive data on reported crimes and arrests, and use this data to predict locations where high crime or arrest rates are expected. They then send additional police patrols to those areas. If those patrols are more likely to conduct traffic stops, search suspicious pedestrians, and issue tickets for loitering, they are creating more reported events in the data—and hence contribute to the area being predicted to have more events in the future. Police may be sent to areas based on their own work in those areas, not because they have an intrinsically higher crime rate than other areas. This is a feedback loop: the model’s output is creating data that will affect its outputs.\n\nExercise 6 Give an example of how HamsterCard’s marketing models—which choose potential customers to target with advertising—could create a feedback loop.\n\nPreventing feedback loops can be difficult. After all, the point of the model is often to make changes in the real world. There is no one strategy to prevent them, but in particular situations we may be able to.\nFor example, recommender systems can easily have feedback. If you recommend movies to viewers based on what you think they’ll like, but estimate whether they’ll like movies based on how many other people watch and enjoy them, you may tend to make popular movies more popular while rarely recommending movies that people haven’t viewed yet. This creates a “rich get richer” situation: nobody watches unknown movies, so you don’t know if anyone will like them, so you don’t recommend them to anyone. This can be avoided by deliberately recommending a mix of movies highly rated by your algorithm and less well-known movies with high uncertainty.\n\n\n\nTogether, these problems suggest an important fact: We cannot simply train a model, deploy it, and never look back. We must periodically check that the model is still performing well and serving its intended purpose.\nWhat you monitor depends on what is available to you. If you eventually observe outcomes for your predictions, you can measure their accuracy or success rate and track it over time. You can also track metrics about the typical inputs to your model, the distribution of covariates, or other features relevant to your prediction.\n\nExercise 7 Give some examples of metrics HamsterCard might track for its marketing models."
  },
  {
    "objectID": "data-engineering/etl.html",
    "href": "data-engineering/etl.html",
    "title": "Extract, Transform, and Load",
    "section": "",
    "text": "Extract, Transform, and Load"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee @knuth84 for additional discussion of literate programming."
  },
  {
    "objectID": "computational-methods.html",
    "href": "computational-methods.html",
    "title": "Computational Methods for Statistics",
    "section": "",
    "text": "Course not available\n\n\n\nThis course is not currently offered.\n\n\n\n\n\nSpring 2023, mini 4\nMonday and Wednesday, 10-10:50am\nWean 5415\nInstructor: Alex Reinhart\nTAs: Anna Rosengart and Jinjin Tian\nOffice hours:\n\nAlex: Tuesdays, 10-11am, BH 232K\nAnna: Mondays 9-10am, Wean 3715\nJinjin: Fridays 1-2pm, Wean 3715\n\n\n\n\n\nIn the prior course, 36-615 Software for Large-Scale Data, we learned to use deep learning to construct flexible models of complex datasets. In this mini, we will return to Apache Spark, using its machine learning library to fit various ML models to distributed datasets. We’ll learn about cloud computing and how it is often used in data science, and we’ll learn additional techniques for using Python in computational statistics and data science.\nThis course is primarily for students in the Master of Statistical Practice program. Students in this course should have prior experience programming in Python (such as through 36-650), using Git, and working from the command-line shell.\n\n\n\nThere is no required book for this course. We recommend the following reference resources:\n\nChambers and Zaharia, Spark: The Definitive Guide, O’Reilly. Available free through the CMU library.\nHuyen, Designing Machine Learning Systems, O’Reilly. Available free through the CMU library.\n\n\n\n\nThis course features regular homework assignments, due Wednesday afternoons. Many of these will involve writing code to complete particular data tasks. Homework assignments will be posted on Canvas. Homework must be submitted as a PDF on Gradescope, and should be produced either with a Jupyter Notebook.\nIt’s your responsibility to ensure the PDF file is legible in Gradescope. This includes ensuring the pages are a reasonable size, and also using headings, formatting, and text to make it easy to find your answers. You should not include extraneous code or output not relevant to the problems.\nYou are expected to follow consistent style in your code. For Python code, the style we will use is PEP 8, the standard Python style guide. Style will be part of the homework grade.\n\n\nFor homework submissions, you will have three “grace days” you can use throughout the semester. Each time you use a grace day for an assignment, you get 24 hours extra to submit the assignment. You do not need any excuse to use grace days. Once you have used all three grace days, late work will not be accepted.\nThis system is meant to allow you flexibility, so that ordinary problems (minor illness, forgot a deadline, had to finish another class’s big assignment, traveled to an event) don’t harm you, and so you do not need my permission to handle unexpected problems. If you experience a serious emergency that prevents you from completing work for a longer time, contact me so we can make arrangements.\n\n\n\n\nClass attendance and participation is essential. If there’s any one message to be learned from pedagogical research, it’s that listening passively to a lecture is not a good way to learn how to think about complicated problems. I will often ask class questions or ask you to complete short activities in small groups. You are expected to attend class and participate in these activities.\nWhen attending class, you are expected to follow all COVID-19 precautions required by the University.\n\n\n\nThe homework will be the basis of course grades.\nFinal grades will be based on this scale: A = [93, 100]; A- = [90, 93); B+ = [87, 90); B = [83, 87); B- = [80, 83); C+ = [77, 80); C = [73, 77); C- = [70, 73); D = [60, 70); R = [0, 60).\n\n\n\nDiscussing homework and projects with your classmates is allowed and encouraged, and helping explain ideas to each other is a core part of the academic experience. But it is important that every student get practice working on their own. This means that all the work you turn in must be your own. You must devise and write your own code, generate your own graphics, and write your own solutions and reports.\nYou may use external sources (books, websites, papers) to\n\nLook up Python documentation, find useful packages, find explanations for error messages, or remind yourself about the functions to do some task,\nFind reference materials on statistical methods,\nClarify material from the course notes or examples.\n\nBut external sources must be used to support your work, not to obtain your work. You may not use them to copy code, text, or graphics without attribution. You may not use any prior course’s or textbook’s homework solutions in any way. This prohibition applies even to students who are re-taking the course. Do not copy old solutions (in whole or in part), and do not “consult” or read them. Doing any of that is cheating, making any feedback you get meaningless and any evaluation based on that assignment unfair.\nIf you do use any material from other sources, you must clearly mark its source. Text taken from other sources must be in quotation marks with citations; figures from other sources need a caption indicating the source; and code from other sources must have a comment indicating the source. We must be able to determine who wrote any material you submit, and you must not falsely imply that you completed work actually done by others.\nPlease talk to me if you have any questions about this policy. Any form of cheating or plagiarism is grounds for sanctions to be determined by the instructor, including grade penalties or course failure. Students taking the course pass/fail may have this status revoked. I am also obliged in these situations to report the incident to your academic program and the appropriate University authorities. Please refer to the University Policy on Academic Integrity.\n\n\n\nIf you have a disability and have an accommodations letter from the Disability Resources office, I encourage you to discuss your accommodations and needs with me as early in the semester as possible. I will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, I encourage you to contact them at access@andrew.cmu.edu.\n\n\n\nWe must treat every individual with respect. We are diverse in many ways, and this diversity is fundamental to building and maintaining an equitable and inclusive campus community. Diversity can refer to multiple ways that we identify ourselves, including but not limited to race, color, national origin, language, sex, disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic information. Each of these diverse identities, along with many others not mentioned here, shape the perspectives our students, faculty, and staff bring to our campus. We, at CMU, will work to promote diversity, equity and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice. We acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a campus community that increasingly embraces these core values.\nEach of us is responsible for creating a safer, more inclusive environment.\nUnfortunately, incidents of bias or discrimination do occur, whether intentional or unintentional. They contribute to creating an unwelcoming environment for individuals and groups at the university. Therefore, the university encourages anyone who experiences or observes unfair or hostile treatment on the basis of identity to speak out for justice and support, within the moment of the incident or after the incident has passed. Anyone can share these experiences using the following resources:\n\nCenter for Student Diversity and Inclusion: csdi@andrew.cmu.edu, (412) 268-2150\nReport-It online anonymous reporting platform. username: tartans password: plaid\n\nAll reports will be documented and deliberated to determine if there should be any following actions. Regardless of incident type, the university will use all shared experiences to transform our campus climate to be more equitable and just.\n\n\n\nAll of us benefit from support during times of struggle. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is almost always helpful.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 or visit their website. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help."
  },
  {
    "objectID": "computational-methods.html#basic-information",
    "href": "computational-methods.html#basic-information",
    "title": "Computational Methods for Statistics",
    "section": "",
    "text": "Spring 2023, mini 4\nMonday and Wednesday, 10-10:50am\nWean 5415\nInstructor: Alex Reinhart\nTAs: Anna Rosengart and Jinjin Tian\nOffice hours:\n\nAlex: Tuesdays, 10-11am, BH 232K\nAnna: Mondays 9-10am, Wean 3715\nJinjin: Fridays 1-2pm, Wean 3715"
  },
  {
    "objectID": "computational-methods.html#course-description",
    "href": "computational-methods.html#course-description",
    "title": "Computational Methods for Statistics",
    "section": "",
    "text": "In the prior course, 36-615 Software for Large-Scale Data, we learned to use deep learning to construct flexible models of complex datasets. In this mini, we will return to Apache Spark, using its machine learning library to fit various ML models to distributed datasets. We’ll learn about cloud computing and how it is often used in data science, and we’ll learn additional techniques for using Python in computational statistics and data science.\nThis course is primarily for students in the Master of Statistical Practice program. Students in this course should have prior experience programming in Python (such as through 36-650), using Git, and working from the command-line shell."
  },
  {
    "objectID": "computational-methods.html#books-and-references",
    "href": "computational-methods.html#books-and-references",
    "title": "Computational Methods for Statistics",
    "section": "",
    "text": "There is no required book for this course. We recommend the following reference resources:\n\nChambers and Zaharia, Spark: The Definitive Guide, O’Reilly. Available free through the CMU library.\nHuyen, Designing Machine Learning Systems, O’Reilly. Available free through the CMU library."
  },
  {
    "objectID": "computational-methods.html#homework",
    "href": "computational-methods.html#homework",
    "title": "Computational Methods for Statistics",
    "section": "",
    "text": "This course features regular homework assignments, due Wednesday afternoons. Many of these will involve writing code to complete particular data tasks. Homework assignments will be posted on Canvas. Homework must be submitted as a PDF on Gradescope, and should be produced either with a Jupyter Notebook.\nIt’s your responsibility to ensure the PDF file is legible in Gradescope. This includes ensuring the pages are a reasonable size, and also using headings, formatting, and text to make it easy to find your answers. You should not include extraneous code or output not relevant to the problems.\nYou are expected to follow consistent style in your code. For Python code, the style we will use is PEP 8, the standard Python style guide. Style will be part of the homework grade.\n\n\nFor homework submissions, you will have three “grace days” you can use throughout the semester. Each time you use a grace day for an assignment, you get 24 hours extra to submit the assignment. You do not need any excuse to use grace days. Once you have used all three grace days, late work will not be accepted.\nThis system is meant to allow you flexibility, so that ordinary problems (minor illness, forgot a deadline, had to finish another class’s big assignment, traveled to an event) don’t harm you, and so you do not need my permission to handle unexpected problems. If you experience a serious emergency that prevents you from completing work for a longer time, contact me so we can make arrangements."
  },
  {
    "objectID": "computational-methods.html#attendance-and-participation",
    "href": "computational-methods.html#attendance-and-participation",
    "title": "Computational Methods for Statistics",
    "section": "",
    "text": "Class attendance and participation is essential. If there’s any one message to be learned from pedagogical research, it’s that listening passively to a lecture is not a good way to learn how to think about complicated problems. I will often ask class questions or ask you to complete short activities in small groups. You are expected to attend class and participate in these activities.\nWhen attending class, you are expected to follow all COVID-19 precautions required by the University."
  },
  {
    "objectID": "computational-methods.html#grading",
    "href": "computational-methods.html#grading",
    "title": "Computational Methods for Statistics",
    "section": "",
    "text": "The homework will be the basis of course grades.\nFinal grades will be based on this scale: A = [93, 100]; A- = [90, 93); B+ = [87, 90); B = [83, 87); B- = [80, 83); C+ = [77, 80); C = [73, 77); C- = [70, 73); D = [60, 70); R = [0, 60)."
  },
  {
    "objectID": "computational-methods.html#academic-integrity",
    "href": "computational-methods.html#academic-integrity",
    "title": "Computational Methods for Statistics",
    "section": "",
    "text": "Discussing homework and projects with your classmates is allowed and encouraged, and helping explain ideas to each other is a core part of the academic experience. But it is important that every student get practice working on their own. This means that all the work you turn in must be your own. You must devise and write your own code, generate your own graphics, and write your own solutions and reports.\nYou may use external sources (books, websites, papers) to\n\nLook up Python documentation, find useful packages, find explanations for error messages, or remind yourself about the functions to do some task,\nFind reference materials on statistical methods,\nClarify material from the course notes or examples.\n\nBut external sources must be used to support your work, not to obtain your work. You may not use them to copy code, text, or graphics without attribution. You may not use any prior course’s or textbook’s homework solutions in any way. This prohibition applies even to students who are re-taking the course. Do not copy old solutions (in whole or in part), and do not “consult” or read them. Doing any of that is cheating, making any feedback you get meaningless and any evaluation based on that assignment unfair.\nIf you do use any material from other sources, you must clearly mark its source. Text taken from other sources must be in quotation marks with citations; figures from other sources need a caption indicating the source; and code from other sources must have a comment indicating the source. We must be able to determine who wrote any material you submit, and you must not falsely imply that you completed work actually done by others.\nPlease talk to me if you have any questions about this policy. Any form of cheating or plagiarism is grounds for sanctions to be determined by the instructor, including grade penalties or course failure. Students taking the course pass/fail may have this status revoked. I am also obliged in these situations to report the incident to your academic program and the appropriate University authorities. Please refer to the University Policy on Academic Integrity."
  },
  {
    "objectID": "computational-methods.html#accommodations-for-students-with-disabilities",
    "href": "computational-methods.html#accommodations-for-students-with-disabilities",
    "title": "Computational Methods for Statistics",
    "section": "",
    "text": "If you have a disability and have an accommodations letter from the Disability Resources office, I encourage you to discuss your accommodations and needs with me as early in the semester as possible. I will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, I encourage you to contact them at access@andrew.cmu.edu."
  },
  {
    "objectID": "computational-methods.html#diversity-and-inclusion",
    "href": "computational-methods.html#diversity-and-inclusion",
    "title": "Computational Methods for Statistics",
    "section": "",
    "text": "We must treat every individual with respect. We are diverse in many ways, and this diversity is fundamental to building and maintaining an equitable and inclusive campus community. Diversity can refer to multiple ways that we identify ourselves, including but not limited to race, color, national origin, language, sex, disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic information. Each of these diverse identities, along with many others not mentioned here, shape the perspectives our students, faculty, and staff bring to our campus. We, at CMU, will work to promote diversity, equity and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice. We acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a campus community that increasingly embraces these core values.\nEach of us is responsible for creating a safer, more inclusive environment.\nUnfortunately, incidents of bias or discrimination do occur, whether intentional or unintentional. They contribute to creating an unwelcoming environment for individuals and groups at the university. Therefore, the university encourages anyone who experiences or observes unfair or hostile treatment on the basis of identity to speak out for justice and support, within the moment of the incident or after the incident has passed. Anyone can share these experiences using the following resources:\n\nCenter for Student Diversity and Inclusion: csdi@andrew.cmu.edu, (412) 268-2150\nReport-It online anonymous reporting platform. username: tartans password: plaid\n\nAll reports will be documented and deliberated to determine if there should be any following actions. Regardless of incident type, the university will use all shared experiences to transform our campus climate to be more equitable and just."
  },
  {
    "objectID": "computational-methods.html#wellness",
    "href": "computational-methods.html#wellness",
    "title": "Computational Methods for Statistics",
    "section": "",
    "text": "All of us benefit from support during times of struggle. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is almost always helpful.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 or visit their website. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help."
  },
  {
    "objectID": "large-scale-data.html",
    "href": "large-scale-data.html",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "Spring 2025, mini 3\nMonday and Wednesday, 9:30-10:50am\nWean 4708\nInstructor: Alex Reinhart\nTAs: Elizabeth Cucuzzella\nOffice hours:\n\nAlex: Mondays 11am-noon, Baker 232K (except occasionally for travel to NYC)\nElizabeth: Tuesdays 12:30-1:30pm, Wean 3711\n\n\n\n\n\nIn the prior course, 36-614 Data Engineering and Distributed Environments, we learned how to manage large volumes of data by using relational databases and distributed systems like Spark. In this course, we’ll return to Apache Spark, learn to build distributed applications, and use its machine learning library to fit various ML models to distributed datasets. We’ll then learn about distributed computation through web APIs and see how to make and serve data requests through HTTP. All examples and assignments will be written in Python.\nThis course is primarily for students in the Master of Science in Applied Data Science program. Students in this course should have prior experience programming in Python (such as through 36-650), using Git, and working from the command-line shell.\n\n\n\nBy the end of this course, students will be able to:\n\nLoad large quantities of data into Spark.\nUse Spark to do analytics and reporting on large distributed datasets.\nUse Spark’s machine learning features to fit models on large distributed datasets.\nFit machine learning models on a cloud platforms such as Azure.\n\n\n\n\nThere is no required book for this course. We recommend the following reference resources:\n\nChambers and Zaharia, Spark: The Definitive Guide, O’Reilly. Available free through the CMU library.\n\n\n\n\nThis course features regular homework assignments, due Wednesday afternoons. Many of these will involve writing code to complete particular data tasks. Homework assignments will be posted on Canvas. Homework must be submitted as a PDF on Gradescope, and should be produced either with a Jupyter Notebook or the Azure workspace notebook system.\nIt’s your responsibility to ensure the PDF file is legible in Gradescope. This includes ensuring the pages are a reasonable size, and also using headings, formatting, and text to make it easy to find your answers. You should not include extraneous code or output not relevant to the problems.\nYou are expected to follow consistent style in your code. For Python code, the style we will use is PEP 8, the standard Python style guide. Style will be part of the homework grade.\nAlong with the homework, there will be a semester-long project, completed in groups of 2-3 students. The project will be completed in several parts throughout the semester. The project will culminate with a final submission at the end of the semester in the form of a GitHub repository containing working code.\n\n\nFor homework submissions, you will have three “grace days” you can use throughout the semester. Each time you use a grace day for an assignment, you get 24 hours extra to submit the assignment. You do not need any excuse to use grace days. Once you have used all three grace days, late work will not be accepted.\nThis system is meant to allow you flexibility, so that ordinary problems (minor illness, forgot a deadline, had to finish another class’s big assignment, traveled to an event) don’t harm you, and so you do not need my permission to handle unexpected problems. If you experience a serious emergency that prevents you from completing work for a longer time, contact me so we can make arrangements.\n\n\n\n\nClass attendance and participation is essential. If there’s any one message to be learned from pedagogical research, it’s that listening passively to a lecture is not a good way to learn how to think about complicated problems. I will often ask class questions or ask you to complete short activities in small groups. You are expected to attend class and participate in these activities.\nWhen attending class, you are expected to follow all COVID-19 precautions required by the University.\n\n\n\nThe homework and project will be the basis of course grades: 60% homework, 40% project.\nFinal grades will be based on this scale: A = [93, 100]; A- = [90, 93); B+ = [87, 90); B = [83, 87); B- = [80, 83); C+ = [77, 80); C = [73, 77); C- = [70, 73); D = [60, 70); R = [0, 60).\n\n\n\nDiscussing homework and projects with your classmates is allowed and encouraged, and helping explain ideas to each other is a core part of the academic experience. But it is important that every student get practice working on their own. This means that all the work you turn in must be your own. You must devise and write your own code, generate your own graphics, and write your own solutions and reports.\nYou may use external sources (books, websites, papers) to\n\nLook up Python documentation, find useful packages, find explanations for error messages, or remind yourself about the functions to do some task,\nFind reference materials on statistical methods,\nClarify material from the course notes or examples.\n\nBut external sources must be used to support your work, not to obtain your work. You may not use them to copy code, text, or graphics without attribution. You may not use any prior course’s or textbook’s homework solutions in any way. This prohibition applies even to students who are re-taking the course. Do not copy old solutions (in whole or in part), and do not “consult” or read them. Doing any of that is cheating, making any feedback you get meaningless and any evaluation based on that assignment unfair.\nIf you do use any material from other sources, you must clearly mark its source. Text taken from other sources must be in quotation marks with citations; figures from other sources need a caption indicating the source; and code from other sources must have a comment indicating the source. We must be able to determine who wrote any material you submit, and you must not falsely imply that you completed work actually done by others.\nPlease talk to me if you have any questions about this policy. Any form of cheating or plagiarism is grounds for sanctions to be determined by the instructor, including grade penalties or course failure. Students taking the course pass/fail may have this status revoked. I am also obliged in these situations to report the incident to your academic program and the appropriate University authorities. Please refer to the University Policy on Academic Integrity.\n\n\n\nIf you have a disability and have an accommodations letter from the Disability Resources office, I encourage you to discuss your accommodations and needs with me as early in the semester as possible. I will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, I encourage you to contact them at access@andrew.cmu.edu.\n\n\n\nWe must treat every individual with respect. We are diverse in many ways, and this diversity is fundamental to building and maintaining an equitable and inclusive campus community. Diversity can refer to multiple ways that we identify ourselves, including but not limited to race, color, national origin, language, sex, disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic information. Each of these diverse identities, along with many others not mentioned here, shape the perspectives our students, faculty, and staff bring to our campus. We, at CMU, will work to promote diversity, equity and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice. We acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a campus community that increasingly embraces these core values.\nEach of us is responsible for creating a safer, more inclusive environment.\nUnfortunately, incidents of bias or discrimination do occur, whether intentional or unintentional. They contribute to creating an unwelcoming environment for individuals and groups at the university. Therefore, the university encourages anyone who experiences or observes unfair or hostile treatment on the basis of identity to speak out for justice and support, within the moment of the incident or after the incident has passed. Anyone can share these experiences using the following resources:\n\nCenter for Student Diversity and Inclusion: csdi@andrew.cmu.edu, (412) 268-2150\nReport-It online anonymous reporting platform. username: tartans password: plaid\n\nAll reports will be documented and deliberated to determine if there should be any following actions. Regardless of incident type, the university will use all shared experiences to transform our campus climate to be more equitable and just.\n\n\n\nAll of us benefit from support during times of struggle. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is almost always helpful.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 or visit their website. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help."
  },
  {
    "objectID": "large-scale-data.html#basic-information",
    "href": "large-scale-data.html#basic-information",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "Spring 2025, mini 3\nMonday and Wednesday, 9:30-10:50am\nWean 4708\nInstructor: Alex Reinhart\nTAs: Elizabeth Cucuzzella\nOffice hours:\n\nAlex: Mondays 11am-noon, Baker 232K (except occasionally for travel to NYC)\nElizabeth: Tuesdays 12:30-1:30pm, Wean 3711"
  },
  {
    "objectID": "large-scale-data.html#course-description",
    "href": "large-scale-data.html#course-description",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "In the prior course, 36-614 Data Engineering and Distributed Environments, we learned how to manage large volumes of data by using relational databases and distributed systems like Spark. In this course, we’ll return to Apache Spark, learn to build distributed applications, and use its machine learning library to fit various ML models to distributed datasets. We’ll then learn about distributed computation through web APIs and see how to make and serve data requests through HTTP. All examples and assignments will be written in Python.\nThis course is primarily for students in the Master of Science in Applied Data Science program. Students in this course should have prior experience programming in Python (such as through 36-650), using Git, and working from the command-line shell."
  },
  {
    "objectID": "large-scale-data.html#learning-objectives",
    "href": "large-scale-data.html#learning-objectives",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "By the end of this course, students will be able to:\n\nLoad large quantities of data into Spark.\nUse Spark to do analytics and reporting on large distributed datasets.\nUse Spark’s machine learning features to fit models on large distributed datasets.\nFit machine learning models on a cloud platforms such as Azure."
  },
  {
    "objectID": "large-scale-data.html#books-and-references",
    "href": "large-scale-data.html#books-and-references",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "There is no required book for this course. We recommend the following reference resources:\n\nChambers and Zaharia, Spark: The Definitive Guide, O’Reilly. Available free through the CMU library."
  },
  {
    "objectID": "large-scale-data.html#homework-and-project",
    "href": "large-scale-data.html#homework-and-project",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "This course features regular homework assignments, due Wednesday afternoons. Many of these will involve writing code to complete particular data tasks. Homework assignments will be posted on Canvas. Homework must be submitted as a PDF on Gradescope, and should be produced either with a Jupyter Notebook or the Azure workspace notebook system.\nIt’s your responsibility to ensure the PDF file is legible in Gradescope. This includes ensuring the pages are a reasonable size, and also using headings, formatting, and text to make it easy to find your answers. You should not include extraneous code or output not relevant to the problems.\nYou are expected to follow consistent style in your code. For Python code, the style we will use is PEP 8, the standard Python style guide. Style will be part of the homework grade.\nAlong with the homework, there will be a semester-long project, completed in groups of 2-3 students. The project will be completed in several parts throughout the semester. The project will culminate with a final submission at the end of the semester in the form of a GitHub repository containing working code.\n\n\nFor homework submissions, you will have three “grace days” you can use throughout the semester. Each time you use a grace day for an assignment, you get 24 hours extra to submit the assignment. You do not need any excuse to use grace days. Once you have used all three grace days, late work will not be accepted.\nThis system is meant to allow you flexibility, so that ordinary problems (minor illness, forgot a deadline, had to finish another class’s big assignment, traveled to an event) don’t harm you, and so you do not need my permission to handle unexpected problems. If you experience a serious emergency that prevents you from completing work for a longer time, contact me so we can make arrangements."
  },
  {
    "objectID": "large-scale-data.html#attendance-and-participation",
    "href": "large-scale-data.html#attendance-and-participation",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "Class attendance and participation is essential. If there’s any one message to be learned from pedagogical research, it’s that listening passively to a lecture is not a good way to learn how to think about complicated problems. I will often ask class questions or ask you to complete short activities in small groups. You are expected to attend class and participate in these activities.\nWhen attending class, you are expected to follow all COVID-19 precautions required by the University."
  },
  {
    "objectID": "large-scale-data.html#grading",
    "href": "large-scale-data.html#grading",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "The homework and project will be the basis of course grades: 60% homework, 40% project.\nFinal grades will be based on this scale: A = [93, 100]; A- = [90, 93); B+ = [87, 90); B = [83, 87); B- = [80, 83); C+ = [77, 80); C = [73, 77); C- = [70, 73); D = [60, 70); R = [0, 60)."
  },
  {
    "objectID": "large-scale-data.html#academic-integrity",
    "href": "large-scale-data.html#academic-integrity",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "Discussing homework and projects with your classmates is allowed and encouraged, and helping explain ideas to each other is a core part of the academic experience. But it is important that every student get practice working on their own. This means that all the work you turn in must be your own. You must devise and write your own code, generate your own graphics, and write your own solutions and reports.\nYou may use external sources (books, websites, papers) to\n\nLook up Python documentation, find useful packages, find explanations for error messages, or remind yourself about the functions to do some task,\nFind reference materials on statistical methods,\nClarify material from the course notes or examples.\n\nBut external sources must be used to support your work, not to obtain your work. You may not use them to copy code, text, or graphics without attribution. You may not use any prior course’s or textbook’s homework solutions in any way. This prohibition applies even to students who are re-taking the course. Do not copy old solutions (in whole or in part), and do not “consult” or read them. Doing any of that is cheating, making any feedback you get meaningless and any evaluation based on that assignment unfair.\nIf you do use any material from other sources, you must clearly mark its source. Text taken from other sources must be in quotation marks with citations; figures from other sources need a caption indicating the source; and code from other sources must have a comment indicating the source. We must be able to determine who wrote any material you submit, and you must not falsely imply that you completed work actually done by others.\nPlease talk to me if you have any questions about this policy. Any form of cheating or plagiarism is grounds for sanctions to be determined by the instructor, including grade penalties or course failure. Students taking the course pass/fail may have this status revoked. I am also obliged in these situations to report the incident to your academic program and the appropriate University authorities. Please refer to the University Policy on Academic Integrity."
  },
  {
    "objectID": "large-scale-data.html#accommodations-for-students-with-disabilities",
    "href": "large-scale-data.html#accommodations-for-students-with-disabilities",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "If you have a disability and have an accommodations letter from the Disability Resources office, I encourage you to discuss your accommodations and needs with me as early in the semester as possible. I will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, I encourage you to contact them at access@andrew.cmu.edu."
  },
  {
    "objectID": "large-scale-data.html#diversity-and-inclusion",
    "href": "large-scale-data.html#diversity-and-inclusion",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "We must treat every individual with respect. We are diverse in many ways, and this diversity is fundamental to building and maintaining an equitable and inclusive campus community. Diversity can refer to multiple ways that we identify ourselves, including but not limited to race, color, national origin, language, sex, disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic information. Each of these diverse identities, along with many others not mentioned here, shape the perspectives our students, faculty, and staff bring to our campus. We, at CMU, will work to promote diversity, equity and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice. We acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a campus community that increasingly embraces these core values.\nEach of us is responsible for creating a safer, more inclusive environment.\nUnfortunately, incidents of bias or discrimination do occur, whether intentional or unintentional. They contribute to creating an unwelcoming environment for individuals and groups at the university. Therefore, the university encourages anyone who experiences or observes unfair or hostile treatment on the basis of identity to speak out for justice and support, within the moment of the incident or after the incident has passed. Anyone can share these experiences using the following resources:\n\nCenter for Student Diversity and Inclusion: csdi@andrew.cmu.edu, (412) 268-2150\nReport-It online anonymous reporting platform. username: tartans password: plaid\n\nAll reports will be documented and deliberated to determine if there should be any following actions. Regardless of incident type, the university will use all shared experiences to transform our campus climate to be more equitable and just."
  },
  {
    "objectID": "large-scale-data.html#wellness",
    "href": "large-scale-data.html#wellness",
    "title": "Software for Large-Scale Data",
    "section": "",
    "text": "All of us benefit from support during times of struggle. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is almost always helpful.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 or visit their website. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help."
  },
  {
    "objectID": "servers.html",
    "href": "servers.html",
    "title": "Working with Servers",
    "section": "",
    "text": "TODO\n\n\n\nThe scp command, for “secure copy”, uses the SSH protocol to move files from one computer to another.\nFor example, suppose you have a file foo.txt in your current directory.\nscp foo.txt yourusername@server.name.example.com:~/some-directory/\nThis takes foo.txt and puts it into some-directory/ in your home directory on the server. In the argument yourusername@server.name.example.com:~/some-directory/, everything before the colon is the server information, matching what you’d provide to ssh to log in; everything after the colon is the path you’d like the file to be uploaded to.\nSymmetrically,\nscp yourusername@server.name.example.com:~/some-directory/data.txt .\ncopies some-directory/data.txt to your current directory (signified by .).\nYou can also use wildcards:\nscp \"yourusername@server.name.example.com:~/data/*.csv\" data/\nThis copies all CSV files in the given directory into the data/ directory on your computer. Notice the quotation marks: without them, your local shell will try to match the wildcard * against files in your current directory. By quoting the argument, your shell won’t touch it, and it’ll be passed to the server, where the server can match * against the files present there."
  },
  {
    "objectID": "servers.html#connecting-to-a-server-with-ssh",
    "href": "servers.html#connecting-to-a-server-with-ssh",
    "title": "Working with Servers",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "servers.html#moving-files-with-scp",
    "href": "servers.html#moving-files-with-scp",
    "title": "Working with Servers",
    "section": "",
    "text": "The scp command, for “secure copy”, uses the SSH protocol to move files from one computer to another.\nFor example, suppose you have a file foo.txt in your current directory.\nscp foo.txt yourusername@server.name.example.com:~/some-directory/\nThis takes foo.txt and puts it into some-directory/ in your home directory on the server. In the argument yourusername@server.name.example.com:~/some-directory/, everything before the colon is the server information, matching what you’d provide to ssh to log in; everything after the colon is the path you’d like the file to be uploaded to.\nSymmetrically,\nscp yourusername@server.name.example.com:~/some-directory/data.txt .\ncopies some-directory/data.txt to your current directory (signified by .).\nYou can also use wildcards:\nscp \"yourusername@server.name.example.com:~/data/*.csv\" data/\nThis copies all CSV files in the given directory into the data/ directory on your computer. Notice the quotation marks: without them, your local shell will try to match the wildcard * against files in your current directory. By quoting the argument, your shell won’t touch it, and it’ll be passed to the server, where the server can match * against the files present there."
  },
  {
    "objectID": "data-engineering.html",
    "href": "data-engineering.html",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "Fall 2024, mini 2\nMonday and Wednesday, 9:30-10:50am\nWean 4709\nInstructor: Alex Reinhart\nTA: Anna Rosengart\nOffice hours:\n\nAnna: Tuesdays 1-2pm, FMS Interview Room 2\nAlex: Fridays 1:30-2:30pm, Baker 232K\n\n\n\n\n\nStatisticians and data scientists in industry are increasingly expected to work with data from complex sources: not just spreadsheets and CSV files, but relational databases, distributed databases, and streams, often integrating data from dozens of different systems. This course introduces the basic principles of data engineering, beginning with relational databases and continuing to distributed databases in the cloud. Students will learn SQL, practice using Python to query databases, and be introduced to cloud computing.\nThis course is primarily for students in the Master of Science in Applied Data Science program. Students in this course should have prior experience programming in Python (such as through 36-650), using Git, and working from the command-line shell.\n\n\n\nBy the end of this course, students will be able to:\n\nDesign SQL tables to store complex, frequently updated data.\nWrite SQL queries to group, aggregate, and summarize data stored in multiple interconnected tables.\nUse Python to execute SQL queries and dynamically act upon stored data.\nLoad data into SQL databases and update existing data.\nDevelop Python data pipelines that ingest data from multiple sources, load it into a database, and produce automated reports summarizing that data.\nRun data pipelines on cloud computing resources.\nWork with data stored in distributed file systems.\n\n\n\n\nThere is no required book for this course. We recommend the following reference resources:\n\nMark Lutz, Learning Python, 5th edition, O’Reilly. Available free through the CMU library.\nAlan Beaulieu, Learning SQL, O’Reilly. Also available free.\nThe PostgreSQL manual.\n\n\n\n\nThis course features regular homework assignments, due Wednesday afternoons. Many of these will involve writing code to complete particular data tasks. Homework assignments will be posted on Canvas. Homework must be submitted as a PDF on Gradescope, and can be produced either with a Jupyter Notebook or in a Quarto document with Python code cells.\nIt’s your responsibility to ensure the PDF file is legible in Gradescope. This includes ensuring the pages are a reasonable size, and also using headings, formatting, and text to make it easy to find your answers. You should not include extraneous code or output not relevant to the problems.\nYou are expected to follow consistent style in your code. For Python code, the style we will use is PEP 8, the standard Python style guide. Style will be part of the homework grade.\nAlong with the homework, there will be a semester-long project, completed in groups of 3-4 students. The project will be completed in several parts throughout the semester. The project will culminate with a final submission at the end of the semester in the form of a GitHub repository containing working code.\nParts of the project will be coordinated with 36-611, so you can practice relevant professional skills.\n\n\nFor homework submissions, you will have three “grace days” you can use throughout the semester. Each time you use a grace day for an assignment, you get 24 hours extra to submit the assignment. You do not need any excuse to use grace days. Once you have used all three grace days, late work will not be accepted.\nThis system is meant to allow you flexibility, so that ordinary problems (minor illness, forgot a deadline, had to finish another class’s big assignment, traveled to an event) don’t harm you, and so you do not need my permission to handle unexpected problems. If you experience a serious emergency that prevents you from completing work for a longer time, contact me so we can make arrangements.\n\n\n\n\nClass attendance and participation is essential. If there’s any one message to be learned from pedagogical research, it’s that listening passively to a lecture is not a good way to learn how to think about complicated problems. I will often ask class questions or ask you to complete short activities in small groups. You are expected to attend class and participate in these activities.\nWhen attending class, you are expected to follow all COVID-19 precautions required by the University.\n\n\n\nThe homework and project will be the basis of course grades: 60% homework, 40% project.\nFinal grades will be based on this scale: A = [93, 100]; A- = [90, 93); B+ = [87, 90); B = [83, 87); B- = [80, 83); C+ = [77, 80); C = [73, 77); C- = [70, 73); D = [60, 70); R = [0, 60).\n\n\n\nDiscussing homework and projects with your classmates is allowed and encouraged, and helping explain ideas to each other is a core part of the academic experience. But it is important that every student get practice working on their own. This means that all the work you turn in must be your own. You must devise and write your own code, generate your own graphics, and write your own solutions and reports.\nYou may use external sources (books, websites, papers) to\n\nLook up Python documentation, find useful packages, find explanations for error messages, or remind yourself about the functions to do some task,\nFind reference materials on statistical methods,\nClarify material from the course notes or examples.\n\nBut external sources must be used to support your work, not to obtain your work. You may not use them to copy code, text, or graphics without attribution. You may not use any prior course’s or textbook’s homework solutions in any way. This prohibition applies even to students who are re-taking the course. Do not copy old solutions (in whole or in part), and do not “consult” or read them. Doing any of that is cheating, making any feedback you get meaningless and any evaluation based on that assignment unfair.\nIf you do use any material from other sources, you must clearly mark its source. Text taken from other sources must be in quotation marks with citations; figures from other sources need a caption indicating the source; and code from other sources must have a comment indicating the source. We must be able to determine who wrote any material you submit, and you must not falsely imply that you completed work actually done by others.\n\n\nSome of you may be tempted to use generative AI tools like ChatGPT, Gemini, Llama, or Claude to complete some of your work in this course. These tools can help explain code and debug problems. However, the same rules described above apply: your use of generative AI must support your work. You may not simply copy and paste questions into generative AI and submit the answers as your own work. To build expertise in data engineering, you must practice basic skills so you can master the core concepts; if you outsource the basic skills, you will never get the practice you need to become an expert.\nI will consider the use of generative AI tools beyond these boundaries to be “unauthorized assistance”, as defined in the University Policy on Academic Integrity.\n\n\n\nPlease talk to me if you have any questions about this policy. Any form of cheating or plagiarism is grounds for sanctions to be determined by the instructor, including grade penalties or course failure. Students taking the course pass/fail may have this status revoked. I am also obliged in these situations to report the incident to your academic program and the appropriate University authorities. Please refer to the University Policy on Academic Integrity.\n\n\n\n\nIf you have a disability and have an accommodations letter from the Disability Resources office, I encourage you to discuss your accommodations and needs with me as early in the semester as possible. I will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, I encourage you to contact them at access@andrew.cmu.edu.\n\n\n\nWe must treat every individual with respect. We are diverse in many ways, and this diversity is fundamental to building and maintaining an equitable and inclusive campus community. Diversity can refer to multiple ways that we identify ourselves, including but not limited to race, color, national origin, language, sex, disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic information. Each of these diverse identities, along with many others not mentioned here, shape the perspectives our students, faculty, and staff bring to our campus. We, at CMU, will work to promote diversity, equity and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice. We acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a campus community that increasingly embraces these core values.\nEach of us is responsible for creating a safer, more inclusive environment.\nUnfortunately, incidents of bias or discrimination do occur, whether intentional or unintentional. They contribute to creating an unwelcoming environment for individuals and groups at the university. Therefore, the university encourages anyone who experiences or observes unfair or hostile treatment on the basis of identity to speak out for justice and support, within the moment of the incident or after the incident has passed. Anyone can share these experiences using the following resources:\n\nCenter for Student Diversity and Inclusion: csdi@andrew.cmu.edu, (412) 268-2150\nReport-It online anonymous reporting platform. username: tartans password: plaid\n\nAll reports will be documented and deliberated to determine if there should be any following actions. Regardless of incident type, the university will use all shared experiences to transform our campus climate to be more equitable and just.\n\n\n\nAll of us benefit from support during times of struggle. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is almost always helpful.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 or visit their website. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help."
  },
  {
    "objectID": "data-engineering.html#basic-information",
    "href": "data-engineering.html#basic-information",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "Fall 2024, mini 2\nMonday and Wednesday, 9:30-10:50am\nWean 4709\nInstructor: Alex Reinhart\nTA: Anna Rosengart\nOffice hours:\n\nAnna: Tuesdays 1-2pm, FMS Interview Room 2\nAlex: Fridays 1:30-2:30pm, Baker 232K"
  },
  {
    "objectID": "data-engineering.html#course-description",
    "href": "data-engineering.html#course-description",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "Statisticians and data scientists in industry are increasingly expected to work with data from complex sources: not just spreadsheets and CSV files, but relational databases, distributed databases, and streams, often integrating data from dozens of different systems. This course introduces the basic principles of data engineering, beginning with relational databases and continuing to distributed databases in the cloud. Students will learn SQL, practice using Python to query databases, and be introduced to cloud computing.\nThis course is primarily for students in the Master of Science in Applied Data Science program. Students in this course should have prior experience programming in Python (such as through 36-650), using Git, and working from the command-line shell."
  },
  {
    "objectID": "data-engineering.html#learning-objectives",
    "href": "data-engineering.html#learning-objectives",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "By the end of this course, students will be able to:\n\nDesign SQL tables to store complex, frequently updated data.\nWrite SQL queries to group, aggregate, and summarize data stored in multiple interconnected tables.\nUse Python to execute SQL queries and dynamically act upon stored data.\nLoad data into SQL databases and update existing data.\nDevelop Python data pipelines that ingest data from multiple sources, load it into a database, and produce automated reports summarizing that data.\nRun data pipelines on cloud computing resources.\nWork with data stored in distributed file systems."
  },
  {
    "objectID": "data-engineering.html#books-and-references",
    "href": "data-engineering.html#books-and-references",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "There is no required book for this course. We recommend the following reference resources:\n\nMark Lutz, Learning Python, 5th edition, O’Reilly. Available free through the CMU library.\nAlan Beaulieu, Learning SQL, O’Reilly. Also available free.\nThe PostgreSQL manual."
  },
  {
    "objectID": "data-engineering.html#homework-and-project",
    "href": "data-engineering.html#homework-and-project",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "This course features regular homework assignments, due Wednesday afternoons. Many of these will involve writing code to complete particular data tasks. Homework assignments will be posted on Canvas. Homework must be submitted as a PDF on Gradescope, and can be produced either with a Jupyter Notebook or in a Quarto document with Python code cells.\nIt’s your responsibility to ensure the PDF file is legible in Gradescope. This includes ensuring the pages are a reasonable size, and also using headings, formatting, and text to make it easy to find your answers. You should not include extraneous code or output not relevant to the problems.\nYou are expected to follow consistent style in your code. For Python code, the style we will use is PEP 8, the standard Python style guide. Style will be part of the homework grade.\nAlong with the homework, there will be a semester-long project, completed in groups of 3-4 students. The project will be completed in several parts throughout the semester. The project will culminate with a final submission at the end of the semester in the form of a GitHub repository containing working code.\nParts of the project will be coordinated with 36-611, so you can practice relevant professional skills.\n\n\nFor homework submissions, you will have three “grace days” you can use throughout the semester. Each time you use a grace day for an assignment, you get 24 hours extra to submit the assignment. You do not need any excuse to use grace days. Once you have used all three grace days, late work will not be accepted.\nThis system is meant to allow you flexibility, so that ordinary problems (minor illness, forgot a deadline, had to finish another class’s big assignment, traveled to an event) don’t harm you, and so you do not need my permission to handle unexpected problems. If you experience a serious emergency that prevents you from completing work for a longer time, contact me so we can make arrangements."
  },
  {
    "objectID": "data-engineering.html#attendance-and-participation",
    "href": "data-engineering.html#attendance-and-participation",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "Class attendance and participation is essential. If there’s any one message to be learned from pedagogical research, it’s that listening passively to a lecture is not a good way to learn how to think about complicated problems. I will often ask class questions or ask you to complete short activities in small groups. You are expected to attend class and participate in these activities.\nWhen attending class, you are expected to follow all COVID-19 precautions required by the University."
  },
  {
    "objectID": "data-engineering.html#grading",
    "href": "data-engineering.html#grading",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "The homework and project will be the basis of course grades: 60% homework, 40% project.\nFinal grades will be based on this scale: A = [93, 100]; A- = [90, 93); B+ = [87, 90); B = [83, 87); B- = [80, 83); C+ = [77, 80); C = [73, 77); C- = [70, 73); D = [60, 70); R = [0, 60)."
  },
  {
    "objectID": "data-engineering.html#academic-integrity",
    "href": "data-engineering.html#academic-integrity",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "Discussing homework and projects with your classmates is allowed and encouraged, and helping explain ideas to each other is a core part of the academic experience. But it is important that every student get practice working on their own. This means that all the work you turn in must be your own. You must devise and write your own code, generate your own graphics, and write your own solutions and reports.\nYou may use external sources (books, websites, papers) to\n\nLook up Python documentation, find useful packages, find explanations for error messages, or remind yourself about the functions to do some task,\nFind reference materials on statistical methods,\nClarify material from the course notes or examples.\n\nBut external sources must be used to support your work, not to obtain your work. You may not use them to copy code, text, or graphics without attribution. You may not use any prior course’s or textbook’s homework solutions in any way. This prohibition applies even to students who are re-taking the course. Do not copy old solutions (in whole or in part), and do not “consult” or read them. Doing any of that is cheating, making any feedback you get meaningless and any evaluation based on that assignment unfair.\nIf you do use any material from other sources, you must clearly mark its source. Text taken from other sources must be in quotation marks with citations; figures from other sources need a caption indicating the source; and code from other sources must have a comment indicating the source. We must be able to determine who wrote any material you submit, and you must not falsely imply that you completed work actually done by others.\n\n\nSome of you may be tempted to use generative AI tools like ChatGPT, Gemini, Llama, or Claude to complete some of your work in this course. These tools can help explain code and debug problems. However, the same rules described above apply: your use of generative AI must support your work. You may not simply copy and paste questions into generative AI and submit the answers as your own work. To build expertise in data engineering, you must practice basic skills so you can master the core concepts; if you outsource the basic skills, you will never get the practice you need to become an expert.\nI will consider the use of generative AI tools beyond these boundaries to be “unauthorized assistance”, as defined in the University Policy on Academic Integrity.\n\n\n\nPlease talk to me if you have any questions about this policy. Any form of cheating or plagiarism is grounds for sanctions to be determined by the instructor, including grade penalties or course failure. Students taking the course pass/fail may have this status revoked. I am also obliged in these situations to report the incident to your academic program and the appropriate University authorities. Please refer to the University Policy on Academic Integrity."
  },
  {
    "objectID": "data-engineering.html#accommodations-for-students-with-disabilities",
    "href": "data-engineering.html#accommodations-for-students-with-disabilities",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "If you have a disability and have an accommodations letter from the Disability Resources office, I encourage you to discuss your accommodations and needs with me as early in the semester as possible. I will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, I encourage you to contact them at access@andrew.cmu.edu."
  },
  {
    "objectID": "data-engineering.html#diversity-and-inclusion",
    "href": "data-engineering.html#diversity-and-inclusion",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "We must treat every individual with respect. We are diverse in many ways, and this diversity is fundamental to building and maintaining an equitable and inclusive campus community. Diversity can refer to multiple ways that we identify ourselves, including but not limited to race, color, national origin, language, sex, disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic information. Each of these diverse identities, along with many others not mentioned here, shape the perspectives our students, faculty, and staff bring to our campus. We, at CMU, will work to promote diversity, equity and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice. We acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a campus community that increasingly embraces these core values.\nEach of us is responsible for creating a safer, more inclusive environment.\nUnfortunately, incidents of bias or discrimination do occur, whether intentional or unintentional. They contribute to creating an unwelcoming environment for individuals and groups at the university. Therefore, the university encourages anyone who experiences or observes unfair or hostile treatment on the basis of identity to speak out for justice and support, within the moment of the incident or after the incident has passed. Anyone can share these experiences using the following resources:\n\nCenter for Student Diversity and Inclusion: csdi@andrew.cmu.edu, (412) 268-2150\nReport-It online anonymous reporting platform. username: tartans password: plaid\n\nAll reports will be documented and deliberated to determine if there should be any following actions. Regardless of incident type, the university will use all shared experiences to transform our campus climate to be more equitable and just."
  },
  {
    "objectID": "data-engineering.html#wellness",
    "href": "data-engineering.html#wellness",
    "title": "Data Engineering and Distributed Environments",
    "section": "",
    "text": "All of us benefit from support during times of struggle. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is almost always helpful.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 or visit their website. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nThis site contains lecture notes and materials for the data engineering and computing sequence of the Master of Science in Applied Data Science program, including the courses:\n\n36-614 Data Engineering and Distributed Environments\n36-615 Software for Large-Scale Data\n36-616 Computational Methods for Statistics\n\nThese courses are primarily for students in the MADS program. Other students will only be admitted with the instructor’s permission."
  },
  {
    "objectID": "data-engineering/advanced-sql.html",
    "href": "data-engineering/advanced-sql.html",
    "title": "Advanced SQL",
    "section": "",
    "text": "Note\n\n\n\nPortions of these notes are based on material co-developed with Christopher Genovese for 36-750.\n\n\n\n\nThe key design principle for database schema is to keep the design DRY – that is, eliminate data redundancy. The process of making a design DRY is called normalization, and a DRY database is said to be in “normal form.”\nThe basic modeling process:\n\nIdentify and model the entities in your problem\nModel the relationships between entities\nInclude relevant attributes\nNormalize by the steps below\n\n\nExample 1 (Managing songs) Consider a database to manage songs:\n\n\n\nAlbum\nArtist\nLabel\nSongs\n\n\n\n\nTalking Book\nStevie Wonder\nMotown\nYou are the sunshine of my life, Maybe your baby, Superstition, …\n\n\nMiles Smiles\nMiles Davis Quintet\nColumbia\nOrbits, Circle, …\n\n\nSpeak No Evil\nWayne Shorter\nBlue Note\nWitch Hunt, Fee-Fi-Fo-Fum, …\n\n\nHeadhunters\nHerbie Hancock\nColumbia\nChameleon, Watermelon Man, …\n\n\nMaiden Voyage\nHerbie Hancock\nBlue Note\nMaiden Voyage\n\n\nAmerican Fool\nJohn Couger\nRiva\nHurts so good, Jack & Diane, …\n\n\n\nThis seems fine at first, but why might this format be problematic or inconvenient?\n\nIt’s difficult to get individual songs from a long list in one column\nIf we want to store additional information about each artist or label, where do we put it?\nArtists often have “Best Of” albums that repeat many songs from previous albums; if we store information about each song, does it get duplicated?\nA few thoughts:\n\nWhat happens if an artist changes names partway through his or her career (e.g., John Cougar)?\nSuppose we want mis-spelled “Herbie Hancock” and wanted to update it. We would have to change every row corresponding to a Herbie Hancock album.\nSuppose we want to search for albums with a particular song; we have to search specially within the list for each album.\n\n\nLet’s write this schema as Album (artist, name, record_label, song_list), where Album is the entity and the labels in parentheses are its attributes. To normalize this design, we will add new entities and define their attributes so each piece of data has a single authoritative copy.\n\n\n\nFirst, we give entities unique identifiers. This will be their primary key.\nKey features of a primary key are that it is unique, non-null, and it never changes for the lifetime of the entity.\nSome entities already have unique identifiers attached to them – but you should consider whether those identifiers can ever change. Social Security numbers are often used to uniquely identify people, but they can be changed in rare circumstances; books can be uniquely identified by ISBNs, but these change for new editions or formats; and sometimes supposedly unique numbers get accidentally reused by other people you can’t control.\n\n\n\nWhat does each attribute describe? What attributes are repeated in Albums, either implicitly or explicitly?\nConsider the relationship between albums and songs. An album can have one or more songs; in other words, the attribute song_list is non-atomic (it is composed of other types, in this case a list of text strings). The attribute describes a collection of another entity – Song.\nSo, we now have two entities, Album and Song. How do we express these entities in our design? It depends on our model. Let’s look at two ways this could play out.\n\nAssume (at least hypothetically) that each song can only appear on one album. Then Album and Song would have a one-to-many relationship.\n\nAlbum(id, title, label, artist)\nSong(id, name, duration, album_id)\n\nQuestion: What do our CREATE TABLE commands look like under this model?\nAlternatively, suppose our model recognizes that while an album can have one or more songs, a song can also appear on one or more albums (e.g., a greatest hits album). Then, these two entities have a many-to-many relationship.\nThis gives us two entities that look like:\n\nAlbum(id, title, label, artist)\nSong(id, name, duration)\n\nThis is fine, but it doesn’t seem to capture that many-to-many relationship. How should we capture that?\n\nAn answer: This model actually describes a new entity – Track. The schema looks like:\n\nAlbum(id, title, label, artist)\nSong(id, name, duration)\nTrack(id, song_id, album_id, index)\n\n\n\n\n\n\nThis step is satisfied if each non-key column in the table serves to describe what the primary key identifies.\nAny attributes that do not satisfy this condition should be moved to another table.\nIn our schema of the last step (and in the example table), both the artist and label field contain data that describes something else. We should move these to new tables, which leads to two new entities:\n\nArtist(id, name)\nRecordLabel(id, name, street_address, city, state_name, state_abbrev, zip)\n\nEach of these may have additional attributes. For instance, producer in the latter case, and in the former, we may have additional entities describing members in the band.\nWe could then update our Album schema to use these as foreign keys: Album(id, title, label_id, artist_id).\n\n\n\nConsider RecordLabel. The state_name, state_abbrev, and zip code are all non-key fields that depend on each other. (If you know the zip code, you know the state name and thus the abbreviation.)\nThis suggests to another entity State, with name and abbreviation as attributes. And so on.\n\n\n\nIf we take this process to the extreme, we could end up with dozens of tables, each describing a unique entity and linked to dozens of other tables. Indeed, if you look up “database normalization”, you will find there are extensive and detailed definitions of different degrees of normalization and the properties required of tables in each.\nWe can exercise our judgment to determine the amount of normalization required; real-world applications rarely have perfectly normalized tables. For example, if you’re building a database of songs for your own use to track your record collection, you probably don’t care too much that RecordLabel has both a state_name and a state_abbrev; you don’t care too much about the label, and for the queries you’re doing, it’s not important that these could be moved to their own State table. It’s convenient to just have them in RecordLabel and be able to get them without a join.\nBut if you were building an authoritative song database for a music streaming service, you may need your database schema to be much more robust and detailed: you need to track enough information to know where to send royalty checks, you have millions of songs, you deal with thousands of record labels with similar-sounding names that change owners and locations, and the cost of messing up is high; so you’d like your database to be as accurate and consistent as possible at all times. Structuring the database in normal form lets the RDB software help you ensure it is consistent.\n\nExercise 1 (Schema for songs) Based on the discussion above, write a series of CREATE TABLE commands to create a set of tables for tracking songs. Your schema should support songs that appear on multiple albums, basic information about artists (such as their name and year of birth), information about record labels, song titles and duration, and other things from the discussion above.\n\n\nSolution. Here’s one example schema; names and details may vary:\nCREATE TABLE artist (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    birthdate DATE\n);\n\nCREATE TABLE record_label (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    street_address TEXT,\n    state_abbrev CHAR(2),\n    zip CHAR(5) -- ZIPs aren't integers; they can begin with 0\n);\n\nCREATE TABLE album (\n    id SERIAL PRIMARY KEY,\n    title TEXT,\n    label_id INTEGER REFERENCES record_label (id),\n    artist_id INTEGER REFERENCES artist (id)\n);\n\nCREATE TABLE song (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    duration INTERVAL -- represents time interval\n);\n\nCREATE TABLE track (\n    id SERIAL PRIMARY KEY,\n    song_id INTEGER REFERENCES song (id),\n    album_id INTEGER REFERENCES album (id)\n);\nThe important parts are that songs, tracks, and albums are separate entities; that reasonable types (like text and char) are used; and that REFERENCES is used to enforce foreign key relationships.\n\n\nExercise 2 (Schema for sports) Pick your favorite sport (or, at least, a sport you know something about). Consider designing a database to track players, teams, and games. Write a schema (as a series of CREATE TABLE statements) to store information about them. Your schema should support queries that:\n\nIdentify which players were on a team at a particular time (as players can move between teams)\nIdentify which team won a particular game, and where that game was played\nFind all the games a particular player was involved in. (You don’t have to track if they sat out particular games, just which games their team played in.)\nFind all games played in a particular place.\n\nYou can support additional query types, but start with a schema that can handle these.\n\n\n\n\n\nTransactions are the fundamental mechanism relational databases use to manage both atomicity and concurrency. How do we ensure multiple queries run atomically (either all succeed or all fail), and how do we allow multiple people to be changing the database at the same time? To see the problem, consider an example.\n\nExample 2 (The need for transactions) Suppose we’re working with an R data frame, except it is shared: multiple people can edit it at once, and their edits instantaneously change the data frame. (There’s no easy way to do this in base R; we’re just using R syntax to illustrate the problem a SQL database faces.)\nUser 1 and user 2 are both working with the bank_accounts data frame, which stores bank balances and information. User 1 needs to record that account 777 transferred $500 to account 778; user 2 needs to record that account 777 withdrew $50 in cash.\nEach column shows one user’s operations; each line is one time point, so two side-by-side lines occur at the same time:\n\n\n\n# User 1:\n\na777 &lt;- bank_accounts |&gt;\n  filter(account_id = 777)\nstart &lt;- a777$balance\n\na778 &lt;- bank_accounts |&gt;\n  filter(account_id = 778)\n\na777$balance &lt;- start - 500\na778$balance &lt;- a778$balance + 500\n\n\n# User 2:\n\na777 &lt;- bank_accounts |&gt;\n  filter(account_id = 777)\namount &lt;- a777$balance\n\na777$balance &lt;- amount - 50\n\n\n\nSuppose both accounts start with $1000. The desired outcome is that account 777 ends with $450 ($500 sent to 778 and $50 withdrawn) and 778 ends with $1500. But instead:\n\nUser 1 stores $1000 in start.\nUser 2 stores $1000 in amount.\nUser 2 updates account 777’s balance to be amount - 50, or $950.\nUser 1 updates account 777’s balance to be start - 500, or $500.\nUser 1 updates account 778’s balance to be $500 higher, or $1500.\n\nAt the end, 777’s balance is $500, not $450! All because of the timing of the operations. If user 1’s work was guaranteed to happen first, without user 2 interfering, this wouldn’t happen.\n\nPostgreSQL uses Multiversion Concurrency Control, which means that each database user sees a snapshot of the database as it was at some specific point in time. If someone else is changing the database at the same time, this does not affect queries that are currently running, so the data they see remains self-consistent.\nThe basic mechanism is straightforward, though the implementation is complicated. Queries are grouped into transactions using BEGIN and COMMIT statements. The queries within a transaction do not update the data seen by other users until COMMIT “commits” the changes. An error in any query in the transaction halts the entire transaction and “rolls back” its changes, so it is as if it never happened.\n\nExample 3 (Concurrency with transactions) Suppose we’re implementing Example 2 using PostgreSQL and its default concurrency settings. Each user submits a transaction:\n-- User 1:\nBEGIN;\n\nUPDATE bank_accounts\nSET balance = balance - 500::money\nWHERE account_id = 777;\n\nUPDATE bank_accounts\nSET balance = balance + 500::money\nWHERE account_id = 778;\n\nCOMMIT;\n\n-- User 2\nBEGIN;\n\nUPDATE bank_accounts\nSET balance = balance - 50::money\nWHERE account_id = 777;\n\nCOMMIT;\nInternally, PostgreSQL must do the same work as in Example 2—but it prevents any problems from occurring:\n\nUser 1 begins by changing account 777. This locks the row, preventing other changes until user 1’s transaction is complete.\nUser 2 then attempts to change it. The UPDATE query must wait for the lock to be released, so their query is stalled.\nUser 1 commits their transaction with the updated balances, releasing the lock on account 777.\nUser 2’s query can now go ahead, subtracting $50 from the account’s balance.\n\nThis avoids the problem entirely, at the cost of sometimes requiring queries to wait for others to complete.\n\nPostgreSQL’s locking system automatically locks individual rows or entire tables when UPDATE, INSERT, or other commands make it necessary. The locks are released when the transaction is committed, allowing other changes to be made to the same row or table. SELECT does not lock anything, but PostgreSQL does ensure that a SELECT only sees the version of the database as it was when the query began: if a SELECT involves complicated joins and aggregations that take some time to execute, and the database is changed by another user while those happen, the SELECT’s results will not be affected by those changes. This prevents strange inconsistencies where rows change while they’re being sorted and joined.\nTransactions also provide the atomicity required by the ACID guarantees: PostgreSQL ensures that either all queries in a transaction succeed or none of them do.\nFor example, suppose a transaction involves 10 queries doing various updates, but the 6th query causes an error. The entire transaction will be aborted and PostgreSQL will refuse to execute any further queries until you send a ROLLBACK command, which reverts the database back to its state before the transaction started.\nIn Example 3, atomicity is important for user 1: if an error occurs between the two UPDATE queries, we would have subtracted $500 from one account but not added it to the other, and the $500 would be lost. Grouping the queries into a transaction makes this impossible.\nPostgreSQL also supports “savepoints”, which are like checkpoints inside a transaction. (Or quicksaves, if you play video games.) Within one transaction, we can make multiple savepoints, and revert back to a specific changepoint instead of rolling back the entire transaction. Again, the changes only become visible to other users when the entire transaction is committed, but this gives us flexibility in dealing with errors and selectively undoing changes. See the SAVEPOINT reference for examples.\n\n\n\nSQL allows subqueries, where part of a query is itself defined by another query.\nThe simplest case is in FROM clauses. Normally a FROM clause specifies a table, or multiple tables combined with joins. But we can also select from the table of results of another query. Here’s a trivial example using the tables defined in ?@sec-example-db:\nSELECT * FROM\n(SELECT moment, persona, score\n FROM events) AS e\nINNER JOIN\n(SELECT id, firstname, lastname\n FROM personae) AS p\nON e.persona = p.id;\nThe subqueries are given in parentheses. Note they can’t refer to things outside of the subqueries; essentially, Postgres runs the subqueries first, gets the tables of results, and then uses them like it would use any other table in the outer query.\nThis query, of course, could be written easily without the subequeries. But often in complicated queries with WHERE, GROUP BY, and HAVING clauses, it can be easier to understand when written with subqueries than when written as one large query.\nYou can also use subqueries within a WHERE clause. These are called subquery expressions.\nSELECT moment, persona, score FROM events\nWHERE persona IN (SELECT id FROM personae\n                  WHERE account_balance &lt; 0::money);\nHere the subquery returns exactly one column (id), and so IN checks if each event’s persona is contained in that column. This query could be written equivalently as a join, but sometimes it’s easier to work with subqueries to put together a larger query out of pieces, instead of writing one large query to do everything.\nThere are various other functions/operators that can be used on subqueries as well, such as IN, NOT IN, EXISTS, ANY, ALL, and SOME.\nFor example, this looks like an inner join:\nSELECT moment, persona, score FROM events\nWHERE EXISTS (SELECT 1 FROM personae WHERE id = events.persona);\nHere EXISTS is an operator that looks for there to be at least one row in the subquery. The subquery does refer to the outer tables and variables, so you can think of this subquery as being run once per row in events, to determine if there is a matching persona. If the subquery does not return any rows, that event is not included in the final results.\n\nExercise 3 (Translating subqueries) Translate the following queries to JOIN queries without any subqueries.\n\n   SELECT moment, persona, score FROM events\n   WHERE persona IN (SELECT id FROM personae\n                     WHERE account_balance &lt; 0::money);\n   SELECT moment, persona, score FROM events\n   WHERE EXISTS (SELECT 1 FROM personae\n                 WHERE id = events.persona);\n\n\n\nSolution. These examples don’t really illustrate the power of subqueries, but nevertheless.\nSELECT moment, persona, score\nFROM events\nINNER JOIN personae ON events.persona = personae.id\nWHERE personae.account_balance &lt; 0;\n\nSELECT moment, persona, score\nFROM events\nINNER JOIN personae ON events.persona = personae.id;\n\n\n\n\n?@sec-grouping-aggregate introduced aggregate functions and the GROUP BY clause, which allowed us to split data into groups and summarize the groups with aggregates. GROUP BY is fairly restrictive, however: we can only split the data into disjoint groups by unique values of a column, or unique combinations of several columns.\nSometimes we want grouping and aggregation over overlapping groups. For example, if I have data on students at CMU, I might want averages by their department, by college, and overall averages for the university. With GROUP BY that requires running three different queries.\nInstead, we can provide grouping sets, where we provide multiple sets of variables to group by and aggregate over. For example:\nSELECT element, persona, AVG(score), COUNT(*)\nFROM events\nGROUP BY GROUPING SETS ((element), (persona), ());\nThis query asks Postgres to group by three different sets of grouping variables: element, persona, and (), which represents no grouping (aggregate all the data). The result looks like this:\n element | persona |         avg          | count\n---------+---------+----------------------+-------\n         |         | 507.6059701492537313 |  1005\n   29409 |         | 335.8750000000000000 |     8\n   29406 |         | 588.1764705882352941 |    17\n   29408 |         | 572.2666666666666667 |    15\n   29395 |         | 623.5454545454545455 |    11\n   29430 |         | 588.0000000000000000 |    10\n[...]\n         |    1162 | 567.5384615384615385 |    13\n         |    1233 | 437.6666666666666667 |     3\n         |    1157 | 333.5000000000000000 |    12\n         |    1247 | 508.4117647058823529 |    17\n         |    1189 | 546.6666666666666667 |     6\n[...]\nThe blanks here represent NULLs. The first row is the overall average score of all events. The next rows are averages for specific elements, followed by averages for specific personas. Rows grouped by element have a NULL for persona and vice versa.\nNotice the SELECT could list both the element and persona columns. If we simply did GROUP BY element, we couldn’t list persona in the SELECT column list because it’s not the grouping column or an aggregate—but here, Postgres is smart enough to understand that it’s in some of the grouping sets, so it will fill it in for those rows and leave it NULL otherwise.\nThere are other shortcuts. Often we do hierarchical aggregates, like the CMU example above. If we had a students table with columns for department and college, we could write\nSELECT department, college, COUNT(*)\nFROM students\nGROUP BY ROLLUP (college, department);\nThis will first group by (college, department), producing one row per department in each college, then group by college, then group by (), producing (hypothetical) results like\n college | department | count\n---------+------------+-------\n         |            | 16335\nDietrich | Statistics |   632\nDietrich |    History |   127\n[...]\nDietrich |            |  2310\n     MCS |            |  6847\n[...]\nFinally, we can group by every combination of a set of variables. GROUP BY CUBE (a, b, c) will group by (a, b, c); then by (a, b), (a, c), and (b, c); then by a, and so on, all the way up to ().\n\n\n\nIn ?@sec-grouping-aggregate, we saw how GROUP BY and aggregate functions allowed us to calculate aggregate values across many rows. We could, for instance, get the average score per student, or the maximum latency per student, or the earliest event, or…\nIn each of those uses, we were limited to getting one row per group. We had to write our SELECT clauses to ensure each column we requested had one result per group.\nBut sometimes I want to get individual rows of data, with additional information attached about the group. For example, using the events table from ?@sec-example-db, how could we get the score for every individual event, compared to that student’s average score for all events?\nWe could do this with a clever subquery (Exercise 4), but an alternative is a window function. While an aggregate function takes a group and produces a single row for that group, a window function can calculate an aggregate for every row.\nFor example, let’s get each event’s score, and the z-score of that event relative to the student’s average:\nSELECT\n  id, persona, element, score,\n  ((score - avg(score) OVER (PARTITION BY persona)) /\n   stddev_samp(score) OVER (PARTITION BY persona)) AS z_score\nFROM events\nLIMIT 10;\navg() and stddev_samp() are aggregate functions, but OVER turns them into a window function. The OVER clause tells Postgres which rows to calculate the aggregate for. Here we’ve told it to PARTITION BY persona, so each row gets the average and standard deviation calculated from rows with the same persona.\nWindow functions don’t have to operate on the entire partition. For instance, if we specify ORDER BY in the OVER clause, the window is (by default) the first row in the partition up to the current row, but not the following rows. For instance, this query gets the z score relative to all previous events by the user, but not future ones:\nSELECT\n  id, persona, element, score, moment,\n  ((score - avg(score) OVER (PARTITION BY persona ORDER BY moment)) /\n   stddev_samp(score) OVER (PARTITION BY persona ORDER BY moment)) AS z_score\nFROM events\nLIMIT 10;\nInstead of repeating the PARTITION BY clause, we can name a window:\nSELECT\n  id, persona, element, score, moment,\n  ((score - avg(score) OVER p) /\n   stddev_samp(score) OVER p) AS z_score\nFROM events\nWINDOW p AS (PARTITION BY persona ORDER BY moment)\nLIMIT 10;\nNaturally, there are many additional options to customize this; you could have the window be the following rows, exclude the current row (like a studentized z score), be only some of the rows in the partition (by including a FILTER before OVER), or many other complicated variations.\nWindow functions can only be used in the SELECT column list and in ORDER BY, but not in GROUP BY, HAVING, or WHERE clauses, because they’re calculated after these are run.\nThere are also specific window functions that are not aggregate functions. For example, rank() gives the rank of each row within the partition.\nSee the window function tutorial and reference manual page for details.\n\nExercise 4 (Writing window functions with a subquery) Write a query to get the z-score for every event score, relative to the student’s overall average score. Write this using a join and a subquery, rather than by using a window function.\nThe result should have the same columns as the first version in the window function example above.\nNote: Once you write the query, it’s worth thinking about how much more difficult it would be to do the second window function example above, where we included ORDER BY. The simple aggregation approach that worked here would not work for that query.\n\n\nSolution. Here’s one way:\nSELECT e.id, e.persona, e.score, (e.score - z.avg_score) / z.sd_score AS z_score\nFROM events AS e\nLEFT JOIN\n(SELECT persona, avg(score) AS avg_score, stddev_samp(score) AS sd_score\n FROM events\n GROUP BY persona) AS z\nON e.persona = z.persona;\nNotice we’re using a LEFT JOIN so we get one row per event, rather than one row per persona (as in a RIGHT JOIN).\n\n\nExercise 5 (Window functions) Write each of the following queries using window functions.\n\nFor each event, report the difference between the student’s score and the average score of all students who did that element. (element identifies the specific quiz question or instructional item being scored.) Report columns for the student ID, element ID, score, and difference from average.\nFor each event, give the rank of the student’s score out of all students who completed that element, where 1 means they got the highest score. Report columns for the student, element, score, and rank.\nHint: The rank() function, described in table 9.63 of the manual, will be useful here. There are examples in the official tutorial.\n\n\n\nSolution. Here’s one way:\nSELECT persona, element, score,\n       score - avg(score) OVER (PARTITION BY element) AS score_diff\nFROM events;\n\nSELECT persona, element, score,\n       rank() OVER (PARTITION BY element ORDER BY score DESC)\nFROM events;"
  },
  {
    "objectID": "data-engineering/advanced-sql.html#sec-schema-design",
    "href": "data-engineering/advanced-sql.html#sec-schema-design",
    "title": "Advanced SQL",
    "section": "",
    "text": "The key design principle for database schema is to keep the design DRY – that is, eliminate data redundancy. The process of making a design DRY is called normalization, and a DRY database is said to be in “normal form.”\nThe basic modeling process:\n\nIdentify and model the entities in your problem\nModel the relationships between entities\nInclude relevant attributes\nNormalize by the steps below\n\n\nExample 1 (Managing songs) Consider a database to manage songs:\n\n\n\nAlbum\nArtist\nLabel\nSongs\n\n\n\n\nTalking Book\nStevie Wonder\nMotown\nYou are the sunshine of my life, Maybe your baby, Superstition, …\n\n\nMiles Smiles\nMiles Davis Quintet\nColumbia\nOrbits, Circle, …\n\n\nSpeak No Evil\nWayne Shorter\nBlue Note\nWitch Hunt, Fee-Fi-Fo-Fum, …\n\n\nHeadhunters\nHerbie Hancock\nColumbia\nChameleon, Watermelon Man, …\n\n\nMaiden Voyage\nHerbie Hancock\nBlue Note\nMaiden Voyage\n\n\nAmerican Fool\nJohn Couger\nRiva\nHurts so good, Jack & Diane, …\n\n\n\nThis seems fine at first, but why might this format be problematic or inconvenient?\n\nIt’s difficult to get individual songs from a long list in one column\nIf we want to store additional information about each artist or label, where do we put it?\nArtists often have “Best Of” albums that repeat many songs from previous albums; if we store information about each song, does it get duplicated?\nA few thoughts:\n\nWhat happens if an artist changes names partway through his or her career (e.g., John Cougar)?\nSuppose we want mis-spelled “Herbie Hancock” and wanted to update it. We would have to change every row corresponding to a Herbie Hancock album.\nSuppose we want to search for albums with a particular song; we have to search specially within the list for each album.\n\n\nLet’s write this schema as Album (artist, name, record_label, song_list), where Album is the entity and the labels in parentheses are its attributes. To normalize this design, we will add new entities and define their attributes so each piece of data has a single authoritative copy.\n\n\n\nFirst, we give entities unique identifiers. This will be their primary key.\nKey features of a primary key are that it is unique, non-null, and it never changes for the lifetime of the entity.\nSome entities already have unique identifiers attached to them – but you should consider whether those identifiers can ever change. Social Security numbers are often used to uniquely identify people, but they can be changed in rare circumstances; books can be uniquely identified by ISBNs, but these change for new editions or formats; and sometimes supposedly unique numbers get accidentally reused by other people you can’t control.\n\n\n\nWhat does each attribute describe? What attributes are repeated in Albums, either implicitly or explicitly?\nConsider the relationship between albums and songs. An album can have one or more songs; in other words, the attribute song_list is non-atomic (it is composed of other types, in this case a list of text strings). The attribute describes a collection of another entity – Song.\nSo, we now have two entities, Album and Song. How do we express these entities in our design? It depends on our model. Let’s look at two ways this could play out.\n\nAssume (at least hypothetically) that each song can only appear on one album. Then Album and Song would have a one-to-many relationship.\n\nAlbum(id, title, label, artist)\nSong(id, name, duration, album_id)\n\nQuestion: What do our CREATE TABLE commands look like under this model?\nAlternatively, suppose our model recognizes that while an album can have one or more songs, a song can also appear on one or more albums (e.g., a greatest hits album). Then, these two entities have a many-to-many relationship.\nThis gives us two entities that look like:\n\nAlbum(id, title, label, artist)\nSong(id, name, duration)\n\nThis is fine, but it doesn’t seem to capture that many-to-many relationship. How should we capture that?\n\nAn answer: This model actually describes a new entity – Track. The schema looks like:\n\nAlbum(id, title, label, artist)\nSong(id, name, duration)\nTrack(id, song_id, album_id, index)\n\n\n\n\n\n\nThis step is satisfied if each non-key column in the table serves to describe what the primary key identifies.\nAny attributes that do not satisfy this condition should be moved to another table.\nIn our schema of the last step (and in the example table), both the artist and label field contain data that describes something else. We should move these to new tables, which leads to two new entities:\n\nArtist(id, name)\nRecordLabel(id, name, street_address, city, state_name, state_abbrev, zip)\n\nEach of these may have additional attributes. For instance, producer in the latter case, and in the former, we may have additional entities describing members in the band.\nWe could then update our Album schema to use these as foreign keys: Album(id, title, label_id, artist_id).\n\n\n\nConsider RecordLabel. The state_name, state_abbrev, and zip code are all non-key fields that depend on each other. (If you know the zip code, you know the state name and thus the abbreviation.)\nThis suggests to another entity State, with name and abbreviation as attributes. And so on.\n\n\n\nIf we take this process to the extreme, we could end up with dozens of tables, each describing a unique entity and linked to dozens of other tables. Indeed, if you look up “database normalization”, you will find there are extensive and detailed definitions of different degrees of normalization and the properties required of tables in each.\nWe can exercise our judgment to determine the amount of normalization required; real-world applications rarely have perfectly normalized tables. For example, if you’re building a database of songs for your own use to track your record collection, you probably don’t care too much that RecordLabel has both a state_name and a state_abbrev; you don’t care too much about the label, and for the queries you’re doing, it’s not important that these could be moved to their own State table. It’s convenient to just have them in RecordLabel and be able to get them without a join.\nBut if you were building an authoritative song database for a music streaming service, you may need your database schema to be much more robust and detailed: you need to track enough information to know where to send royalty checks, you have millions of songs, you deal with thousands of record labels with similar-sounding names that change owners and locations, and the cost of messing up is high; so you’d like your database to be as accurate and consistent as possible at all times. Structuring the database in normal form lets the RDB software help you ensure it is consistent.\n\nExercise 1 (Schema for songs) Based on the discussion above, write a series of CREATE TABLE commands to create a set of tables for tracking songs. Your schema should support songs that appear on multiple albums, basic information about artists (such as their name and year of birth), information about record labels, song titles and duration, and other things from the discussion above.\n\n\nSolution. Here’s one example schema; names and details may vary:\nCREATE TABLE artist (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    birthdate DATE\n);\n\nCREATE TABLE record_label (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    street_address TEXT,\n    state_abbrev CHAR(2),\n    zip CHAR(5) -- ZIPs aren't integers; they can begin with 0\n);\n\nCREATE TABLE album (\n    id SERIAL PRIMARY KEY,\n    title TEXT,\n    label_id INTEGER REFERENCES record_label (id),\n    artist_id INTEGER REFERENCES artist (id)\n);\n\nCREATE TABLE song (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    duration INTERVAL -- represents time interval\n);\n\nCREATE TABLE track (\n    id SERIAL PRIMARY KEY,\n    song_id INTEGER REFERENCES song (id),\n    album_id INTEGER REFERENCES album (id)\n);\nThe important parts are that songs, tracks, and albums are separate entities; that reasonable types (like text and char) are used; and that REFERENCES is used to enforce foreign key relationships.\n\n\nExercise 2 (Schema for sports) Pick your favorite sport (or, at least, a sport you know something about). Consider designing a database to track players, teams, and games. Write a schema (as a series of CREATE TABLE statements) to store information about them. Your schema should support queries that:\n\nIdentify which players were on a team at a particular time (as players can move between teams)\nIdentify which team won a particular game, and where that game was played\nFind all the games a particular player was involved in. (You don’t have to track if they sat out particular games, just which games their team played in.)\nFind all games played in a particular place.\n\nYou can support additional query types, but start with a schema that can handle these."
  },
  {
    "objectID": "data-engineering/advanced-sql.html#sec-transactions",
    "href": "data-engineering/advanced-sql.html#sec-transactions",
    "title": "Advanced SQL",
    "section": "",
    "text": "Transactions are the fundamental mechanism relational databases use to manage both atomicity and concurrency. How do we ensure multiple queries run atomically (either all succeed or all fail), and how do we allow multiple people to be changing the database at the same time? To see the problem, consider an example.\n\nExample 2 (The need for transactions) Suppose we’re working with an R data frame, except it is shared: multiple people can edit it at once, and their edits instantaneously change the data frame. (There’s no easy way to do this in base R; we’re just using R syntax to illustrate the problem a SQL database faces.)\nUser 1 and user 2 are both working with the bank_accounts data frame, which stores bank balances and information. User 1 needs to record that account 777 transferred $500 to account 778; user 2 needs to record that account 777 withdrew $50 in cash.\nEach column shows one user’s operations; each line is one time point, so two side-by-side lines occur at the same time:\n\n\n\n# User 1:\n\na777 &lt;- bank_accounts |&gt;\n  filter(account_id = 777)\nstart &lt;- a777$balance\n\na778 &lt;- bank_accounts |&gt;\n  filter(account_id = 778)\n\na777$balance &lt;- start - 500\na778$balance &lt;- a778$balance + 500\n\n\n# User 2:\n\na777 &lt;- bank_accounts |&gt;\n  filter(account_id = 777)\namount &lt;- a777$balance\n\na777$balance &lt;- amount - 50\n\n\n\nSuppose both accounts start with $1000. The desired outcome is that account 777 ends with $450 ($500 sent to 778 and $50 withdrawn) and 778 ends with $1500. But instead:\n\nUser 1 stores $1000 in start.\nUser 2 stores $1000 in amount.\nUser 2 updates account 777’s balance to be amount - 50, or $950.\nUser 1 updates account 777’s balance to be start - 500, or $500.\nUser 1 updates account 778’s balance to be $500 higher, or $1500.\n\nAt the end, 777’s balance is $500, not $450! All because of the timing of the operations. If user 1’s work was guaranteed to happen first, without user 2 interfering, this wouldn’t happen.\n\nPostgreSQL uses Multiversion Concurrency Control, which means that each database user sees a snapshot of the database as it was at some specific point in time. If someone else is changing the database at the same time, this does not affect queries that are currently running, so the data they see remains self-consistent.\nThe basic mechanism is straightforward, though the implementation is complicated. Queries are grouped into transactions using BEGIN and COMMIT statements. The queries within a transaction do not update the data seen by other users until COMMIT “commits” the changes. An error in any query in the transaction halts the entire transaction and “rolls back” its changes, so it is as if it never happened.\n\nExample 3 (Concurrency with transactions) Suppose we’re implementing Example 2 using PostgreSQL and its default concurrency settings. Each user submits a transaction:\n-- User 1:\nBEGIN;\n\nUPDATE bank_accounts\nSET balance = balance - 500::money\nWHERE account_id = 777;\n\nUPDATE bank_accounts\nSET balance = balance + 500::money\nWHERE account_id = 778;\n\nCOMMIT;\n\n-- User 2\nBEGIN;\n\nUPDATE bank_accounts\nSET balance = balance - 50::money\nWHERE account_id = 777;\n\nCOMMIT;\nInternally, PostgreSQL must do the same work as in Example 2—but it prevents any problems from occurring:\n\nUser 1 begins by changing account 777. This locks the row, preventing other changes until user 1’s transaction is complete.\nUser 2 then attempts to change it. The UPDATE query must wait for the lock to be released, so their query is stalled.\nUser 1 commits their transaction with the updated balances, releasing the lock on account 777.\nUser 2’s query can now go ahead, subtracting $50 from the account’s balance.\n\nThis avoids the problem entirely, at the cost of sometimes requiring queries to wait for others to complete.\n\nPostgreSQL’s locking system automatically locks individual rows or entire tables when UPDATE, INSERT, or other commands make it necessary. The locks are released when the transaction is committed, allowing other changes to be made to the same row or table. SELECT does not lock anything, but PostgreSQL does ensure that a SELECT only sees the version of the database as it was when the query began: if a SELECT involves complicated joins and aggregations that take some time to execute, and the database is changed by another user while those happen, the SELECT’s results will not be affected by those changes. This prevents strange inconsistencies where rows change while they’re being sorted and joined.\nTransactions also provide the atomicity required by the ACID guarantees: PostgreSQL ensures that either all queries in a transaction succeed or none of them do.\nFor example, suppose a transaction involves 10 queries doing various updates, but the 6th query causes an error. The entire transaction will be aborted and PostgreSQL will refuse to execute any further queries until you send a ROLLBACK command, which reverts the database back to its state before the transaction started.\nIn Example 3, atomicity is important for user 1: if an error occurs between the two UPDATE queries, we would have subtracted $500 from one account but not added it to the other, and the $500 would be lost. Grouping the queries into a transaction makes this impossible.\nPostgreSQL also supports “savepoints”, which are like checkpoints inside a transaction. (Or quicksaves, if you play video games.) Within one transaction, we can make multiple savepoints, and revert back to a specific changepoint instead of rolling back the entire transaction. Again, the changes only become visible to other users when the entire transaction is committed, but this gives us flexibility in dealing with errors and selectively undoing changes. See the SAVEPOINT reference for examples."
  },
  {
    "objectID": "data-engineering/advanced-sql.html#subqueries",
    "href": "data-engineering/advanced-sql.html#subqueries",
    "title": "Advanced SQL",
    "section": "",
    "text": "SQL allows subqueries, where part of a query is itself defined by another query.\nThe simplest case is in FROM clauses. Normally a FROM clause specifies a table, or multiple tables combined with joins. But we can also select from the table of results of another query. Here’s a trivial example using the tables defined in ?@sec-example-db:\nSELECT * FROM\n(SELECT moment, persona, score\n FROM events) AS e\nINNER JOIN\n(SELECT id, firstname, lastname\n FROM personae) AS p\nON e.persona = p.id;\nThe subqueries are given in parentheses. Note they can’t refer to things outside of the subqueries; essentially, Postgres runs the subqueries first, gets the tables of results, and then uses them like it would use any other table in the outer query.\nThis query, of course, could be written easily without the subequeries. But often in complicated queries with WHERE, GROUP BY, and HAVING clauses, it can be easier to understand when written with subqueries than when written as one large query.\nYou can also use subqueries within a WHERE clause. These are called subquery expressions.\nSELECT moment, persona, score FROM events\nWHERE persona IN (SELECT id FROM personae\n                  WHERE account_balance &lt; 0::money);\nHere the subquery returns exactly one column (id), and so IN checks if each event’s persona is contained in that column. This query could be written equivalently as a join, but sometimes it’s easier to work with subqueries to put together a larger query out of pieces, instead of writing one large query to do everything.\nThere are various other functions/operators that can be used on subqueries as well, such as IN, NOT IN, EXISTS, ANY, ALL, and SOME.\nFor example, this looks like an inner join:\nSELECT moment, persona, score FROM events\nWHERE EXISTS (SELECT 1 FROM personae WHERE id = events.persona);\nHere EXISTS is an operator that looks for there to be at least one row in the subquery. The subquery does refer to the outer tables and variables, so you can think of this subquery as being run once per row in events, to determine if there is a matching persona. If the subquery does not return any rows, that event is not included in the final results.\n\nExercise 3 (Translating subqueries) Translate the following queries to JOIN queries without any subqueries.\n\n   SELECT moment, persona, score FROM events\n   WHERE persona IN (SELECT id FROM personae\n                     WHERE account_balance &lt; 0::money);\n   SELECT moment, persona, score FROM events\n   WHERE EXISTS (SELECT 1 FROM personae\n                 WHERE id = events.persona);\n\n\n\nSolution. These examples don’t really illustrate the power of subqueries, but nevertheless.\nSELECT moment, persona, score\nFROM events\nINNER JOIN personae ON events.persona = personae.id\nWHERE personae.account_balance &lt; 0;\n\nSELECT moment, persona, score\nFROM events\nINNER JOIN personae ON events.persona = personae.id;"
  },
  {
    "objectID": "data-engineering/advanced-sql.html#more-advanced-grouping",
    "href": "data-engineering/advanced-sql.html#more-advanced-grouping",
    "title": "Advanced SQL",
    "section": "",
    "text": "?@sec-grouping-aggregate introduced aggregate functions and the GROUP BY clause, which allowed us to split data into groups and summarize the groups with aggregates. GROUP BY is fairly restrictive, however: we can only split the data into disjoint groups by unique values of a column, or unique combinations of several columns.\nSometimes we want grouping and aggregation over overlapping groups. For example, if I have data on students at CMU, I might want averages by their department, by college, and overall averages for the university. With GROUP BY that requires running three different queries.\nInstead, we can provide grouping sets, where we provide multiple sets of variables to group by and aggregate over. For example:\nSELECT element, persona, AVG(score), COUNT(*)\nFROM events\nGROUP BY GROUPING SETS ((element), (persona), ());\nThis query asks Postgres to group by three different sets of grouping variables: element, persona, and (), which represents no grouping (aggregate all the data). The result looks like this:\n element | persona |         avg          | count\n---------+---------+----------------------+-------\n         |         | 507.6059701492537313 |  1005\n   29409 |         | 335.8750000000000000 |     8\n   29406 |         | 588.1764705882352941 |    17\n   29408 |         | 572.2666666666666667 |    15\n   29395 |         | 623.5454545454545455 |    11\n   29430 |         | 588.0000000000000000 |    10\n[...]\n         |    1162 | 567.5384615384615385 |    13\n         |    1233 | 437.6666666666666667 |     3\n         |    1157 | 333.5000000000000000 |    12\n         |    1247 | 508.4117647058823529 |    17\n         |    1189 | 546.6666666666666667 |     6\n[...]\nThe blanks here represent NULLs. The first row is the overall average score of all events. The next rows are averages for specific elements, followed by averages for specific personas. Rows grouped by element have a NULL for persona and vice versa.\nNotice the SELECT could list both the element and persona columns. If we simply did GROUP BY element, we couldn’t list persona in the SELECT column list because it’s not the grouping column or an aggregate—but here, Postgres is smart enough to understand that it’s in some of the grouping sets, so it will fill it in for those rows and leave it NULL otherwise.\nThere are other shortcuts. Often we do hierarchical aggregates, like the CMU example above. If we had a students table with columns for department and college, we could write\nSELECT department, college, COUNT(*)\nFROM students\nGROUP BY ROLLUP (college, department);\nThis will first group by (college, department), producing one row per department in each college, then group by college, then group by (), producing (hypothetical) results like\n college | department | count\n---------+------------+-------\n         |            | 16335\nDietrich | Statistics |   632\nDietrich |    History |   127\n[...]\nDietrich |            |  2310\n     MCS |            |  6847\n[...]\nFinally, we can group by every combination of a set of variables. GROUP BY CUBE (a, b, c) will group by (a, b, c); then by (a, b), (a, c), and (b, c); then by a, and so on, all the way up to ()."
  },
  {
    "objectID": "data-engineering/advanced-sql.html#sec-sql-window",
    "href": "data-engineering/advanced-sql.html#sec-sql-window",
    "title": "Advanced SQL",
    "section": "",
    "text": "In ?@sec-grouping-aggregate, we saw how GROUP BY and aggregate functions allowed us to calculate aggregate values across many rows. We could, for instance, get the average score per student, or the maximum latency per student, or the earliest event, or…\nIn each of those uses, we were limited to getting one row per group. We had to write our SELECT clauses to ensure each column we requested had one result per group.\nBut sometimes I want to get individual rows of data, with additional information attached about the group. For example, using the events table from ?@sec-example-db, how could we get the score for every individual event, compared to that student’s average score for all events?\nWe could do this with a clever subquery (Exercise 4), but an alternative is a window function. While an aggregate function takes a group and produces a single row for that group, a window function can calculate an aggregate for every row.\nFor example, let’s get each event’s score, and the z-score of that event relative to the student’s average:\nSELECT\n  id, persona, element, score,\n  ((score - avg(score) OVER (PARTITION BY persona)) /\n   stddev_samp(score) OVER (PARTITION BY persona)) AS z_score\nFROM events\nLIMIT 10;\navg() and stddev_samp() are aggregate functions, but OVER turns them into a window function. The OVER clause tells Postgres which rows to calculate the aggregate for. Here we’ve told it to PARTITION BY persona, so each row gets the average and standard deviation calculated from rows with the same persona.\nWindow functions don’t have to operate on the entire partition. For instance, if we specify ORDER BY in the OVER clause, the window is (by default) the first row in the partition up to the current row, but not the following rows. For instance, this query gets the z score relative to all previous events by the user, but not future ones:\nSELECT\n  id, persona, element, score, moment,\n  ((score - avg(score) OVER (PARTITION BY persona ORDER BY moment)) /\n   stddev_samp(score) OVER (PARTITION BY persona ORDER BY moment)) AS z_score\nFROM events\nLIMIT 10;\nInstead of repeating the PARTITION BY clause, we can name a window:\nSELECT\n  id, persona, element, score, moment,\n  ((score - avg(score) OVER p) /\n   stddev_samp(score) OVER p) AS z_score\nFROM events\nWINDOW p AS (PARTITION BY persona ORDER BY moment)\nLIMIT 10;\nNaturally, there are many additional options to customize this; you could have the window be the following rows, exclude the current row (like a studentized z score), be only some of the rows in the partition (by including a FILTER before OVER), or many other complicated variations.\nWindow functions can only be used in the SELECT column list and in ORDER BY, but not in GROUP BY, HAVING, or WHERE clauses, because they’re calculated after these are run.\nThere are also specific window functions that are not aggregate functions. For example, rank() gives the rank of each row within the partition.\nSee the window function tutorial and reference manual page for details.\n\nExercise 4 (Writing window functions with a subquery) Write a query to get the z-score for every event score, relative to the student’s overall average score. Write this using a join and a subquery, rather than by using a window function.\nThe result should have the same columns as the first version in the window function example above.\nNote: Once you write the query, it’s worth thinking about how much more difficult it would be to do the second window function example above, where we included ORDER BY. The simple aggregation approach that worked here would not work for that query.\n\n\nSolution. Here’s one way:\nSELECT e.id, e.persona, e.score, (e.score - z.avg_score) / z.sd_score AS z_score\nFROM events AS e\nLEFT JOIN\n(SELECT persona, avg(score) AS avg_score, stddev_samp(score) AS sd_score\n FROM events\n GROUP BY persona) AS z\nON e.persona = z.persona;\nNotice we’re using a LEFT JOIN so we get one row per event, rather than one row per persona (as in a RIGHT JOIN).\n\n\nExercise 5 (Window functions) Write each of the following queries using window functions.\n\nFor each event, report the difference between the student’s score and the average score of all students who did that element. (element identifies the specific quiz question or instructional item being scored.) Report columns for the student ID, element ID, score, and difference from average.\nFor each event, give the rank of the student’s score out of all students who completed that element, where 1 means they got the highest score. Report columns for the student, element, score, and rank.\nHint: The rank() function, described in table 9.63 of the manual, will be useful here. There are examples in the official tutorial.\n\n\n\nSolution. Here’s one way:\nSELECT persona, element, score,\n       score - avg(score) OVER (PARTITION BY element) AS score_diff\nFROM events;\n\nSELECT persona, element, score,\n       rank() OVER (PARTITION BY element ORDER BY score DESC)\nFROM events;"
  },
  {
    "objectID": "data-engineering/sql-code.html",
    "href": "data-engineering/sql-code.html",
    "title": "Using SQL from Code",
    "section": "",
    "text": "Note\n\n\n\nPortions of these notes are based on material co-developed with Christopher Genovese for 36-750.\n\n\nIt’s nice to be able to type queries into psql and see results, but most often you’d like to do more than that. You’re not just making a database to run handwritten queries – you’re using it to store data for a big project, and that data then needs to be used to fit models, make plots, prepare reports, and all sorts of other useful things. Or perhaps your code is generating data which needs to be stored in a database for later use.\nRegardless, you’d like to run queries inside R, Python, or your preferred programming language, and get the results back in a form that can easily be manipulated and used.\nFortunately, PostgreSQL – and most other SQL database systems – use the client-server model of database access. The database is a server, accessible to any program on the local machine (like the psql client) and even to programs on other machines, if the firewall allows it.\n\n\nPsycopg is a popular PostgreSQL package for Python. Check out the reference guide for full documentation of all its functions and features. It can be installed by using\npip install \"psycopg[binary]\"\nPyscopg is based on two core concepts: connections and cursors. Both are represented by Python objects. A connection object represents your connection to the database server, and allows you to create transactions and commit them. A cursor object represents one particular interaction with that connection: you can submit a query, then use the cursor to fetch its results.\nTo connect:\nimport psycopg\n\nconn = psycopg.connect(\n    host=\"pinniped.postgres.database.azure.com\", dbname=\"yourusername\",\n    user=\"yourusername\", password=\"yourpassword\"\n)\nconn is now a connection object. (See Section 1.2 below on how to store your password securely; including it directly in your Python files is not a good idea.) We can create a cursor and use it to submit (“execute”) a query:\ncur = conn.cursor()\n\nbaz = \"walrus\"\nspam = \"penguin\"\n\ncur.execute(\"INSERT INTO foo (bar, baz, spam) \"\n            \"VALUES (17, %s, %s)\", (baz, spam))\nNotice the use of interpolation to put variables into the query – see Section 1.3 below. Thou shalt not concatenate values directly into the SQL string.\nIf we do a SELECT, we can get the results with a for loop or the fetchone and fetchmany methods:\n# Python concatenates adjacent string literals, which makes it\n# easy to split queries across lines:\ncur.execute(\"SELECT time, persona, element, score \"\n            \"FROM events\")\n\n# iterating over tuples; each tuple is one row:\nfor row in cur:\n    # Destructuring lets us assign each tuple entry to a variable\n    time, persona, element, score = row\n\n    # do something with the results here\n\n# alternately, one at a time:\nrow = cur.fetchone()\nThe execute method is used regardless of the type of query.\nIf you use pandas for your data frames, you can also convert the results of any query directly into a pandas data frame:\nimport pandas as pd\n\nd = pd.read_sql_query(\"SELECT persona, element FROM events WHERE id = %(id)s\",\n                      conn, params = {'id': 17})\nThe psycopg3 package automatically starts a SQL transaction (?@sec-transactions) when you create a connection with psycopg.connect(). You must explicitly commit the transaction if you are inserting data:\nfor event in events:\n    cur.execute(\"INSERT INTO events (...)\")\n\nconn.commit() # commit transaction\nconn.close() # close connection\nIf you do not explicitly commit the transaction, it will be rolled back when your script exits.\n\n\n\nThe code above stores your password right in the source file. This is a bad idea. If the code is ever shared with anyone, posted online, or otherwise revealed, anyone who sees it now has your database username and password and can view or modify any of your data. If you commit the file to Git, your password is now in your Git history forever. Fortunately, there are ways to work around this.\nA simple approach is to create a file called credentials.py that looks like this:\nDB_USER = \"yourusername\"\nDB_PASSWORD = \"yourpassword\"\nThen, in other files, you can use import credentials and use credentials.DB_PASSWORD instead of writing in your password. Add credentials.py to your .gitignore to avoid accidentally committing it.\nLarge companies often use systems for managing passwords and tokens required by scripts, such as Vault or cloud-specific systems like Azure Key Vault and the AWS Secrets Manager. These are much better than creating hidden files.\n\n\n\nSuppose you’ve loaded some data from an external source – a CSV file, input from a user, from a website, another database, wherever. You need to use some of this data to do a SQL query.\ncur.execute(\"SELECT * FROM users WHERE username = '\" + username + \"' \" +\n            \"AND password = '\" + password + \"'\"))\nNow suppose username is the string '; DROP TABLE users;--. What does the query look like before we send it to Postgres?\nSELECT * FROM users\nWHERE username = ''; DROP TABLE users; -- AND password = 'theirpassword'\nWe have injected a new SQL statement, which drops the table. Because -- represents a comment in SQL, the commands following are not executed.\n\n\n\nxkcd #327\n\n\nLess maliciously, the username might contain a single quote, confusing Postgres about where the string ends and causing syntax errors. Or any number of other weird characters which mess up the query. Clever attackers can use SQL injection to do all kinds of things: imagine if the password variable were foo' OR 1=1 – we’d be able to log in without knowing the right password!\n(For a time in the early 2000s, SQL injection was incredibly common, and numerous services were hacked using it. It still crops up when unwary developers don’t think about their queries.)\nWe need a better way of writing queries with parameters determined by the code. Fortunately, database systems provide parametrized queries, where the database software is explicitly told “this is an input, with this value” so it knows not to treat it as SQL syntax. For example:\nusername = \"'; DROP TABLE users;--\"\npassword = \"walruses\"\n\ncur.execute(\"SELECT * FROM users \"\n            \"WHERE username = %(user)s AND password = %(pass)s\",\n            {\"user\": username, \"pass\": password})\nIn this form, psycopg sends the query (with placeholders) and the values separately, so that the SQL server knows which parts are values and which parts are part of the SQL syntax. It is impossible for anyone to maliciously slip different SQL syntax into your queries.\nConsult the psycopg documentation on query parameters for more details and options.\nYou should always use this approach to insert data into SQL queries. You may think it’s safe with your data, but as the documentation notes:\n\n\nDon’t manually merge values to a query: hackers from a foreign country will break into your computer and steal not only your disks, but also your cds, leaving you only with the three most embarrassing records you ever bought. On cassette tapes.\nIf you use the % operator [or f-strings] to merge values to a query, con artists will seduce your cat, who will run away taking your credit card and your sunglasses with them.\nIf you use + to merge a textual value to a string, bad guys in balaclava will find their way to your fridge, drink all your beer, and leave your toilet seat up and your toilet paper in the wrong orientation.\n\n\nYou may find older code that does not use parametrized queries, but instead concatenates queries together with escapes. In short, before concatenating values into a query, you check that their types match what they should be (e.g. integer values really are integers), and for strings, you replace ' with '', the escape for representing ' within a string. Then you concatenate.\npsycopg doesn’t actually contain a function for escaping strings, because it’s too easy to mess up and forget to escape something, or to escape something incorrectly—just use a parametrized query, which cannot go wrong. But you may find escaping commonly used in other languages.1 If you’ve ever seen text with apostrophes come out of a website with extra backslashes, you’ve seen a poorly done escaping system (as other SQL databases use \\' to escape single quotes).\n\nExercise 1 (SQL injection) Suppose we have a users table with username, email, and admin columns. The admin column is a boolean (true/false) that determines whether the user has administrative privileges in the application.\nThe application includes a form to update your email address. When you submit the form, it does the following:\ncur.execute(\"UPDATE users \"\n            \"SET email = '\" + email_address \"' \"\n            \"WHERE username = '\" + username + \"'\")\nSuggest an email address and username to submit so that your user account will be given administrative privileges.\n\n\nSolution. The key is your email address. Set it to something like\n', admin = true, email = 'user@example.com\nThis sets the email column twice, but ensures the query still has valid syntax. You could also do, say,\nuser@example.com', admin = TRUE, username = 'yourusername\nas well. Notice we’re providing a value so the final closing ' in the original query is not a syntax error.\n\n\n\n\nQuery errors are converted into Python exceptions. For example, here I’m trying to insert a country that’s already in the countries table, and the country code is defined to be a primary key, so it must be unique:\nIn [7]: cur.execute(\"INSERT INTO countries (country_code, country_name) VALUES ('us', 'Estados Unidos')\")\n---------------------------------------------------------------------------\nUniqueViolation                           Traceback (most recent call last)\n&lt;ipython-input-7-5df5a6b218e6&gt; in &lt;module&gt;\n----&gt; 1 cur.execute(\"INSERT INTO countries (country_code, country_name) VALUES ('us', 'Estados Unidos')\");\n\n~/miniconda3/lib/python3.6/site-packages/psycopg/cursor.py in execute(self, query, params, prepare, binary)\n    546                 )\n    547         except e.Error as ex:\n--&gt; 548             raise ex.with_traceback(None)\n    549         return self\n    550\n\nUniqueViolation: duplicate key value violates unique constraint \"countries_pkey\"\nDETAIL:  Key (country_code)=(us) already exists.\nYou can use Python’s exception management features (try, except, finally, etc.) with these exceptions just like with any others:\ntry:\n    cur.execute(\"INSERT INTO countries (country_code, country_name) \"\n                \"VALUES ('us', 'Estados Unidos')\")\nexcept psycopg.errors.UniqueViolation as e:\n    # do something with the exception here\nSee the Python errors and exceptions tutorial for more information.\nNotice that the exception is a UniqueViolation. Psycopg translates Postgres error codes into distinct Python exceptions, so if you use except to catch them, you can catch different types of Postgres errors and handle them differently. The psycopg.errors module defines the exception types that are available.\nIf an error occurs in any query in your transaction, any subsequent queries will produce errors like:\nInFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block\nThis means you need to call conn.rollback() to roll back the transaction explicitly so you can start over. Or you can use the transaction features described below.\n\nExercise 2 (Handling SQL errors) Load the events data into your own database so that you have permission to edit it. (You can open the SQL file in Azure Data Studio to run it in your database, creating the table.)\nWrite a short Python script using psycopg to connect to the database. Have the script issue an INSERT query that causes a foreign key constraint violation. Use try and except to catch the exception and print a message about the error.\nYour script should only catch foreign key constraint violations, and not, say, SQL syntax errors or other unrelated problems.\n\n\nSolution. The psycopg exception for foreign keys is psycopg.errors.ForeignKeyViolation, so there should be a try/except block like this:\ntry:\n    cur.execute(\"bad query\")\nexcept psycopg.errors.ForeignKeyViolation as e:\n    print(\"some message\")\n\n\n\n\nPsycopg provides convenient ways to manage transactions (?@sec-transactions). (See the psycopg manual chapter for full details.)\nThe most convenient method is with transaction contexts. Python provides a feature called context managers that allow actions to be automatically run to create and destroy resources, and psycopg reuses these to create and commit (or rollback) transactions. Context managers are created with the with statement.\nFor example:\nconn = psycopg.connect(...)\ncur = conn.cursor()\n\nwith conn.transaction():\n    # This creates a new transaction\n\n    cur.execute(\"INSERT INTO foo ...\")\n    cur.execute(\"INSERT INTO bar ...\")\n\n# Now the block is done and committed. But if there was an exception raised in\n# the block that was not caught, the context manager would automatically call\n# conn.rollback() for us.\nWhat’s great is that these can be nested! For instance, suppose you’re loading lots of data by looping through a data file doing INSERT. You want the entire data to be loaded or not loaded; you don’t want it to crash halfway through and only half the data is loaded. But you also want to recover from bad rows and keep inserting the rest. You could do the following:\nnum_rows_inserted = 0\n\n# make a new transaction\nwith conn.transaction():\n    for row in big_data_set:\n        try:\n            # make a new SAVEPOINT -- like a save in a video game\n            with conn.transaction():\n                # perhaps a bunch of reformatting and data manipulation goes here\n\n                # now insert the data\n                cur.execute(\"INSERT INTO foo ...\", ...)\n        except Exception as e:\n            # if an exception/error happens in this block, Postgres goes back to\n            # the last savepoint upon exiting the `with` block\n            print(\"insert failed\")\n            # add additional logging, error handling here\n        else:\n            # no exception happened, so we continue without reverting the savepoint\n            num_rows_inserted += 1\n\n# now we commit the entire transaction\nconn.commit()\nThis lets us handle errors in individual rows without stopping or losing all the data we have inserted, but still ensures that other users see all or none of the data, not part of it.\n\n\n\nWhen the database is on a different machine than your Python script, the delay in sending messages between them can become a problem. Each time you use cur.execute() to run a query, your Python script must send several messages to Postgres with the query and the parametrized data; then it must wait for PostgreSQL to send back the results and indicate it’s ready for the next query.\nFrom the CMU network to our Azure database server, I measured a 25-millisecond round trip. That means it takes 25 milliseconds to send a message and get a response back, even when the response is sent instantly. The round-trip time to a server in the UK was closer to 100 milliseconds; if you’re on crappier Internet than the CMU connection, you can easily have 200-millisecond round trips to some servers.\nThat means every query to our Azure database must take at least 25 milliseconds, plus whatever time it takes Postgres to do the actual work. That imposes a limit of 40 queries per second—lower if your Internet is slower or if the queries take Postgres some time to execute.\nThis is a problem if we’re executing large batches of queries, e.g. if we’re sending many INSERT commands to load lots of data. Being limited to 40 rows per second when you have thousands of rows is a big problem.\nFortunately there is an alternative. PostgreSQL now supports pipeline mode. In this mode, you can send a stream of many queries to Postgres without waiting for their results; Postgres will send the results back to you as they become available. You could send a batch of 100 without having to wait \\(100 \\times 25\\) milliseconds (2.5 seconds!), and only wait briefly at the end to receive the final confirmation that they’re done.\nPipeline mode can be invoked manually (see the manual page), but it can also be done with the executemany() method. For example:\ncur.executemany(\"INSERT INTO tablename (foo, bar, baz) VALUES (%s, %s, %s)\",\n                [(\"foo for row 1\", 9, \"baz1\"),\n                 (\"foo for row 2\", -9, \"baz2\"),\n                 (\"foo for row 3\", 16, \"ducks\")])\nAs of psycopg version 3.1, this automatically sends the batch of rows in pipeline mode. Each entry in the list is a tuple of values defining one row; the query is hence sent three times.\nThis can be used for INSERT queries or any other query where you want to run the same query many times with different values.\n\n\n\nThere are a few common use-cases for using SQL within Python code (or other languages):\n\nSupporting an interactive application, like a web site or an app. The program responds to actions by the user by performing SQL queries and displaying the results. Facebook, for instance, originally stored all its posts and comments in a SQL database, so viewing the website or app involves running many SQL queries to fetch content to display.\nAutomating tasks that are done occasionally. For example, I might occasionally have to set up accounts for MADS students on a database server, so I could write a Python script that loops over the course roster and runs the necessary commands for each student.\nAutomating workflows that run on a schedule, such as a data pipeline that gets the latest data from several sources, processes it, and loads it into a database.\n\nThe integrity and error-handling requirements for these cases can vary, and so part of your design process is to consider how to handle problems. For example:\n\nIn an interactive website, recovering from errors may not be important; if the system isn’t working, you can check your Facebook feed later. Simply catch exceptions and display an error message to the user. It may also be helpful to log detailed information about the error to a file so that the software developer can debug it later.\nIn a manually run task, like loading new users, it may be best to present the error to the user, let them fix any issues, and then let them start over. That means not partially loading data or leaving the task half-done, because then redoing it is much harder.\nBut a scheduled workflow may be used for many other tasks later: your data pipeline loads data that an automated reporting system uses, and its reports get sent to another system for analysis, and so on. If your job fails, the downstream tasks can’t run at all. If there’s a problem loading one or two rows of data, you may want to load the rest so the other jobs can run on schedule – but record which data failed to load so an analyst can examine it, fix the problem, and manually add it later. But if the integrity of the later analyses depends on loading everything perfectly – as in an accounting system – you may have to catch the error, roll back, and alert someone to investigate.\n\nIn each case, the decision depends on the task requirements. In some tasks, the integrity of data is key, and any error indicates a serious problem that must be fixed before the database can be updated; in others, minor issues can be bypassed for later investigation.\n\n\n\n\nExercise 3 (Automated reporting) Write a Python script called event-report.py. This script should take a command-line argument specifying a date (such as 2015-03-23), so it can be invoked like this:\npython event-report.py 2015-03-23\nWhen it runs, it should connect to the Postgres database and query the events table for all events in the week preceding the given date. It should produce a table of the top 10 students that week, ranked by their cumulative scores on all events during that week. The table should include their name, birthday, score from that week, rank, and overall average score across all weeks.\nPrint this table out as a nicely formatted table. The tabulate package can help format it, or you can use the pandas to_string() method if you have a pandas data frame.\nOnce your code works, paste it into a notebook cell to submit with your homework. Include a text cell with the formatted output from your script, run with 2015-03-23 as the date.\n\n\nExercise 4 (NBA teams) The nba-teams.txt data file contains information about each team in the NBA.\nWrite a table schema to store this data. Provide the CREATE TABLE command with your submission. Include a primary key and appropriate NOT NULL constraints. Consider using enumerated types to represent columns that can only take on one of several fixed values (i.e. like a factor in R).\nWrite Python code to read the data from nba-teams.txt and run a series of INSERT commands to populate the table. The code should loop through the entire file and repeatedly run INSERT. When it’s done, it should print out a summary of how many rows were successfully inserted.\nBe sure to commit the transaction so the data is saved to the database."
  },
  {
    "objectID": "data-engineering/sql-code.html#sql-in-python",
    "href": "data-engineering/sql-code.html#sql-in-python",
    "title": "Using SQL from Code",
    "section": "",
    "text": "Psycopg is a popular PostgreSQL package for Python. Check out the reference guide for full documentation of all its functions and features. It can be installed by using\npip install \"psycopg[binary]\"\nPyscopg is based on two core concepts: connections and cursors. Both are represented by Python objects. A connection object represents your connection to the database server, and allows you to create transactions and commit them. A cursor object represents one particular interaction with that connection: you can submit a query, then use the cursor to fetch its results.\nTo connect:\nimport psycopg\n\nconn = psycopg.connect(\n    host=\"pinniped.postgres.database.azure.com\", dbname=\"yourusername\",\n    user=\"yourusername\", password=\"yourpassword\"\n)\nconn is now a connection object. (See Section 1.2 below on how to store your password securely; including it directly in your Python files is not a good idea.) We can create a cursor and use it to submit (“execute”) a query:\ncur = conn.cursor()\n\nbaz = \"walrus\"\nspam = \"penguin\"\n\ncur.execute(\"INSERT INTO foo (bar, baz, spam) \"\n            \"VALUES (17, %s, %s)\", (baz, spam))\nNotice the use of interpolation to put variables into the query – see Section 1.3 below. Thou shalt not concatenate values directly into the SQL string.\nIf we do a SELECT, we can get the results with a for loop or the fetchone and fetchmany methods:\n# Python concatenates adjacent string literals, which makes it\n# easy to split queries across lines:\ncur.execute(\"SELECT time, persona, element, score \"\n            \"FROM events\")\n\n# iterating over tuples; each tuple is one row:\nfor row in cur:\n    # Destructuring lets us assign each tuple entry to a variable\n    time, persona, element, score = row\n\n    # do something with the results here\n\n# alternately, one at a time:\nrow = cur.fetchone()\nThe execute method is used regardless of the type of query.\nIf you use pandas for your data frames, you can also convert the results of any query directly into a pandas data frame:\nimport pandas as pd\n\nd = pd.read_sql_query(\"SELECT persona, element FROM events WHERE id = %(id)s\",\n                      conn, params = {'id': 17})\nThe psycopg3 package automatically starts a SQL transaction (?@sec-transactions) when you create a connection with psycopg.connect(). You must explicitly commit the transaction if you are inserting data:\nfor event in events:\n    cur.execute(\"INSERT INTO events (...)\")\n\nconn.commit() # commit transaction\nconn.close() # close connection\nIf you do not explicitly commit the transaction, it will be rolled back when your script exits."
  },
  {
    "objectID": "data-engineering/sql-code.html#sec-storing-sql-password",
    "href": "data-engineering/sql-code.html#sec-storing-sql-password",
    "title": "Using SQL from Code",
    "section": "",
    "text": "The code above stores your password right in the source file. This is a bad idea. If the code is ever shared with anyone, posted online, or otherwise revealed, anyone who sees it now has your database username and password and can view or modify any of your data. If you commit the file to Git, your password is now in your Git history forever. Fortunately, there are ways to work around this.\nA simple approach is to create a file called credentials.py that looks like this:\nDB_USER = \"yourusername\"\nDB_PASSWORD = \"yourpassword\"\nThen, in other files, you can use import credentials and use credentials.DB_PASSWORD instead of writing in your password. Add credentials.py to your .gitignore to avoid accidentally committing it.\nLarge companies often use systems for managing passwords and tokens required by scripts, such as Vault or cloud-specific systems like Azure Key Vault and the AWS Secrets Manager. These are much better than creating hidden files."
  },
  {
    "objectID": "data-engineering/sql-code.html#sec-safe-sql",
    "href": "data-engineering/sql-code.html#sec-safe-sql",
    "title": "Using SQL from Code",
    "section": "",
    "text": "Suppose you’ve loaded some data from an external source – a CSV file, input from a user, from a website, another database, wherever. You need to use some of this data to do a SQL query.\ncur.execute(\"SELECT * FROM users WHERE username = '\" + username + \"' \" +\n            \"AND password = '\" + password + \"'\"))\nNow suppose username is the string '; DROP TABLE users;--. What does the query look like before we send it to Postgres?\nSELECT * FROM users\nWHERE username = ''; DROP TABLE users; -- AND password = 'theirpassword'\nWe have injected a new SQL statement, which drops the table. Because -- represents a comment in SQL, the commands following are not executed.\n\n\n\nxkcd #327\n\n\nLess maliciously, the username might contain a single quote, confusing Postgres about where the string ends and causing syntax errors. Or any number of other weird characters which mess up the query. Clever attackers can use SQL injection to do all kinds of things: imagine if the password variable were foo' OR 1=1 – we’d be able to log in without knowing the right password!\n(For a time in the early 2000s, SQL injection was incredibly common, and numerous services were hacked using it. It still crops up when unwary developers don’t think about their queries.)\nWe need a better way of writing queries with parameters determined by the code. Fortunately, database systems provide parametrized queries, where the database software is explicitly told “this is an input, with this value” so it knows not to treat it as SQL syntax. For example:\nusername = \"'; DROP TABLE users;--\"\npassword = \"walruses\"\n\ncur.execute(\"SELECT * FROM users \"\n            \"WHERE username = %(user)s AND password = %(pass)s\",\n            {\"user\": username, \"pass\": password})\nIn this form, psycopg sends the query (with placeholders) and the values separately, so that the SQL server knows which parts are values and which parts are part of the SQL syntax. It is impossible for anyone to maliciously slip different SQL syntax into your queries.\nConsult the psycopg documentation on query parameters for more details and options.\nYou should always use this approach to insert data into SQL queries. You may think it’s safe with your data, but as the documentation notes:\n\n\nDon’t manually merge values to a query: hackers from a foreign country will break into your computer and steal not only your disks, but also your cds, leaving you only with the three most embarrassing records you ever bought. On cassette tapes.\nIf you use the % operator [or f-strings] to merge values to a query, con artists will seduce your cat, who will run away taking your credit card and your sunglasses with them.\nIf you use + to merge a textual value to a string, bad guys in balaclava will find their way to your fridge, drink all your beer, and leave your toilet seat up and your toilet paper in the wrong orientation.\n\n\nYou may find older code that does not use parametrized queries, but instead concatenates queries together with escapes. In short, before concatenating values into a query, you check that their types match what they should be (e.g. integer values really are integers), and for strings, you replace ' with '', the escape for representing ' within a string. Then you concatenate.\npsycopg doesn’t actually contain a function for escaping strings, because it’s too easy to mess up and forget to escape something, or to escape something incorrectly—just use a parametrized query, which cannot go wrong. But you may find escaping commonly used in other languages.1 If you’ve ever seen text with apostrophes come out of a website with extra backslashes, you’ve seen a poorly done escaping system (as other SQL databases use \\' to escape single quotes).\n\nExercise 1 (SQL injection) Suppose we have a users table with username, email, and admin columns. The admin column is a boolean (true/false) that determines whether the user has administrative privileges in the application.\nThe application includes a form to update your email address. When you submit the form, it does the following:\ncur.execute(\"UPDATE users \"\n            \"SET email = '\" + email_address \"' \"\n            \"WHERE username = '\" + username + \"'\")\nSuggest an email address and username to submit so that your user account will be given administrative privileges.\n\n\nSolution. The key is your email address. Set it to something like\n', admin = true, email = 'user@example.com\nThis sets the email column twice, but ensures the query still has valid syntax. You could also do, say,\nuser@example.com', admin = TRUE, username = 'yourusername\nas well. Notice we’re providing a value so the final closing ' in the original query is not a syntax error."
  },
  {
    "objectID": "data-engineering/sql-code.html#handling-errors",
    "href": "data-engineering/sql-code.html#handling-errors",
    "title": "Using SQL from Code",
    "section": "",
    "text": "Query errors are converted into Python exceptions. For example, here I’m trying to insert a country that’s already in the countries table, and the country code is defined to be a primary key, so it must be unique:\nIn [7]: cur.execute(\"INSERT INTO countries (country_code, country_name) VALUES ('us', 'Estados Unidos')\")\n---------------------------------------------------------------------------\nUniqueViolation                           Traceback (most recent call last)\n&lt;ipython-input-7-5df5a6b218e6&gt; in &lt;module&gt;\n----&gt; 1 cur.execute(\"INSERT INTO countries (country_code, country_name) VALUES ('us', 'Estados Unidos')\");\n\n~/miniconda3/lib/python3.6/site-packages/psycopg/cursor.py in execute(self, query, params, prepare, binary)\n    546                 )\n    547         except e.Error as ex:\n--&gt; 548             raise ex.with_traceback(None)\n    549         return self\n    550\n\nUniqueViolation: duplicate key value violates unique constraint \"countries_pkey\"\nDETAIL:  Key (country_code)=(us) already exists.\nYou can use Python’s exception management features (try, except, finally, etc.) with these exceptions just like with any others:\ntry:\n    cur.execute(\"INSERT INTO countries (country_code, country_name) \"\n                \"VALUES ('us', 'Estados Unidos')\")\nexcept psycopg.errors.UniqueViolation as e:\n    # do something with the exception here\nSee the Python errors and exceptions tutorial for more information.\nNotice that the exception is a UniqueViolation. Psycopg translates Postgres error codes into distinct Python exceptions, so if you use except to catch them, you can catch different types of Postgres errors and handle them differently. The psycopg.errors module defines the exception types that are available.\nIf an error occurs in any query in your transaction, any subsequent queries will produce errors like:\nInFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block\nThis means you need to call conn.rollback() to roll back the transaction explicitly so you can start over. Or you can use the transaction features described below.\n\nExercise 2 (Handling SQL errors) Load the events data into your own database so that you have permission to edit it. (You can open the SQL file in Azure Data Studio to run it in your database, creating the table.)\nWrite a short Python script using psycopg to connect to the database. Have the script issue an INSERT query that causes a foreign key constraint violation. Use try and except to catch the exception and print a message about the error.\nYour script should only catch foreign key constraint violations, and not, say, SQL syntax errors or other unrelated problems.\n\n\nSolution. The psycopg exception for foreign keys is psycopg.errors.ForeignKeyViolation, so there should be a try/except block like this:\ntry:\n    cur.execute(\"bad query\")\nexcept psycopg.errors.ForeignKeyViolation as e:\n    print(\"some message\")"
  },
  {
    "objectID": "data-engineering/sql-code.html#sec-transactions-python",
    "href": "data-engineering/sql-code.html#sec-transactions-python",
    "title": "Using SQL from Code",
    "section": "",
    "text": "Psycopg provides convenient ways to manage transactions (?@sec-transactions). (See the psycopg manual chapter for full details.)\nThe most convenient method is with transaction contexts. Python provides a feature called context managers that allow actions to be automatically run to create and destroy resources, and psycopg reuses these to create and commit (or rollback) transactions. Context managers are created with the with statement.\nFor example:\nconn = psycopg.connect(...)\ncur = conn.cursor()\n\nwith conn.transaction():\n    # This creates a new transaction\n\n    cur.execute(\"INSERT INTO foo ...\")\n    cur.execute(\"INSERT INTO bar ...\")\n\n# Now the block is done and committed. But if there was an exception raised in\n# the block that was not caught, the context manager would automatically call\n# conn.rollback() for us.\nWhat’s great is that these can be nested! For instance, suppose you’re loading lots of data by looping through a data file doing INSERT. You want the entire data to be loaded or not loaded; you don’t want it to crash halfway through and only half the data is loaded. But you also want to recover from bad rows and keep inserting the rest. You could do the following:\nnum_rows_inserted = 0\n\n# make a new transaction\nwith conn.transaction():\n    for row in big_data_set:\n        try:\n            # make a new SAVEPOINT -- like a save in a video game\n            with conn.transaction():\n                # perhaps a bunch of reformatting and data manipulation goes here\n\n                # now insert the data\n                cur.execute(\"INSERT INTO foo ...\", ...)\n        except Exception as e:\n            # if an exception/error happens in this block, Postgres goes back to\n            # the last savepoint upon exiting the `with` block\n            print(\"insert failed\")\n            # add additional logging, error handling here\n        else:\n            # no exception happened, so we continue without reverting the savepoint\n            num_rows_inserted += 1\n\n# now we commit the entire transaction\nconn.commit()\nThis lets us handle errors in individual rows without stopping or losing all the data we have inserted, but still ensures that other users see all or none of the data, not part of it."
  },
  {
    "objectID": "data-engineering/sql-code.html#batching-many-queries",
    "href": "data-engineering/sql-code.html#batching-many-queries",
    "title": "Using SQL from Code",
    "section": "",
    "text": "When the database is on a different machine than your Python script, the delay in sending messages between them can become a problem. Each time you use cur.execute() to run a query, your Python script must send several messages to Postgres with the query and the parametrized data; then it must wait for PostgreSQL to send back the results and indicate it’s ready for the next query.\nFrom the CMU network to our Azure database server, I measured a 25-millisecond round trip. That means it takes 25 milliseconds to send a message and get a response back, even when the response is sent instantly. The round-trip time to a server in the UK was closer to 100 milliseconds; if you’re on crappier Internet than the CMU connection, you can easily have 200-millisecond round trips to some servers.\nThat means every query to our Azure database must take at least 25 milliseconds, plus whatever time it takes Postgres to do the actual work. That imposes a limit of 40 queries per second—lower if your Internet is slower or if the queries take Postgres some time to execute.\nThis is a problem if we’re executing large batches of queries, e.g. if we’re sending many INSERT commands to load lots of data. Being limited to 40 rows per second when you have thousands of rows is a big problem.\nFortunately there is an alternative. PostgreSQL now supports pipeline mode. In this mode, you can send a stream of many queries to Postgres without waiting for their results; Postgres will send the results back to you as they become available. You could send a batch of 100 without having to wait \\(100 \\times 25\\) milliseconds (2.5 seconds!), and only wait briefly at the end to receive the final confirmation that they’re done.\nPipeline mode can be invoked manually (see the manual page), but it can also be done with the executemany() method. For example:\ncur.executemany(\"INSERT INTO tablename (foo, bar, baz) VALUES (%s, %s, %s)\",\n                [(\"foo for row 1\", 9, \"baz1\"),\n                 (\"foo for row 2\", -9, \"baz2\"),\n                 (\"foo for row 3\", 16, \"ducks\")])\nAs of psycopg version 3.1, this automatically sends the batch of rows in pipeline mode. Each entry in the list is a tuple of values defining one row; the query is hence sent three times.\nThis can be used for INSERT queries or any other query where you want to run the same query many times with different values."
  },
  {
    "objectID": "data-engineering/sql-code.html#building-automated-workflows",
    "href": "data-engineering/sql-code.html#building-automated-workflows",
    "title": "Using SQL from Code",
    "section": "",
    "text": "There are a few common use-cases for using SQL within Python code (or other languages):\n\nSupporting an interactive application, like a web site or an app. The program responds to actions by the user by performing SQL queries and displaying the results. Facebook, for instance, originally stored all its posts and comments in a SQL database, so viewing the website or app involves running many SQL queries to fetch content to display.\nAutomating tasks that are done occasionally. For example, I might occasionally have to set up accounts for MADS students on a database server, so I could write a Python script that loops over the course roster and runs the necessary commands for each student.\nAutomating workflows that run on a schedule, such as a data pipeline that gets the latest data from several sources, processes it, and loads it into a database.\n\nThe integrity and error-handling requirements for these cases can vary, and so part of your design process is to consider how to handle problems. For example:\n\nIn an interactive website, recovering from errors may not be important; if the system isn’t working, you can check your Facebook feed later. Simply catch exceptions and display an error message to the user. It may also be helpful to log detailed information about the error to a file so that the software developer can debug it later.\nIn a manually run task, like loading new users, it may be best to present the error to the user, let them fix any issues, and then let them start over. That means not partially loading data or leaving the task half-done, because then redoing it is much harder.\nBut a scheduled workflow may be used for many other tasks later: your data pipeline loads data that an automated reporting system uses, and its reports get sent to another system for analysis, and so on. If your job fails, the downstream tasks can’t run at all. If there’s a problem loading one or two rows of data, you may want to load the rest so the other jobs can run on schedule – but record which data failed to load so an analyst can examine it, fix the problem, and manually add it later. But if the integrity of the later analyses depends on loading everything perfectly – as in an accounting system – you may have to catch the error, roll back, and alert someone to investigate.\n\nIn each case, the decision depends on the task requirements. In some tasks, the integrity of data is key, and any error indicates a serious problem that must be fixed before the database can be updated; in others, minor issues can be bypassed for later investigation."
  },
  {
    "objectID": "data-engineering/sql-code.html#exercises",
    "href": "data-engineering/sql-code.html#exercises",
    "title": "Using SQL from Code",
    "section": "",
    "text": "Exercise 3 (Automated reporting) Write a Python script called event-report.py. This script should take a command-line argument specifying a date (such as 2015-03-23), so it can be invoked like this:\npython event-report.py 2015-03-23\nWhen it runs, it should connect to the Postgres database and query the events table for all events in the week preceding the given date. It should produce a table of the top 10 students that week, ranked by their cumulative scores on all events during that week. The table should include their name, birthday, score from that week, rank, and overall average score across all weeks.\nPrint this table out as a nicely formatted table. The tabulate package can help format it, or you can use the pandas to_string() method if you have a pandas data frame.\nOnce your code works, paste it into a notebook cell to submit with your homework. Include a text cell with the formatted output from your script, run with 2015-03-23 as the date.\n\n\nExercise 4 (NBA teams) The nba-teams.txt data file contains information about each team in the NBA.\nWrite a table schema to store this data. Provide the CREATE TABLE command with your submission. Include a primary key and appropriate NOT NULL constraints. Consider using enumerated types to represent columns that can only take on one of several fixed values (i.e. like a factor in R).\nWrite Python code to read the data from nba-teams.txt and run a series of INSERT commands to populate the table. The code should loop through the entire file and repeatedly run INSERT. When it’s done, it should print out a summary of how many rows were successfully inserted.\nBe sure to commit the transaction so the data is saved to the database."
  },
  {
    "objectID": "data-engineering/sql-code.html#footnotes",
    "href": "data-engineering/sql-code.html#footnotes",
    "title": "Using SQL from Code",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example, PHP used to have mysql_escape_string() to escape strings being sent to MySQL. Except it didn’t keep track of which character encoding was being used, so it could escape strings incorrectly. It was replaced with mysql_real_escape_string(), and eventually by parametrized queries.↩︎"
  },
  {
    "objectID": "data-engineering/database-fundamentals.html",
    "href": "data-engineering/database-fundamentals.html",
    "title": "Database Fundamentals",
    "section": "",
    "text": "Note\n\n\n\nPortions of these notes are based on material co-developed with Christopher Genovese for 36-750.\n\n\nIn statistics and data science classes, you often work with data provided to you as files. Your homework or project requires you to analyze some data, so the professor gives you a CSV file to download and open in R or Python. This is also standard practice in scientific data analysis: a scientist collects some data and sends you a CSV or spreadsheet to analyze.\nBut in businesses, data is constantly being updated and changed. You might be asked to do things like:\n\nBuild a system to analyze sales records and produce reports every quarter.\nDevelop a model that identifies fraudulent credit card transactions so they can be rejected.\nAnalyze supply and demand data to automatically tweak product pricing.\n\nAll of these would involve building a system that runs constantly and gets updated with the latest data. That data may come from many sources at once: in a large company, you might have sales data from in-person stores, billing data from the online store, analytics data tracking visitors to the website, marketing data from ad campaigns, product inventory data from the company logistics system, visitor data from a company that uses cell phones to track how many people visit your stores, and many other things besides.\nWorse, all of that data comes from systems run by entirely different parts of the company, all using different software sold by different companies. And all of that data is being constantly changed by many people at the same time. Using a CSV file would be impossible: how do you let hundreds of people work on the same CSV at once, ensuring their work is always using the latest version of the file even as other people edit it?\nThis is why we use databases.\n\n\nA database is a system for organizing information so it can be efficiently accessed, updated, and managed, even when the data is large and the updates are frequent. Relational databases (RDBs) have been the most common type of database since the 1970s and are still very important. They are organized around data tables with fixed columns and rows for each record or observation.\nRelational databases tend to be design-first systems. First, you specify a schema for your data—giving the tables, the columns they contain, and the types of data stored in each column—and then you enter data that conforms to that schema. A properly designed schema can provide very flexible and powerful queries.\nOnce you have a schema and data, RDBs make it easy to select, filter, and order data, and are particularly suited for combining data from multiple tables—their explicit modeling of the relationships between tables is the source of their name. This is commonly done using a (mostly) standardized language called SQL, which stands for Structured Query Language. Requests written in SQL are called queries.\nMany of the fundamental relational query operations will be familiar to you, since they are supported in many packages for working with data. For example, in R, the dplyr package provides a set of functions for querying, modifying, and joining data frames, and these are very similar to the operations supported by relational databases.\n\n\nThe basic unit of data storage in an RDB is the table. Tables are also sometimes called relations, schemas, and entities in an RDB context. There may be many tables in a single database, so a database is a collection of related tables. A table is defined by its attributes, or columns, each of which has a name and a type.\nYou can think of tables as being much like data frames in R or Python, since each row defines a mapping from attribute names (the columns) to values (stored in that column). Unlike data frames in R or Python, the rows do not have an order; there is no notion of row indices unless you create a column for that purpose.\nAlso unlike data frames in R or Python, tables in relational databases are designed in advance: the columns and their types are specified first, before creating data. While it is possible to modify a table, one does not add and delete columns from tables nearly as often as you do in R or Python.\n\n\n\nThe type of a piece of data describes the set of possible values that data can have and the operations that can apply to it.\nIn an RDB, we specify the type of each column in advance. PostgreSQL, for instance, supports a wide variety of data types, including:\n\nNumeric types, such as integers, fixed-precision floating point numbers, arbitrary precision real numbers, and auto-incrementing integers (serial).\nText, including fixed-length and arbitrary character strings.\nMonetary values\nDates and times\nBoolean values\nGeometric types, such as points, lines, shapes\nElements in sets\nJSON structures\n\nDatabase systems understand how to operate on these types, allowing you to search for all dates that are on a Wednesday, or all text containing particular substrings, and so on.\n\n\n\nWe can think of tables as representing some entity that we are modeling in our problem. For example, a students table might contain enrollment information for a university, where each row represents a single student. Each row in the courses table might represent a single course offered in a specific semester.\nWe link tables to define relationships among entities.\nFor example, each course is linked to multiple students who are enrolled in it. A separate enrollment table may have one row per student per course, identifying the student and the course they are enrolled in.\nA good design of the database tables can make it more efficient to query these relationships, such as to list the students enrolled in a class.\n\n\n\n\nSo far, relational databases sound much like using a collection of data frames in Python or R. But there is a key difference. Most (but not all!) SQL database systems are based on a client-server model.\n\nServer: A program running continuously on some server. Accepts requests (potentially from many users at the same time) to update and query data, and stores the canonical version of the database.\nClient: Any program that can connect to the server, send queries, and receive results from it.\n\nThe client and server need not be on the same computer, or even on the same continent. Often, companies with large databases will have a central database server with huge hard drives and lots of memory; business systems (like an inventory tracker or logistics system) will send their queries to this database to keep it up-to-date, while analysts will sit on their laptops and send queries to produce reports for their bosses.\nA key feature of the client-server model is that the server can serve multiple clients at the same time. The server goes to extraordinary efforts to ensure that it can process the analyst’s queries while simultaneously recording every new sale on the company’s website—and while keeping all results consistent and complete.\nAnd because the server is separate from the client, the server need not be written in the same programming language, and clients implemented in many different languages can access the same server. You can connect to database servers using code written in Python, R, C, Java, Ruby, PHP, or any one of dozens of languages that have the right packages. Because relational databases are so popular, almost any reasonable language you might use will have packages for connecting to popular relational database systems.\nThe client-server model also means that the server can enforce rules about who is allowed to access, update, or delete any particular data. Each client has a username and password, and every table has a set of permissions defining which users are permitted to see it and what operations they are allowed to perform. These permissions can get very detailed, allowing only very specific types of queries for specific users. Companies commonly use this for internal security, and to ensure nobody accidentally deletes important data they did not intend to.\n\n\n\nTo query a database means to send a request to the database server for data matching specific criteria, to send new data to add to the database, or to request deletions or updates. If a query alters the database by adding or changing data, the server automatically stores the updated data—you do not have to explicitly “save” the new data.\nMost relational databases use a (mostly) standardized language called SQL, or Structured Query Language. Your query is written in SQL and submitted directly to the database, which returns a table of matching results (or information about which records have been updated or deleted).\nBecause the language is standardized, you can use SQL regardless of the database system your company has chosen to use. Many different relational database systems (PostgreSQL, MySQL, MariaDB, Oracle, Microsoft SQL Server, Firebird, …) exist, and while they all have unique features and quirks that mean SQL for one won’t work perfectly with another, the basic ideas translate between them all.\n\n\n\nAn RDB stores our data, and we read and operate on that data through requests sent to the database. These requests can be grouped into transactions. A transaction may be a group of multiple queries (insertions, updates, and so on) that complete a particular task.\nModern RDBs may receive many transactions at once, often operating on the same pieces of data. Particular care is needed to ensure that transactions are performed reliably and consistently.\nFor example, consider what would happen in the following cases:\n\nA transaction for a commercial payment is transferring money from your bank account and to another account. But the process ends after the money is deducted from one account but before adding it to the other.\nA similar transaction completes just before the power goes out in the server room\nA similar transaction completes even though you don’t have enough money in your account to make the payment.\n\nThese are all boundary cases, but they can happen. And if they do, the viability of the entire system can be compromised.\nSo, RDBs are designed to make several strong guarantees about their performance, the so-called ACID guarantees:\n\nAtomic: A transaction either succeeds entirely or fails leaving the database unchanged.\nConsistent: A transaction must change the database in a way that maintains all defined rules and constraints.\nIsolated: Concurrent execution of transactions (by multiple users accessing the database at the same time) results in a transformation that would be obtained if the transactions were executed serially.\nDurable: Once a transaction is committed, it remains so even in the face of crashes, power loss, and other errors.\n\nThis is another advantage of RDBs over ad hoc data storage."
  },
  {
    "objectID": "data-engineering/database-fundamentals.html#relational-databases",
    "href": "data-engineering/database-fundamentals.html#relational-databases",
    "title": "Database Fundamentals",
    "section": "",
    "text": "A database is a system for organizing information so it can be efficiently accessed, updated, and managed, even when the data is large and the updates are frequent. Relational databases (RDBs) have been the most common type of database since the 1970s and are still very important. They are organized around data tables with fixed columns and rows for each record or observation.\nRelational databases tend to be design-first systems. First, you specify a schema for your data—giving the tables, the columns they contain, and the types of data stored in each column—and then you enter data that conforms to that schema. A properly designed schema can provide very flexible and powerful queries.\nOnce you have a schema and data, RDBs make it easy to select, filter, and order data, and are particularly suited for combining data from multiple tables—their explicit modeling of the relationships between tables is the source of their name. This is commonly done using a (mostly) standardized language called SQL, which stands for Structured Query Language. Requests written in SQL are called queries.\nMany of the fundamental relational query operations will be familiar to you, since they are supported in many packages for working with data. For example, in R, the dplyr package provides a set of functions for querying, modifying, and joining data frames, and these are very similar to the operations supported by relational databases.\n\n\nThe basic unit of data storage in an RDB is the table. Tables are also sometimes called relations, schemas, and entities in an RDB context. There may be many tables in a single database, so a database is a collection of related tables. A table is defined by its attributes, or columns, each of which has a name and a type.\nYou can think of tables as being much like data frames in R or Python, since each row defines a mapping from attribute names (the columns) to values (stored in that column). Unlike data frames in R or Python, the rows do not have an order; there is no notion of row indices unless you create a column for that purpose.\nAlso unlike data frames in R or Python, tables in relational databases are designed in advance: the columns and their types are specified first, before creating data. While it is possible to modify a table, one does not add and delete columns from tables nearly as often as you do in R or Python.\n\n\n\nThe type of a piece of data describes the set of possible values that data can have and the operations that can apply to it.\nIn an RDB, we specify the type of each column in advance. PostgreSQL, for instance, supports a wide variety of data types, including:\n\nNumeric types, such as integers, fixed-precision floating point numbers, arbitrary precision real numbers, and auto-incrementing integers (serial).\nText, including fixed-length and arbitrary character strings.\nMonetary values\nDates and times\nBoolean values\nGeometric types, such as points, lines, shapes\nElements in sets\nJSON structures\n\nDatabase systems understand how to operate on these types, allowing you to search for all dates that are on a Wednesday, or all text containing particular substrings, and so on.\n\n\n\nWe can think of tables as representing some entity that we are modeling in our problem. For example, a students table might contain enrollment information for a university, where each row represents a single student. Each row in the courses table might represent a single course offered in a specific semester.\nWe link tables to define relationships among entities.\nFor example, each course is linked to multiple students who are enrolled in it. A separate enrollment table may have one row per student per course, identifying the student and the course they are enrolled in.\nA good design of the database tables can make it more efficient to query these relationships, such as to list the students enrolled in a class."
  },
  {
    "objectID": "data-engineering/database-fundamentals.html#the-client-server-model",
    "href": "data-engineering/database-fundamentals.html#the-client-server-model",
    "title": "Database Fundamentals",
    "section": "",
    "text": "So far, relational databases sound much like using a collection of data frames in Python or R. But there is a key difference. Most (but not all!) SQL database systems are based on a client-server model.\n\nServer: A program running continuously on some server. Accepts requests (potentially from many users at the same time) to update and query data, and stores the canonical version of the database.\nClient: Any program that can connect to the server, send queries, and receive results from it.\n\nThe client and server need not be on the same computer, or even on the same continent. Often, companies with large databases will have a central database server with huge hard drives and lots of memory; business systems (like an inventory tracker or logistics system) will send their queries to this database to keep it up-to-date, while analysts will sit on their laptops and send queries to produce reports for their bosses.\nA key feature of the client-server model is that the server can serve multiple clients at the same time. The server goes to extraordinary efforts to ensure that it can process the analyst’s queries while simultaneously recording every new sale on the company’s website—and while keeping all results consistent and complete.\nAnd because the server is separate from the client, the server need not be written in the same programming language, and clients implemented in many different languages can access the same server. You can connect to database servers using code written in Python, R, C, Java, Ruby, PHP, or any one of dozens of languages that have the right packages. Because relational databases are so popular, almost any reasonable language you might use will have packages for connecting to popular relational database systems.\nThe client-server model also means that the server can enforce rules about who is allowed to access, update, or delete any particular data. Each client has a username and password, and every table has a set of permissions defining which users are permitted to see it and what operations they are allowed to perform. These permissions can get very detailed, allowing only very specific types of queries for specific users. Companies commonly use this for internal security, and to ensure nobody accidentally deletes important data they did not intend to."
  },
  {
    "objectID": "data-engineering/database-fundamentals.html#querying-databases",
    "href": "data-engineering/database-fundamentals.html#querying-databases",
    "title": "Database Fundamentals",
    "section": "",
    "text": "To query a database means to send a request to the database server for data matching specific criteria, to send new data to add to the database, or to request deletions or updates. If a query alters the database by adding or changing data, the server automatically stores the updated data—you do not have to explicitly “save” the new data.\nMost relational databases use a (mostly) standardized language called SQL, or Structured Query Language. Your query is written in SQL and submitted directly to the database, which returns a table of matching results (or information about which records have been updated or deleted).\nBecause the language is standardized, you can use SQL regardless of the database system your company has chosen to use. Many different relational database systems (PostgreSQL, MySQL, MariaDB, Oracle, Microsoft SQL Server, Firebird, …) exist, and while they all have unique features and quirks that mean SQL for one won’t work perfectly with another, the basic ideas translate between them all."
  },
  {
    "objectID": "data-engineering/database-fundamentals.html#sec-acid-guarantees",
    "href": "data-engineering/database-fundamentals.html#sec-acid-guarantees",
    "title": "Database Fundamentals",
    "section": "",
    "text": "An RDB stores our data, and we read and operate on that data through requests sent to the database. These requests can be grouped into transactions. A transaction may be a group of multiple queries (insertions, updates, and so on) that complete a particular task.\nModern RDBs may receive many transactions at once, often operating on the same pieces of data. Particular care is needed to ensure that transactions are performed reliably and consistently.\nFor example, consider what would happen in the following cases:\n\nA transaction for a commercial payment is transferring money from your bank account and to another account. But the process ends after the money is deducted from one account but before adding it to the other.\nA similar transaction completes just before the power goes out in the server room\nA similar transaction completes even though you don’t have enough money in your account to make the payment.\n\nThese are all boundary cases, but they can happen. And if they do, the viability of the entire system can be compromised.\nSo, RDBs are designed to make several strong guarantees about their performance, the so-called ACID guarantees:\n\nAtomic: A transaction either succeeds entirely or fails leaving the database unchanged.\nConsistent: A transaction must change the database in a way that maintains all defined rules and constraints.\nIsolated: Concurrent execution of transactions (by multiple users accessing the database at the same time) results in a transformation that would be obtained if the transactions were executed serially.\nDurable: Once a transaction is committed, it remains so even in the face of crashes, power loss, and other errors.\n\nThis is another advantage of RDBs over ad hoc data storage."
  },
  {
    "objectID": "data-engineering/cloud.html",
    "href": "data-engineering/cloud.html",
    "title": "Cloud Computing",
    "section": "",
    "text": "Nowadays, if you work on a large, complex dataset that can’t be analyzed on your laptop, you probably use The Cloud. Companies run their websites from The Cloud, host their databases in The Cloud, and store their files in The Cloud. Giant neural networks are trained by spending millions of dollars in The Cloud.\nSo what is The Cloud? Fundamentally, it’s just a computer rented from someone else.\n\n\n\nxkcd #908\n\n\nBut to understand what we’re doing with The Cloud, we should perhaps go back a bit and talk about how you rent computers, and what cloud computing allows us to do.\n\n\nSuppose it’s 2005 and you work at a company that runs a large online store. You need:\n\nServers to display the website to customers\nStorage for millions of product images, manuals, and brochures\nA database of product information\nA database of customers and sales\nLogs of customer activity, search terms, product popularity, and so on\nA system to send newsletters, product announcements, and sales to customers by email\nServers to run automatic sales and accounting reports\nServers that data analysts can use to run their programs analyzing sales data\n\nDepending on the size of your company, that might involve dozens of servers each running different software. That’s too many computers to simply buy ordinary desktop computers and stick them under your desk. Instead, you’d get specially designed “rackmount” servers, which look like 19” wide pizza boxes that can slide into a specially designed metal rack that can fit 30 or 40 machines at a time.\n\n\n\nServers running Wikipedia. Photo by Victor Grigas, CC-BY-SA\n\n\nIf your company is small, you might stick the rack in a suitable closet, add extra air-conditioning and power outlets, and connect them all to the Internet with the fastest connection you can afford. As your company gets larger, that gets impractical for several reasons:\n\nComputers generate a lot of waste heat, so you need more air conditioning than an ordinary office or conference room would.\nComputers use a lot of power, so you need an electrician to do new wiring; they also don’t like power outages, so you might want a generator or batteries.\nThe more computers you have, the more time you’ll spend dealing with hardware failures. Disk drives, power supplies, and other components tend to fail, and when you have dozens of machines, you’ll be fixing lots of servers.\nSick of managing software updates on your laptop? Imagine having 100 machines that all want to update, and all have different things installed!\nNow you need employees with expertise in hardware, computer repairs, networking, systems administration, and other things your company doesn’t know much about.\n\nSo instead you might outsource your computing needs. Many companies run “data centers”, which are just big warehouses with fancy air conditioning, backup batteries and generators, high-speed internet connections, staff technicians, and plenty of racks with space for servers. Many of these rent out space, so you can buy servers and have them installed in their data center.\nThis is called “colocation”, because your server is located in someone else’s facility, and it was a popular approach. You still have to set up the servers and manage all the software, but the data center staff can help you replace hard drives, install new servers into racks, and so on. Many companies rent space at many data centers.\nBut there are several weaknesses to this approach:\n\nYou still have to set up all the software and configure the servers. That can be a very complicated task, particularly when you’re running a large service. How do you arrange load-balancing so that requests to your website get split among a hundred servers? How do you do automated backups of all your machines? How do you even keep track of all the servers you have and what they do?\nYou also need a team of people with that expertise, and people are expensive.\nAdding new servers takes time. You have to order them from a supplier, wait for them to arrive at the data center, have them installed in a rack, start installing all the necessary software, and so on—which can take weeks.\n\nSo a variety of alternatives appeared. The simplest one was rental: You could rent a so-called “dedicated server”, which was basically a server that a data center already had sitting in a rack, ready to go. For a monthly fee (based on the size and speed of the server), you could get access to log in and install whatever you wanted. Don’t need it any more? Stop paying, and they’ll rent it to someone else. Need a new server? The data center has spares waiting for customers, so you can rent one. All you need is a staffer with a credit card to fill out the form to rent more servers.\nSoon even more options appeared. Many were based on virtualization: Using special software so that one large server could run several operating systems at the same time, each of which gets a fixed chunk of the disk and memory, and does not have access to the others. This could be more efficient for the data centers. Instead of renting servers of all sizes, to suit different customers, they could buy a bunch of high-end servers with lots of CPUs, lots of memory, and very big disks. Then they could rent out slices of these servers to different customers. These slices were called “virtual private servers”, or VPSs.\nIn 2005, then, your company might have chosen to colocate servers, to rent some servers, or to rent some VPSs, depending on its needs. Maybe its core infrastructure was colocated servers, but some servers were rented as needed when new projects started.\n\n\n\nBy about 2007, rented servers were maturing into a new category: cloud computing. The name “cloud” came from the ultimate goal: instead of dealing with real physical servers, there’d just be a bunch of computing resources available on-demand, for a fee. If you were drawing a diagram of your system infrastructure, you could just draw a cloud-shaped blob in the middle where the computing goes.\nCloud computing builds on the rental framework with several additional features:\n\nMinute-by-minute billing. Instead of renting servers a month at a time, you can rent them for just the time you need them.\nFast setup. If you want more servers or services, you can get them added to your account in minutes. (Also, you previously learned about container systems like Docker. Combine these with cloud computing and it’s easy to get a server running all your software in a container, in minutes.)\nIntegrated computing products. Need a database? It’s a pain to rent a server, install PostgreSQL on it, adjust all the settings, keep it updated, set up backups… why not just rent a PostgreSQL database? Cloud computing companies offer all kinds of integrated services for rent, not just bare servers.\nAccounting systems for keeping track of all your servers and what they do. Many of these systems are built for large companies that have many teams, where you need to keep track of who has the servers, and maybe need to set usage quotas and billing details separately for different parts of the company.\nAPIs to automatically rent more services, shut machines off, change their settings, and so on—so instead of using forms to request services, your Python program can do it automatically.\n\nFrom about 2010 onward, companies rapidly moved their computing over to The Cloud. It offered many advantages: great flexibility, less hardware maintenance (all handled by the cloud provider), faster setup, greater reliability… and as cloud providers can specialize in developing computing services, they could build more sophisticated services than a company could typically build on its own. Rapidly growing companies could quickly rent more services, or even scale services up and down automatically as needed to meet customer demand.\nThere are, of course, disadvantages. Renting a basic server, without any of the fancy services, is much more expensive than just buying one. Different cloud providers offer different services, so once you’ve built a system using one, it’s hard to switch to another. And while it’s nice to automatically rent more servers if your site is suddenly overloaded, it’s also easy to wake up one morning and find a $100,000 bill because one of your services got popular overnight. Or to forget to turn off services you’re not using, and pay for months of “use” you never needed. Anyone at large companies with thousands of cloud servers has stories of systems consuming thousands of dollars a month because someone forgot to turn them off.\n\n\n\nThere are several major cloud providers. Amazon is the most well-known with its Amazon Web Services system, but Microsoft Azure and Google Cloud are competing heavily, and there are many other companies offering cloud services, including IBM, Rackspace, Alibaba, and many others.\nEach has its own set of services, but there are common ones available in different forms by many providers:\n\nRented servers. Amazon calls this the Elastic Compute Cloud (EC2); Google calls this the Compute Engine; but in each case, you specify what memory and processor you want, and you get a machine to rent, just like the old VPS services.\nObject storage. Here “object” typically means “file”; but instead of offering a hard drive with lots of space, cloud providers build object storage systems. These let you organize lots of files into groups, and have the cloud provider automatically store them in a redundant way so they aren’t lost when a hard drive fails. Objects are typically available through an API you can access from various programming languages.\nDatabases. Rent a SQL database where all the software is set up for you, and you just pay by the hour.\n\nIn recent years, many cloud providers have created products focused on data science and machine learning. Many have products based on Apache Spark; many have deep learning products that can store your trained models and produce predictions on request; some even have point-and-click tools that will fit machine learning models to your data without requiring you to write any code."
  },
  {
    "objectID": "data-engineering/cloud.html#the-olden-days",
    "href": "data-engineering/cloud.html#the-olden-days",
    "title": "Cloud Computing",
    "section": "",
    "text": "Suppose it’s 2005 and you work at a company that runs a large online store. You need:\n\nServers to display the website to customers\nStorage for millions of product images, manuals, and brochures\nA database of product information\nA database of customers and sales\nLogs of customer activity, search terms, product popularity, and so on\nA system to send newsletters, product announcements, and sales to customers by email\nServers to run automatic sales and accounting reports\nServers that data analysts can use to run their programs analyzing sales data\n\nDepending on the size of your company, that might involve dozens of servers each running different software. That’s too many computers to simply buy ordinary desktop computers and stick them under your desk. Instead, you’d get specially designed “rackmount” servers, which look like 19” wide pizza boxes that can slide into a specially designed metal rack that can fit 30 or 40 machines at a time.\n\n\n\nServers running Wikipedia. Photo by Victor Grigas, CC-BY-SA\n\n\nIf your company is small, you might stick the rack in a suitable closet, add extra air-conditioning and power outlets, and connect them all to the Internet with the fastest connection you can afford. As your company gets larger, that gets impractical for several reasons:\n\nComputers generate a lot of waste heat, so you need more air conditioning than an ordinary office or conference room would.\nComputers use a lot of power, so you need an electrician to do new wiring; they also don’t like power outages, so you might want a generator or batteries.\nThe more computers you have, the more time you’ll spend dealing with hardware failures. Disk drives, power supplies, and other components tend to fail, and when you have dozens of machines, you’ll be fixing lots of servers.\nSick of managing software updates on your laptop? Imagine having 100 machines that all want to update, and all have different things installed!\nNow you need employees with expertise in hardware, computer repairs, networking, systems administration, and other things your company doesn’t know much about.\n\nSo instead you might outsource your computing needs. Many companies run “data centers”, which are just big warehouses with fancy air conditioning, backup batteries and generators, high-speed internet connections, staff technicians, and plenty of racks with space for servers. Many of these rent out space, so you can buy servers and have them installed in their data center.\nThis is called “colocation”, because your server is located in someone else’s facility, and it was a popular approach. You still have to set up the servers and manage all the software, but the data center staff can help you replace hard drives, install new servers into racks, and so on. Many companies rent space at many data centers.\nBut there are several weaknesses to this approach:\n\nYou still have to set up all the software and configure the servers. That can be a very complicated task, particularly when you’re running a large service. How do you arrange load-balancing so that requests to your website get split among a hundred servers? How do you do automated backups of all your machines? How do you even keep track of all the servers you have and what they do?\nYou also need a team of people with that expertise, and people are expensive.\nAdding new servers takes time. You have to order them from a supplier, wait for them to arrive at the data center, have them installed in a rack, start installing all the necessary software, and so on—which can take weeks.\n\nSo a variety of alternatives appeared. The simplest one was rental: You could rent a so-called “dedicated server”, which was basically a server that a data center already had sitting in a rack, ready to go. For a monthly fee (based on the size and speed of the server), you could get access to log in and install whatever you wanted. Don’t need it any more? Stop paying, and they’ll rent it to someone else. Need a new server? The data center has spares waiting for customers, so you can rent one. All you need is a staffer with a credit card to fill out the form to rent more servers.\nSoon even more options appeared. Many were based on virtualization: Using special software so that one large server could run several operating systems at the same time, each of which gets a fixed chunk of the disk and memory, and does not have access to the others. This could be more efficient for the data centers. Instead of renting servers of all sizes, to suit different customers, they could buy a bunch of high-end servers with lots of CPUs, lots of memory, and very big disks. Then they could rent out slices of these servers to different customers. These slices were called “virtual private servers”, or VPSs.\nIn 2005, then, your company might have chosen to colocate servers, to rent some servers, or to rent some VPSs, depending on its needs. Maybe its core infrastructure was colocated servers, but some servers were rented as needed when new projects started."
  },
  {
    "objectID": "data-engineering/cloud.html#rental-computing",
    "href": "data-engineering/cloud.html#rental-computing",
    "title": "Cloud Computing",
    "section": "",
    "text": "By about 2007, rented servers were maturing into a new category: cloud computing. The name “cloud” came from the ultimate goal: instead of dealing with real physical servers, there’d just be a bunch of computing resources available on-demand, for a fee. If you were drawing a diagram of your system infrastructure, you could just draw a cloud-shaped blob in the middle where the computing goes.\nCloud computing builds on the rental framework with several additional features:\n\nMinute-by-minute billing. Instead of renting servers a month at a time, you can rent them for just the time you need them.\nFast setup. If you want more servers or services, you can get them added to your account in minutes. (Also, you previously learned about container systems like Docker. Combine these with cloud computing and it’s easy to get a server running all your software in a container, in minutes.)\nIntegrated computing products. Need a database? It’s a pain to rent a server, install PostgreSQL on it, adjust all the settings, keep it updated, set up backups… why not just rent a PostgreSQL database? Cloud computing companies offer all kinds of integrated services for rent, not just bare servers.\nAccounting systems for keeping track of all your servers and what they do. Many of these systems are built for large companies that have many teams, where you need to keep track of who has the servers, and maybe need to set usage quotas and billing details separately for different parts of the company.\nAPIs to automatically rent more services, shut machines off, change their settings, and so on—so instead of using forms to request services, your Python program can do it automatically.\n\nFrom about 2010 onward, companies rapidly moved their computing over to The Cloud. It offered many advantages: great flexibility, less hardware maintenance (all handled by the cloud provider), faster setup, greater reliability… and as cloud providers can specialize in developing computing services, they could build more sophisticated services than a company could typically build on its own. Rapidly growing companies could quickly rent more services, or even scale services up and down automatically as needed to meet customer demand.\nThere are, of course, disadvantages. Renting a basic server, without any of the fancy services, is much more expensive than just buying one. Different cloud providers offer different services, so once you’ve built a system using one, it’s hard to switch to another. And while it’s nice to automatically rent more servers if your site is suddenly overloaded, it’s also easy to wake up one morning and find a $100,000 bill because one of your services got popular overnight. Or to forget to turn off services you’re not using, and pay for months of “use” you never needed. Anyone at large companies with thousands of cloud servers has stories of systems consuming thousands of dollars a month because someone forgot to turn them off."
  },
  {
    "objectID": "data-engineering/cloud.html#common-products",
    "href": "data-engineering/cloud.html#common-products",
    "title": "Cloud Computing",
    "section": "",
    "text": "There are several major cloud providers. Amazon is the most well-known with its Amazon Web Services system, but Microsoft Azure and Google Cloud are competing heavily, and there are many other companies offering cloud services, including IBM, Rackspace, Alibaba, and many others.\nEach has its own set of services, but there are common ones available in different forms by many providers:\n\nRented servers. Amazon calls this the Elastic Compute Cloud (EC2); Google calls this the Compute Engine; but in each case, you specify what memory and processor you want, and you get a machine to rent, just like the old VPS services.\nObject storage. Here “object” typically means “file”; but instead of offering a hard drive with lots of space, cloud providers build object storage systems. These let you organize lots of files into groups, and have the cloud provider automatically store them in a redundant way so they aren’t lost when a hard drive fails. Objects are typically available through an API you can access from various programming languages.\nDatabases. Rent a SQL database where all the software is set up for you, and you just pay by the hour.\n\nIn recent years, many cloud providers have created products focused on data science and machine learning. Many have products based on Apache Spark; many have deep learning products that can store your trained models and produce predictions on request; some even have point-and-click tools that will fit machine learning models to your data without requiring you to write any code."
  },
  {
    "objectID": "data-engineering/text-search.html",
    "href": "data-engineering/text-search.html",
    "title": "Full Text Search",
    "section": "",
    "text": "So far our searching, grouping, and aggregation has focused on numerical columns and numerical summaries of them. But text data is quite common. We can easily store text in PostgreSQL by using the TEXT column type, but often we want ways to efficiently search through text. This problem is known as full text search, because you’d like to search using the entire contents of the text, not just features or tags attached to it.\nFull text search requires special consideration because doing it well is a lot of work. A naive approach—looping through every row of data and checking if the text matches a search query—would be impossibly slow and wouldn’t even be very good at searching, so we must plan ahead to make full text search effective."
  },
  {
    "objectID": "data-engineering/text-search.html#text-searching-tasks",
    "href": "data-engineering/text-search.html#text-searching-tasks",
    "title": "Full Text Search",
    "section": "Text searching tasks",
    "text": "Text searching tasks\nText is complicated. As you can imagine, there are many ways you might want to search it.\n\nExample 1 (Movie titles) You run a website that hosts movie information and reviews. Users type the title of a movie—or parts of it—into a search box, and your system must find the matching movies to list for them.\nIf you have a movies table with a title column, a simple query is\nSELECT * FROM movies\nWHERE title = %s\nfilling in the user’s entry for the placeholder. But they may type in only part of the title (everything everywhere to find Everything Everywhere All at Once), they might misspell it (the avngers), or they might misremember it and get parts wrong (star wars the return of the sith). How do you find the best match?\n\n\nExample 2 (Citation search) You’re analyzing a database of scientific papers to find out which ones cite each other. Each paper has a Digital Object Identifier (DOI), a unique ID in the form doi:10.NNNN/suffix, where NNNN is a four-digit (or longer) number and the suffix can be any string of characters not including spaces.\nReference lists for papers often give the DOI for the papers they cite, so you’d like to find all papers with DOIs in them. How do you search for text matching this pattern?\n\n\nExample 3 (Legal documents) You’re suing a large tech company because you believe they violate antitrust law. Through subpoenas, you have obtained millions of their internal emails, and you want to search the emails for ones mentioning monopoly, competition, collusion, and other such words and phrases. But those words have many variations: monopolies, monopolistic, competitive, competitors, colluding, etc.\nIf the emails are in a table with subject and body columns containing their contents, how do you effectively search them?\n\nThere are a variety of basic text operators that can help with simple tasks, and we’ll address those first. Then we’ll move on to more advanced full-text search.\nBut first: What is text, and how is it stored?"
  },
  {
    "objectID": "data-engineering/text-search.html#character-encoding",
    "href": "data-engineering/text-search.html#character-encoding",
    "title": "Full Text Search",
    "section": "Character encoding",
    "text": "Character encoding\nComputer memory is binary: it stores only 0s and 1s. It follows that text must be stored as numbers, not as text. That requires a character encoding: a system to number each character in a given alphabet, so they can be stored as numbers and then displayed as the correct characters.\nSince the 1960s, the common standard has been the American Standard Code for Information Interchange (ASCII). ASCII reflects the biases of American computer engineers in the 1960s: it uses only 7 bits to encode each number, allowing only \\(2^7 = 128\\) possible characters.1 These were used to encode the upper- and lowercase Latin alphabet, spaces, newlines, punctuation marks, and a variety of control characters that tell a teletype machine to do something rather than displaying text. (For example, there’s a character that tells the machine to ring a bell to alert the operator to do something.)\nNotably, ASCII does not include any accented characters (like é, ñ, or ö), characters from any other alphabet (Cyrillic, Arabic, Devanagari, Hangul, etc.), or symbols from any other writing system (Chinese, Japanese, etc.).\nThis led to an explosion of character encoding systems specialized for specific scripts, each supporting only some scripts but not others. You’ll encounter a few of the most common:\n\nISO 8859-1, also called Latin-1: Uses 8 bits instead of 7, allowing it to extend ASCII with accented characters used in many European languages. Commonly also seen as Windows-1252 or CP-1252, which adds some additional characters.\nGB 2312 and GB 18030, Chinese government standards supporting simplified and traditional Chinese characters.\nShift JIS, used for Japanese.\nWindows-1251, used for Cyrillic.\n\nNowadays, the most common encoding is UTF-8, which is designed to cover almost every script used in the world (and many historic ones no longer in use). We’ll discuss it in more detail in Section 2.3.\n\nCharacter encoding mishaps\nCharacter encoding problems occur when text in one encoding is interpreted as being from another encoding.\nFor example, the string can’t (notice the right curly apostrophe) can be encoded in UTF-8, which has a character for the right single quotation mark. But that character is represented as three bytes. In Windows-1252, those three bytes are read as â, €, and ™, so the string appears canâ€™t. If you don’t know the encoding of the text and pick the wrong one, you will display it incorrectly.\nThis is also annoying for searching: if you’re searching for a string that you’ve written in one encoding, but the data you’ve stored is in a different encoding, a naive search that checks the bytes stored will not find it.\n\n\nConverting\nOld programming languages like C do not require strings to be in any specific encoding—you can shove any bytes you want into a string, and it’s your problem if the encoding is wrong.\nModern systems like Python (since Python 3) enforce a consistent encoding. In Python, every string must be read in with a defined encoding, so Python can convert it automatically when doing any operations. For example, if I try to read a file that is not UTF-8-encoded:\nIn [5]: f = open(\"/bin/ls\", \"r\")\nIn [6]: f.readline()\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 f.readline()\n\nFile ~/miniconda3/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    319 def decode(self, input, final=False):\n    320     # decode input (taking the buffer into account)\n    321     data = self.buffer + input\n--&gt; 322     (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323     # keep undecoded input until the next call\n    324     self.buffer = data[consumed:]\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\nThe file /bin/ls is a compiled program, not text, so when Python tries to interpret the bytes as UTF-8 text, it finds some of them are not valid characters and immediately complains.\nThis is why open() has an encoding argument: so you can tell it which encoding to read, and it can automatically translate the encoding as you operate on the text.\n\n\nUTF-8: the only acceptable encoding\nUnicode is a standard that attempts to list the characters in all the world’s writing systems, assigning each a unique identifying number. (Right now nearly 155,000 characters are defined and numbered.) These are called code points. Each code point is, conceptually, one “thing” in the writing system.\nUnicode code points are commonly written with the notation U+NNNN. For example, U+0065 is the code point LATIN SMALL LETTER E, i.e. e. The number is written in hexadecimal.\nThere are several encodings defined in the Unicode standard, which give ways to encode the code points in binary. The most popular, now used by around 99% of websites and applications, is UTF-8.\nUTF-8 is weird and it may break some of your assumptions about how text is stored, so it’s useful to understand how it works.\nUTF-8 is a variable-width encoding. That means some characters are represented with only one byte (8 bits) and others are represented with up to 4 bytes (32 bits). The one-byte characters generally match the encodings used in ASCII, so if you UTF-8 encode English text without any accents or special characters, it will also be valid ASCII.\nThis is useful for compatibility and for compactness: the designers of UTF-8, who were Westerners who mostly dealt with the Latin alphabet, wanted to use the fewest bytes for what was (to them) the most common type of text.\nFor characters outside the basic ASCII range, a special code in the first byte indicates that the character continues to two, three, or four bytes. The encoding is designed so that software reading the bytes can quickly determine where characters start and end, so there’s no danger of accidentally reading, say, the middle two bytes of a four-byte character and interpreting them as a character of their own.\nThis can cause confusion in languages where string indexing and length is based on the number of bytes. For instance, 💩 (U+1F4A9) is encoded with four bytes, but it is only one code point. Python counts code points:\nIn [8]: len(\"💩\")\nOut[8]: 1\nDespite its quirks, UTF-8 is now almost universally supported by software and is widely used across the Internet. If you are creating text data, you should encode it as UTF-8.\n\n\nNormalization\nAnother quick is that Unicode provides two ways to encode many common characters. For example, the code point U+0301 represents the COMBINING ACUTE ACCENT. Place it after any character and it puts an acute accent on that character. Hence we can write é as U+0065 U+0301 (e plus an acute accent).\n(This combining feature is also used for encoding emojis that can be displayed in different skin tones: 👍🏽 is 👍 plus the U+1F3FD medium skin tone modifier.)\nBut Unicode also provides precomposed forms of common characters, so U+00E9 is LATIN SMALL LETTER E WITH ACUTE. Consequently:\nIn [14]: len(\"é\")\nOut[14]: 1\n\nIn [15]: len(\"é\")\nOut[15]: 2\nThis is another problem for searching text: a user searching for “résumé” might not find a document containing “résumé” because it’s encoded differently.\nTo solve this, Unicode defines normalization procedures that take text and turn it into a single canonical form. For instance, NFC (Normalization Form Canonical Composition) composes every character when possible, so é is always one byte, not two. (There are three other normalization schemes that produce different normalized forms, but NFC is the most commonly used.)\nTo effectively search text, then, we must both know its encoding and ensure it is normalized consistently.\n\n\nLanguage differences\nSome operations depend not just on the encoding but on the language.\nFor example: what is the uppercase version of i? In many languages, I; but in Turkish, it’s İ. What’s the lowercase version of I? In Turkish, it’s ı. (Turkish scores a point for logical consistency.)\nOperations like uppercasing and lowercasing hence depend on the language! Even Python doesn’t handle this automatically, and so if you’re serious about text, you’ll need a library like ICU (International Components for Unicode) that provides all the necessary procedures."
  },
  {
    "objectID": "data-engineering/text-search.html#basic-text-operations-in-postgresql",
    "href": "data-engineering/text-search.html#basic-text-operations-in-postgresql",
    "title": "Full Text Search",
    "section": "Basic text operations in PostgreSQL",
    "text": "Basic text operations in PostgreSQL\nIn PostgreSQL, every database has a default encoding (hopefully UTF-8), and PostgreSQL uses that encoding to store all text and character columns. Since it knows the encoding, it can do text operations. Here are a few common ones:\nSELECT trim(' foo bar   '); -- deletes whitespace at beginning and end\n\nSELECT 'foo' || 'bar'; -- concatenate\n\nSELECT 'foo' || 4; -- convert to string and concatenate\n\n-- find location in string:\nSELECT position('doi:' in 'it is doi:10.1073/pnas.2111454118');\nThe SQL standard only defines a small set of string functions, so database engines provide their own additional functions. PostgreSQL supports many common operations you might want to do:\nSELECT starts_with('doi:10.1073/pnas.2111454118', 'doi');\n\nSELECT substr('doi:10.1073/pnas.2111454118', 5);\n\nSELECT length('💩'); -- Azure Data Studio bug\n\nPattern matching\nOften, as in Example 1, it’s sufficient to search a text column for values matching a certain pattern. SQL provides the LIKE operator to do this. LIKE uses a special pattern syntax to define what to look for. In the patterns, % represents any sequence of zero or more characters, so we can write:\nSELECT 'doi:10.1073/pnas.2111454118' LIKE 'doi%';\n\nSELECT 'Gone with the Wind' LIKE '%with the%';\n\n-- case sensitive:\nSELECT 'Gone with the Wind' LIKE '%WITH THE%';\n\n-- or use ILIKE for case-insensitive:\nSELECT 'Gone with the Wind' ILIKE '%WITH THE%';\n\nSELECT 'Return of the Jedi' LIKE '%revenge%';\nSimilarly, _ represents any single character, but not more:\nSELECT 'foo' LIKE '_o_';\n\nSELECT 'foooo' LIKE '_o_';\nInstead of LIKE and ILIKE, you can also write ~~ or ~~*, as in 'Gone with the Wind' ~~ '%with the%'. This is not part of the SQL standard, but PostgreSQL supports them internally.\n\nExercise 1 (Like, totally) In the events table, the feedback column gives a textual description of the feedback to the student. Produce a query that counts how many events have the string “you” in their feedback, in upper- or lowercase.\n\n\nSolution. \nSELECT COUNT(*)\nFROM events\nWHERE feedback ILIKE '%you%';\n\n\n\nRegular expression matching\nLIKE provides simple patterns, but its pattern matching is quite limited. What if you want to, say, find strings containing only the digits 0-9 repeated at most 4 times, followed by at least one uppercase letter? What if you want to check if every password has at least one uppercase letter, one symbol, a Pokémon character name, and a Mersenne prime?2\nTo match more complex passwords, we can use regular expressions. There are multiple ways to write regular expressions; the SQL standard provides the SIMILAR TO operator, which uses a syntax no other software uses, but most database systems also support POSIX regular expressions. These follow a standard syntax widely used by many different systems; even software that varies typically only varies in the details and not the general picture.\nA regular expression is a specially formatted string that defines the pattern we would like to match. Basic patterns are simple: the regular expression ducks matches any portion of text containing the substring \"ducks\". But other syntax in patterns produces more complex behavior. Let’s work through basic syntax with some examples:\n\ndu.ks\n^du.ks\n^du.ks$\ndu.+ks\ndu.*ks\ndu.?ks\n\nThese wildcards are greedy: they will match everything they can, even if a shorter match is possible. However, we can change their behavior with ?:\n\ndu.*?ks\n\nWe can give a set of characters using square brackets:\n\ndu[cn]ks\n&lt;[a-zA-Z]&gt;\n&lt;[^j]&gt;\ndu[cn]ks|g[oe]+se\n\nThere are certain common sets that are represented using special escapes:\n\n\\d: any digit 0-9\n\\s: any whitespace character (space, newline, tab, etc.)\n\\w: any word character (alphanumeric and underscore)\n\nWe can define capturing groups using parentheses, then refer to them later in our pattern:\n\n(du[cn]ks|g[oe]+se)\n&lt;([a-zA-Z]+)&gt;(.*)&lt;/\\1&gt;\n\nIf you need to literally match parentheses, brackets, or other syntax, you must escape them with a backslash: \\( matches an open parenthesis.\nThere are many additional options and syntax features we can define, but it’s best to master the basics first.\nRegular expressions can be used within Postgres using the ~ operator. Regular expressions are specified as strings, so we must be careful with string escaping. For example:\nSELECT 'foo' ~ 'foo';\nSELECT 'ducks' ~ 'du.ks';\nSELECT '&lt;foo&gt;bar&lt;/foo&gt;' ~ '&lt;([a-zA-Z]+)&gt;(.*)&lt;/\\1&gt;';\nUsing regexp_match(), we can see the portion of the string that matches, or the capture groups that match:\nSELECT regexp_match('the mighty ducks', 'du.ks');\nSELECT regexp_match('&lt;foo&gt;bar&lt;/foo&gt;', '&lt;([a-zA-Z]+)&gt;(.*)&lt;/\\1&gt;');\nThere are also functions count how many matches there are; see section 9.7.3 of the manual for details.\nRegExr is a great tool to visualize your regular expressions and see what they match. There are slight syntactic differences between TODO\n\nExercise 2 (Practice regular expressions) For each pattern below, use regexp_match() to confirm the expression matches what you intend it to.\n\nWrite a regular expression that matches strings of the form doi:10., followed by one or more numbers, then a forward slash, then one or more alphanumeric characters. Capture the part after doi: in a capture group.\nMatch strings like [tag]text[/tag], where the two tags match and the text does not contain [/tag]. For instance, in the string \"[foo]text[/foo]bar[/foo]\", the match should only be \"[foo]text[/foo]\".\nMatch strings like key=value, capturing both the key name and the value. The key should be alphabetical and the value should be a decimal number, like 3.14. For example, pi=3.1415 should capture pi and 3.1415, but 4=7.2.4 should not match.\n\n\nFinally, there is one use case that often comes up: If you’re parsing HTML (to scrape a website, for instance), it’s tempting to use regular expressions to match the element you need. Don’t! You can’t parse HTML with regex. The syntax of HTML is too complicated for it to be possible to write a regular expression that always correctly matches HTML tags. See ?@sec-scraping for the correct approach.\n\n\nRegular expression replacement\nOne common use of regular expressions is to replace certain patterns in text with others. Frequently this is used inside text editors to do sophisticated find-and-replace, and it gets used in many other applications processing text that needs to be reformatted to match some specification.\n\nExample 4 (BBCode) You may be familiar with Markdown, the simple text markup language used in systems like R Markdown and in comment boxes on many websites. Before Markdown was widely used, forum software often used BBCode, a system based on a restricted set of “tags” similar to HTML.\nFor example, [b]some phrase[/b] represents bolding the text, and might be translated into the HTML for &lt;b&gt;some phrase&lt;/b&gt; or &lt;strong&gt;some phrase&lt;/strong&gt;.\nUsing BBCode prevents users from writing HTML directly (and hence messing with the site’s formatting) and restricts them to only tags supported by the forum software.\nA simple implementation of BBCode might use regular expressions to match a set of tags and convert them to the corresponding HTML tags.\n\nMost regular expression systems support regular expression-based text replacement. In PostgreSQL this is called regexp_replace(), and takes a regular expression (to choose what to replace) and a replacement pattern. In the simplest form, the replacement is just a static string:\n-- Filter naughty words\nSELECT regexp_replace('recrudescent', 'dang|crud', '****') AS clean;\n    clean\n--------------\n re****escent\n(1 row)\nBut the pattern can also refer to the capture groups in the regular expression. For instance, we can make text dirtier:\n--- Emphasize naughty words\nSELECT regexp_replace('dang recrudescents!', '(dang|crud)', '\\1!!!') AS dirty;\n         dirty\n------------------------\n dang!!! recrudescents!\nNote only the first match was replaced. That’s the default behavior, but regexp_replace() takes additional arguments to control it. A flags argument gives a string of characters setting options, such as i for case-insensitive matching and g for global matching, meaning replacing everything in the string:\nSELECT regexp_replace('dAnG ReCruDeSCeNts!', '(dang|crud)', '\\1!!!', 'ig') AS dirty;\n           dirty\n---------------------------\n dAnG!!! ReCruD!!!eSCeNts!\n(1 row)\n\n\nRegular expressions in Python\nBesides PostgreSQL, you can do regular expressions in most languages, including Python. In Python they are supported by the re module of the standard library.\n\nimport re\n\n# list all matches\nre.findall('(dang|crud)', 'dang recrudescents!')\n\n['dang', 'crud']\n\n\n\n# check if entire string matches\nre.fullmatch(r\"&lt;([a-zA-Z]+)&gt;(.*)&lt;/\\1&gt;\", \"&lt;foo&gt;bar&lt;/foo&gt;\")\n\n&lt;re.Match object; span=(0, 14), match='&lt;foo&gt;bar&lt;/foo&gt;'&gt;\n\n\n\n# substitute (replace) text\nre.sub(\"(dang|crud)\", r\"\\1!!!\", \"dAnG ReCruDeSCeNts!\",\n       flags=re.IGNORECASE)\n\n'dAnG!!! ReCruD!!!eSCeNts!'\n\n\nNotice we’re using raw string syntax (r\"string\") to avoid needing to escape backslashes.\n\nExercise 3 (BBCode with regexp) Suppose you’re building a simple forum software and want to support BBCode (Example 4). Write a single re.sub() call that supports i, b, and u tags, converting them to the HTML tags &lt;i&gt;, &lt;b&gt;, and &lt;u&gt; (italic, bold, and underline).\nFor example, [i]This[/i] is a [b]test![/b] should be converted to &lt;i&gt;This&lt;/i&gt; is a &lt;b&gt;test!&lt;/b&gt;, but Your mother is a [duck]hamster[/duck] should be unchanged. However, when tags are not nested, they should not match: [b]This [i]is[/b] a test[/i] should not be replaced with anything.\n\n\nExercise 4 (Syntax adaptation) Different SQL engines generally support the SQL standard but add variations and special features, causing problems when you use SQL code written for one engine in another. MySQL uses backticks to delimit column and table names so they are not interpreted as SQL syntax (e.g. if you have a table named select).\nFor example, a file full of MySQL queries might contain\nCREATE TABLE `employeelist` (\n  eid INTEGER,\n  firstName varchar(31) NOT NULL DEFAULT '',\n  ... -- more columns go here\n);\nBut PostgreSQL does not support backtick syntax.\nWrite a Python regular expression to take a string containing the MySQL CREATE TABLE syntax, returning a form without the backticks around the table name.\n\n\nSolution. Here’s a solution:\nre.sub(r\"^CREATE\\s+TABLE\\s+`(\\w+)`\", r\"CREATE TABLE \\1\", text,\n       flags=re.IGNORECASE)"
  },
  {
    "objectID": "data-engineering/text-search.html#language-aware-text-search",
    "href": "data-engineering/text-search.html#language-aware-text-search",
    "title": "Full Text Search",
    "section": "Language-aware text search",
    "text": "Language-aware text search\nDespite all this fancy pattern-matching, we’re still not done searching text. Recall Example 3: we wanted to search emails for words like monopoly, while also including variations like monopolies, monopolistic, and so on. We could write many patterns to do this, of course:\nSELECT subject, body FROM emails\nWHERE body LIKE '%monopoly%' OR body LIKE '%monopolies%' OR ...;\nBut that gets boring to write. And inefficient: to search the text, PostgreSQL must load every row of text and search through it—potentially many megabytes or gigabytes of text. (My CMU email account contains 12 gigabytes of messages, for instance.) So: We’d like to automatically handle word variants, and we’d like to index our text so we can quickly find words and phrases.\n\nLexing text\nTo handle word variants, we need two pieces:\n\nA tokenizer that splits text up into discrete tokens, such as words, numbers, links, and so on. Then we can operate on tokens, rather than on individual characters.\nA lexer that normalizes tokenized text to make all variants of a word the same. The lexer might make all words lowercase, remove plural suffixes (s or es), convert conjugated verbs to their infinitive form (washed → wash, monopolized → monopolize), and remove stop words, which are very common words that are useless for searching because they appear everywhere (the, and, is, and so on). The lexer outputs lexemes, the normalized tokens.\n\nBuilding good tokenizers and lexers is hard. They must be specialized to the language and grammar. Languages are often irregular, so they need lots of special-case rules and dictionaries. Fortunately PostgreSQL already has lexers for popular languages (and you can probably find lexers for uncommon languages if you need them).\nPostgreSQL’s lexer system turns strings into tsvectors (for text search vectors). Here’s what they look like:\nSELECT to_tsvector('english', 'the quick brown fox jumps over the lazy dog');\n                      to_tsvector\n-------------------------------------------------------\n 'brown':3 'dog':9 'fox':4 'jump':5 'lazi':8 'quick':2\n(1 row)\nDespite the appearance, the tsvector is not a string. It is an internal data structure presenting the lexemes and their positions within the lexed text. Notice that the has disappeared, because it is a stopword, and lazy has been lexed into lazi. (Laziness, lazies, and similar words all lex to lazi.)\nSimilarly, the first sentence of the Gettysburg address is lexed into this tsvector:\n'ago':6 'brought':9 'conceiv':17 'contin':13 'creat':29 'dedic':21\n'equal':30 'father':8 'forth':10 'four':1 'liberti':19 'men':27\n'nation':16 'new':15 'proposit':24 'score':2 'seven':4 'upon':11 'year':5\n\n\nQuerying documents\nNext, Postgres has a tsquery type for queries. Queries consist of lexed words, optionally in a specific order. The to_tsquery() function converts strings into this format:\nSELECT to_tsquery('english', 'the & quick & brown & fox & jumps');\n              to_tsquery\n------------------------------------\n 'quick' & 'brown' & 'fox' & 'jump'\n(1 row)\nThe query string is written using the & operator to indicate we want all of the words; notice that the was dropped (it’s a stopword) and jumps was lexed to jump. This will match the words in any order. We use the @@ operator to match a tsvector to a tsquery:\nSELECT to_tsvector('the fox brown jumps quickly') @@\n  to_tsquery('the & quick & brown & fox & jumps');\nThe query syntax supports all logical operators, not just &; we can use | (OR) and ! (negation) to express complex queries.\nAlternately, Postgres supports queries written in a way you may be more familiar with from Google and other search systems. Phrases are searched as individual words with &; phrases inside quotation marks must match exactly; writing OR between words allows any to match; and - acts as NOT. For instance:\nSELECT to_tsvector('the fox brown jumps quickly') @@\n  websearch_to_tsquery('\"the quick brown\" -dog');\n\n\nIndexing lexed text\nIt’s clearly inconvenient to use to_tsvector on a text column every time we want to search it. Postgres must go through the entire column, tokenize and lex the text, and then prepare to search it. We can instead ask Postgres to create an index of the tsvector version:\nCREATE INDEX name_of_index ON tablename\nUSING GIN (ts_tsvector('english', columnname));\nNow, whenever we do to_tsvector() on the column in a query, Postgres will use the index instead. A GIN index is an “inverted” index: the index will contain all the lexemes in the column and, for each lexeme, a list of all the rows containing it. The lexemes are organized in a tree so they can quickly be found in a search.\nAlternately, we can ask Postgres to store the tsvectors in a separate column. PostgreSQL can automatically generate columns that are updated whenever a row is changed:\nALTER TABLE tablename\nADD COLUMN column_index_col tsvector\nGENERATED ALWAYS AS (to_tsvector('english', columname)) STORED;\n\nCREATE INDEX column_idx ON tablename USING GIN (column_index_col);\nNow we can write our search queries in terms of column_index_col instead of writing to_tsvector() out every time.\n\n\nSearching with indices\nIn the examples database, I have loaded several tables containing emails from Enron. In brief, Enron was a major electricity, gas, and communications company with revenues of $100 billion dollars in 2000—until, in 2001, it was discovered that its executives used accounting fraud to fake its revenues, and the company collapsed. During the various lawsuits and criminal trails afterward, many of Enron’s internal emails were released, and they are still often used as example data in data analysis. (Often the emails between executives are treated as a social network and used in social network analysis examples, for instance.)\nThe message table contains several hundred thousand emails. I have indexed the subject and body columns containing the email contents, so we can quickly search it:\nSELECT sender, subject FROM message\nWHERE subject_index_col @@ websearch_to_tsquery('monopoly')\nLIMIT 10;\nThis query is blazing fast despite the size of the database. Even searching the bodies is fast:\nSELECT sender, subject FROM message\nWHERE body_index_col @@ websearch_to_tsquery('monopoly')\nLIMIT 10;\nOne approach is to parse the query in the FROM clause, allowing it to be used repeatedly throughout the query. This will be useful when we discuss ranking:\nSELECT sender, subject\nFROM message, websearch_to_tsquery('monopoly') AS query\nWHERE body_index_col @@ query\nLIMIT 10;\nThe query has one row and message has many; implicitly, listing two tables in FROM does a full join, so every row of message has the same query attached to it.\n\n\nRanking matches\nUsually, when we search a database we want the results to be listed in some kind of useful order. Intuitively, the closest match (containing all the words in our query in roughly the same order) should be on top, as should be documents that contain the query terms many times. A document containing the query terms, but very far apart from each other and only once each, should appear near the end of the list of results.\nWe might also want to weight parts of the text differently. Perhaps a search query should apply to both the title and the text of a document, but matches in the title should be more important than matches in the text.\nIn short, we need a ranking function that supports weights. PostgreSQL provides two:\n\nts_rank() ranks matches by how many lexemes in the document match the query\nts_rank_cd() also accounts for how close the matching lexemes are to each other\n\nThe ranks are calculated so that larger ranks mean more relevant matches, so we sort in descending order. For example,\nSELECT mid, sender, subject, ts_rank_cd(body_index_col, query) AS rank\nFROM message, websearch_to_tsquery('monopoly competition') AS query\nWHERE body_index_col @@ query\nORDER BY rank DESC\nLIMIT 10;\nThese ranks do not account for the length of the document. If the document is long and hence includes the matching lexemes many times, it will rank higher than a shorter document that includes them at the same rate. That may not be desirable, so the optional third argument to ts_rank() and ts_rank_cd() selects a normalization method. The default is 0, which does no normalization, 1 normalizes by the log of the document length while 2 normalizes by the length. (There are several additional options and combinations of normalizations can be used; see section 12.3.3 of the manual for details).\nTo make a search index column that does a weighted search of multiple fields, such as the subject and body of an email, we can change how we define the index column:\nALTER TABLE tablename\nADD COLUMN column_index_col tsvector\nGENERATED ALWAYS AS (\n  setweight(to_tsvector('english', coalesce(important_column, '')), 'A') ||\n  setweight(to_tsvector('english', coalesce(less_important_column, '')), 'D')\n)\nSTORED;\n\nCREATE INDEX column_idx ON tablename USING GIN (column_index_col);\nWeights are the letters A through D, where A has highest weight and D has the lowest. Notice we use coalesce() so that if the column value is NULL, it’s replaced with the empty string; and we can use the || operator to concatenate tsvector values, so the index column contains the combined results for both columns."
  },
  {
    "objectID": "data-engineering/text-search.html#exercises",
    "href": "data-engineering/text-search.html#exercises",
    "title": "Full Text Search",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 5 (Find the incriminating evidence) Part of Enron’s fraud involved recording transactions early so the revenue would be included in the current quarter’s financial reports, increasing the reported quarterly revenue. (Some of that revenue wasn’t real either.) Search the Enron messages for emails by employees containing topics about revenue, quarters, and loans. Present your results in a table with the name of the sender, the subject line of the email, and its date. You can adjust your query to give what you think are the most interesting results.\n(Employee names are available in the employeelist table, which lists names and email addresses for known Enron employees.)\n\n\nSolution. TODO"
  },
  {
    "objectID": "data-engineering/text-search.html#footnotes",
    "href": "data-engineering/text-search.html#footnotes",
    "title": "Full Text Search",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAren’t bytes 8 bits each? Yes. But ASCII’s designers were worried about data transmission and storage, which both were expensive. Dropping a bit cut costs by 1/8th. Using 7 bits also allowed use of the 8th bit as an error correcting code to detect errors when text was transmitted between computers.↩︎\nIn case your company ever considers this, consider NIST SP 800-63A, a government standard describing security practices. Section 3.1.1.2 states that government login systems “SHALL NOT impose other composition rules (e.g., requiring mixtures of different character types) for passwords”, because (as Appendix A.3 notes) these rules usually result in people picking dumb passwords like “password1”.↩︎"
  },
  {
    "objectID": "data-engineering/sql.html",
    "href": "data-engineering/sql.html",
    "title": "SQL Basics",
    "section": "",
    "text": "Note\n\n\n\nPortions of these notes are based on material co-developed with Christopher Genovese for 36-750.\n\n\nLet’s learn how to write basic SQL queries. We’ll start by writing the queries by hand, and instead of using Python to submit them, we’ll use the psql command-line program. This program allows you to type queries and get responses back from the server.\nIf you need help writing SQL queries, finding functions in Postgres, and so on, consult the PostgreSQL manual. It is among the best-written and most comprehensive manuals you can get for open-source software.\n\n\nThe Department runs a PostgreSQL server in Microsoft Azure for this course. You should receive an automated email with your username and password.\n\n\nA simple way to work with PostgreSQL in Azure is Microsoft’s Azure Data Studio. Download and install it. Once you’ve installed it, find the Extensions button on the left and search for the PostgreSQL extension by Microsoft, and install it.\nYou can now connect to our PostgreSQL server with the following information:\n\n\n\nField\nValue\n\n\n\n\nConnection type\nPostgreSQL\n\n\nServer name\npinniped.postgres.database.azure.com\n\n\nAuthentication type\nPassword\n\n\nUser name\nYour username\n\n\nPassword\nYour password\n\n\nDatabase\nSelect either your database or examples\n\n\n\nOnce connected, you’ll be able to see all the databases and tables your account can access on the left. The examples database is for examples used in class, and you can read it but not modify it. You have your own database, under your Andrew ID, that you can work in.\nWhen you connect, you’ll get a query window (or can create one with File→New Query) for writing queries. You can select at the top which database to run those queries in.\nYou can also create a notebook (based on Jupyter notebooks) to access a specific database. For example, open the list of databases on the left, right click on examples, and select “New Notebook”. You will use these notebooks for your homework.\nNote: You will not be able to connect through the eduroam wireless network, since it blocks PostgreSQL connections. If you’re on campus, always use CMU-SECURE.\n\n\n\nSQL consists of a sequence of statements.\nEach statement is built around a specific command, like SELECT or INSERT, with a variety of modifiers and optional clauses (like WHERE or LIMIT).\nCommand names and modifiers are not case-sensitive. You can write SELECT, select, or sEleCt. It is standard style to use upper case command names and lower case for table and column names.\nSQL statements can span several lines, and all SQL statements end in a semi-colon (;).\nKeep in mind: strings are delimited by single quotes 'like this', not double quotes \"like this\". To write a single quote in a string, write it twice: 'can''t' represents the string can't.\nSQL comments are lines starting with --.\n\n\n\nYour SQL homework assignments should be completed as notebooks in Azure Data Studio. Use File→New Notebook to create one, and connect it to the right database. (These are a lot like Jupyter notebooks, or the notebooks you may have used in Visual Studio Code in 36-650.)\nUse text cells to create headings labeling each problem, and to include text answers to any questions that ask for your comments. Use SQL cells for your queries. Use LIMIT clauses on queries returning lots of rows so the output isn’t enormous—we don’t want to scroll through 50 pages of results to find your next homework problem.\nSave the notebook as a Jupyter .ipynb file. When you’re ready to submit your homework, upload the notebook to Gradescope.\n\n\n\n\nTo learn how to write queries, we’ll start with a database already filled with example data. This database is for a hypothetical online learning platform where students see material and then take short quizzes. We’ll start by looking at the events table, which records every time a student interacted with some element of the system, such as a quiz item. Table 1 shows a few example rows.\n\n\n\nTable 1: A few rows from the events table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntime\npersona\nelement\nlatency\nscore\nfeedback\n\n\n\n\n17\n2015-07-11 09:42:11\n3271\n97863\n329.4\n240\nConsider…\n\n\n18\n2015-07-11 09:48:37\n3271\n97864\n411.9\n1000\n\n\n\n19\n2015-07-08 11:22:01\n499\n104749\n678.2\n750\nThe mean is…\n\n\n22\n2015-07-30 08:44:22\n6742\n7623\n599.7\n800\nTry to think of…\n\n\n24\n2015-08-04 23:56:33\n1837\n424933\n421.3\n0\nPlease select…\n\n\n32\n2015-07-11 10:11:07\n499\n97863\n702.1\n820\nWhat does the…\n\n\n99\n2015-07-22 16:11:27\n24\n88213\n443.0\n1000\n\n\n\n\n\n\n\nYou can load this data into your own database. Download events.sql and load it in Azure Data Studio. You will see a file full of SQL queries that create the table. At the top, select your database connection, then press Run to run all the queries and create the table.\n\n\n\nThe SELECT command is how we query the database. It is versatile and powerful command.\nThe simplest query is to look at all rows and columns of a table:\nSELECT * FROM events;\nThe * is a shorthand for “all columns.”\nSelects can include expressions, not just column names, as the quantities selected. And we can use as clauses to name (or rename) the results.\nSELECT 1 AS one;\nSELECT persona AS person FROM events;\nMost importantly, we can qualify our queries with conditions that refine the selection. We do this with the WHERE clause, which accepts a logical condition on any expression and selects only those rows that satisfy the condition. The conditional expression can include column names (even temporary ones) as variables.\nSELECT * FROM events WHERE id &gt; 20 AND id &lt; 40;\nWe can also order the output using the ORDER BY clause:\nSELECT score, element FROM events\nWHERE persona = 1150\nORDER BY element, score;\nSpecifying two columns (element, score) means the results will be sorted by the first column, and any ties broken using the second column.\nWe can limit the number of results using LIMIT:\nSELECT score, element FROM events\nWHERE persona = 1150\nORDER BY element, score\nLIMIT 10;\n\nExercise 1 (Basic SELECT queries) Craft queries to do the following in the events table:\n\nList all ids, persona, score where a score &gt; 900 occurred.\nList all persona (sorted numerically) who score &gt; 900. Can you eliminate duplicates here? (Hint: Consider SELECT DISTINCT.)\n\n\n\nSolution. \nSELECT id, persona, score\nFROM events\nWHERE score &gt; 900;\n\nSELECT DISTINCT persona\nFROM events\nWHERE score &gt; 900\nORDER BY persona;\n\n\n\nSQL queries can contain expressions, such as multiplication and addition. For example:\nSELECT 2 * score, element FROM events\nWHERE persona &lt; 1201\nLIMIT 2;\nRelational databases also typically have a library of built-in functions that can do various useful things. The chapter on built-in functions and operators in the PostgreSQL manual lists numerous such functions: mathematical functions, string concatenation and formatting, date and time operations, comparisons, …\nThese functions can be used anywhere you’d expect a value. For example:\nSELECT log(score) AS log_score, element,\n       upper(feedback) AS shouty_feedback\nFROM events\nLIMIT 10;\nYou can think of these functions as being inherently vectorized: given a column of data, they return an entire column of results, much like applying a function to a column of a Pandas data frame.\n\n\n\nEach value in a SQL database has a type controlling the operations you can perform with it: text, numbers, dates, and so on.\nJust as expressions in Python or R can “cast” data from one type to another (like as.character(7) in R, or str(7) in Python), in SQL you can cast to different data types. The CAST() function does this. For example, to convert a string specifying a timestamp to a timestamp object, we can write\nSELECT CAST('2020-10-01 11:00:00' AS TIMESTAMP);\nThe string now has the timestamp type, so functions that work on timestamps (listed in the chapter on date/time functions) can work on it:\nSELECT date_part('dow', CAST('2020-10-01 11:00:00' AS TIMESTAMP));\nThe database will automatically reject operations on the wrong type of data, or that try to insert the wrong type into a column.\nOne special value is NULL. Most data types can be NULL, which represents a missing value, must like NA in R or None in Python. By default, most types allow NULL values, so if you want to enforce that a value must always be provided, you must declare the column to be NOT NULL (see Section 1.7 below).\n\n\n\nSometimes we want to calculate something from many rows, not just one row. For example, most databases have count(), max(), min(), and sum() functions. These are all aggregate functions, meaning they take many values and produce a single value.\nFor example, let’s calculate the average score obtained by students:\nSELECT avg(score) FROM events;\nThis produces a single row: the average score. Any aggregate function takes many rows and reduces them to a single row. This is why you can’t write this:\nSELECT persona, avg(score) FROM events;\nTry it; why does Postgres complain?\nWe often want to apply aggregate functions not just to whole columns but to groups of rows within columns. This is the province of the GROUP BY clause. It groups the data according to a specific value, and aggregate functions then produce a single result per group.\nFor example, if I wanted the average score for each separate user, I could write:\nSELECT persona, avg(score) AS mean_score\nFROM events\nGROUP BY persona\nORDER BY mean_score desc;\nYou can apply conditions on the groups. While WHERE filters the individual rows, HAVING filters the groups after aggregation. For example, we can find groups with average score above 50:\nSELECT persona, avg(score) AS mean_score\nFROM events\nWHERE moment &gt; CAST('2014-10-01 11:00:00' AS timestamp)\nGROUP by persona\nHAVING avg(score) &gt; 300\nORDER BY mean_score DESC;\n\nExercise 2 (Grouping and aggregating) Using the events table,\n\nList all personas whose maximum latency is greater than 575, and calculate their average scores. Sort the results by increasing average score.\nList all persona whose average score &gt; 600. You will need to do a GROUP BY as above. (Hint: use HAVING instead of WHERE for the aggregate condition.)\nProduce a table showing how many times each instructional element was practiced. The COUNT() aggregate function counts the number of rows in each group.\nList all personas and, as a string, the month and year of their first event (by moment) in the table. (For example, “January 2014”.) Limit this to the first 10 results.\nHint: Check the PostgreSQL documentation on data type formatting to see how to convert a timestamp to a string, particularly tables 9.27 and 9.31.\n\n\n\nSolution. \nSELECT persona, avg(score) AS mean_score\nFROM events\nGROUP BY persona\nHAVING MAX(latency) &gt; 575\nORDER BY mean_score;\n\nSELECT persona\nFROM events\nGROUP BY persona\nHAVING avg(score) &gt; 600;\n\nSELECT element, count(element)\nFROM events\nGROUP by element\nORDER by element;\n\nSELECT persona, to_char(min(moment), 'FMMonth YYYY')\nFROM events\nGROUP BY persona\nLIMIT 10;\n\n\n\n\n\nThe INSERT command specifies data to insert.\nThe basic template is\nINSERT INTO &lt;tablename&gt; (&lt;column1&gt;, ..., &lt;columnk&gt;)\n       VALUES (&lt;value1&gt;, ..., &lt;valuek&gt;);\nIf the column names are excluded, then values for all columns must be provided. You can use DEFAULT in place of a value for a column with a default setting.\nYou can also insert multiple rows at once:\nINSERT INTO &lt;tablename&gt; (&lt;column1&gt;, ..., &lt;columnk&gt;)\n       VALUES (&lt;value11&gt;, ..., &lt;value1k&gt;),\n              (&lt;value21&gt;, ..., &lt;value2k&gt;),\n              ...\n              (&lt;valuem1&gt;, ..., &lt;valuemk&gt;);\nFor example:\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1211, 29353, 824, 'C', 'How do the mean and median differ?');\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1207, 29426, 1000, 'A', 'You got it!');\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1217, 29433,  842, 'C', 'Try simplifying earlier.'),\n              (1199, 29435,    0, 'B', 'Your answer was blank'),\n              (1207, 29413, 1000, 'C', 'You got it!'),\n              (1207, 29359,  200, 'A', 'A square cannot be negative');\nBy default, INSERT does not return a table of results. But you can ask it to:\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1217, 29433,  842, 'C', 'Try simplifying earlier.'),\n              (1199, 29435,    0, 'B', 'Your answer was blank'),\n              (1207, 29413, 1000, 'C', 'You got it!'),\n              (1207, 29359,  200, 'A', 'A square cannot be negative')\nRETURNING id;\nThis query returns a table with 4 rows and 1 column: the id column that is automatically filled in (because it is SERIAL). You can return one or more columns or expressions in RETURNING: it takes a list of columns just like SELECT.\n\nExercise 3 (Inserting invalid data) Write a query to insert data into the events table, but provide the wrong data type for one column (such as a string instead of an integer). Report the message you receive. Which of the ACID guarantees (?@sec-acid-guarantees) is the database enforcing here?\n\n\nSolution. Postgres is enforcing consistency: the data must satisfy all defined constraints at all times.\nAny query that inserts invalid data is fine here. Note that Postgres will automatically do some kind of casts: for example, inserting the string '100' for the score is fine, because that string can be automatically converted to a number, and Postgres will not complain. Only if Postgres does not know how to do the conversion, or if the conversion fails, will it complain. Hence inserting 'ducks' into a numeric column will not work.\n\n\n\n\nThe UPDATE command allows us to modify existing entries in any way we like. The basic syntax looks like this:\nUPDATE table\n    SET col1 = expression1,\n        col2 = expression2,\n        ...\n    WHERE condition;\nBeware: If you omit the WHERE clause, the update will be applied to every row.1\nThe UPDATE command can update one or more columns at once.\nAs with INSERT, you can use RETURNING with UPDATE queries. Here it returns one row per row that was modified, and you can choose the columns returned.\n\nExercise 4 (Updating data) In the events table,\n\nSet the answer for events with id &gt; 800 to the letter C.\nUpdate the scores to subtract 50 points for every hint taken (in the hints column). Ensure that the score stays above 0. Use RETURNING to get the id and the new score for each updated event.\nHint: The GREATEST() function returns the maximum of its arguments, so GREATEST(-5, 0, -2) is 0.\n\n\n\nSolution. \nUPDATE events\nSET answer = 'C'\nWHERE id &gt; 800;\n\nUPDATE events\nSET score = GREATEST(score - 50 * hints, 0)\nRETURNING id, score AS new_score;\n\n\n\n\nThe DELETE command allows you to remove rows from a table that satisfy a condition. The basic syntax is:\nDELETE FROM table WHERE condition;\nBeware: If you omit the WHERE clause, all rows in the table will be deleted immediately.\nAs before, you can use RETURNING to get results from DELETE. There will be one row per deleted row.\n\nExercise 5 (Gems data) The file gems.sql contains the commands needed to create a table called gems filled with some random data. Connect to your personal database. Open the file and run the queries inside to create the table.\nTry\nSELECT * FROM gems LIMIT 10;\nto see a few examples of the data in the new gems table.\nNow try writing some queries:\n\nSet the label to 'thin' for all gems with fewer than 10 facets.\nSet the label to 'wow' and the price to $100 for all gems with more than 20 facets.\n(The price column is defined to be of type money. To convert a number to money, you can write cast('100.0' as money). The money type is useful because it represents a number with fixed decimal precision; by default, this is two decimal digits. It stores these exactly, unlike floating-point numbers, which can’t represent some simple decimals.)\nDelete all gems with fewer than four facets.\n\n\n\nSolution. For reference, here is how the table was created:\nCREATE TABLE gems (label TEXT DEFAULT '',\n                   facets INTEGER DEFAULT 0,\n                   price MONEY);\n\nINSERT INTO gems\n(SELECT '', ceiling(20 * random() + 1), '1.00'::money\n FROM generate_series(1, 20) AS k);\n\nUPDATE gems SET label = ('{thin,quality,wow}'::text[])[ceil(random() * 3)];\nHere are the queries:\nUPDATE gems SET label = 'thin'\nWHERE facets &lt; 10;\n\nUPDATE gems SET label = 'wow', price = CAST('100.00' AS money)\nWHERE facets &gt;= 20;\n\nDELETE FROM gems WHERE facets &lt; 4;\n\n\n\n\n\n\nBefore making a table, we must choose what columns it should have, what they will be named, and what types they have.\nWe’ll discuss the selection of columns in more detail in ?@sec-schema-design. Broadly, we choose the columns to describe whatever entity is in the table: if each row is a student, the columns should be features of the students. Their names should be reasonably descriptive without being so long as to make typing queries a pain.\nTypes require careful choice. PostgreSQL supports dozens of types. The most common are:\n\nBooleans: boolean can be either true or false (or yes and no, on or off, or 1 and 0)\nNumeric\n\ninteger for integers\nnumeric for arbitrary-precision numbers (i.e. to any number of decimal places – making them slow to manipulate)\ndouble precision (or just double) for 64-bit floating point numbers\nserial for an integer column whose default value is always the last value plus 1 (great for IDs)\nmoney for monetary amounts with fixed precision\n\nStrings\n\ntext is the general-purpose text type, for strings up to any length. This should be your default choice in PostgreSQL. (However, the SQL standard does not include text, so some databases don’t have it, forcing everyone to use varchar.)\nchar(n) gives a fixed-length string of a certain length, padded with spaces, so char(10) always is 10 characters long\nvarchar(n) gives variable-length strings up to a certain length, so varchar(10) can be any length up to 10 characters.\n\nDates and times\n\ntimestamp records a specific point in time with date and time. Optionally, timestamp with time zone records the time zone associated with the event.\ndate gives a date with no time of day\ntime gives a time of day (from midnight to midnight), but no date, optionally with a time zone.\ninterval represents the interval between two timestamps, and is what you get when you subtract two timestamps\n\nEnumerated types are user-created. Each value takes one of several defined options, much like a factor in R.\n\nThere are many other types and options; view the full Postgres list for more.\n\n\n\nTo create a new table, we use the CREATE TABLE command. In its most basic form, it looks like\nCREATE TABLE name (attribute1 TYPE1, attribute2 TYPE2, ...);\nA simple products table specifying products for sale is:\nCREATE TABLE products (\n       product_id INTEGER,\n       name TEXT,\n       price DOUBLE,\n       sale_price DOUBLE\n);\nHere we have chosen five columns and specified their types, out of the list of types supported by PostgreSQL.\nHere’s a fancier version:\nCREATE TABLE products (\n       product_id SERIAL PRIMARY KEY,\n       name TEXT,\n       price NUMERIC CHECK (price &gt; 0),\n       sale_price NUMERIC CHECK (sale_price &gt; 0),\n       CHECK (price &gt; sale_price)\n);\nThe SERIAL type specifies that product_id is automatically set to an increasing integer whenever we add a row. We have also specified it is the primary key, meaning it uniquely identifies rows in this table. We’ve also set constraints on price, sale price, and their relationship.\nPostgreSQL will reject any data of the wrong type or violating the constraints:\nINSERT INTO products (name, price, sale_price)\nVALUES ('duck', 'duck', 'goose');\n\nINSERT INTO products (name, price, sale_price)\nVALUES ('kirk action figure', 50, 52);\nWe can also specify default values using DEFAULT and require entries to not be null. Here’s an even fancier products table:\nCREATE TABLE products (\n       product_id SERIAL PRIMARY KEY,\n       name TEXT NOT NULL,\n       quantity INTEGER DEFAULT 0,\n       price NUMERIC CHECK (price &gt; 0),\n       sale_price NUMERIC CHECK (sale_price &gt; 0),\n       CHECK (price &gt; sale_price)\n);\n\nExercise 6 (Creating a table) Write the CREATE TABLE command for a table called students containing the following columns:\n\nStudent ID, a unique integer\nStudent name, which cannot be null\nDate of birth (as a date without a time), which must be before the present\nA boolean flag for whether each student has completed orientation, false by default\nThe tuition charged for that student, which must be greater than 0\nThe scholarship amount that student receives, which must be less than the tuition amount, and is 0 by default\n\nYou may choose appropriate column names, and should choose appropriate data types from the many types supported by PostgreSQL. Chapter 5 of the manual gives great detail on defining tables, constraints, defaults, and other features.\nWrite an INSERT query that adds four example students to the table. Write another INSERT query that attempts to insert invalid data, proving that your definition enforces the criteria above.\n\n\nSolution. Names may differ, but the general idea is this:\nCREATE TABLE students (\n    id SERIAL PRIMARY KEY,\n    name text NOT NULL,\n    birthdate date CHECK (birthdate &lt; now()),\n    completed_orientation boolean DEFAULT FALSE,\n    tuition money CHECK (tuition &gt; CAST(0 AS money)),\n    scholarship money,\n    CHECK (scholarship &lt;= tuition)\n);\nThe crucial parts are that\n\nid is a serial primary key, so it is unique and increasing\nname is marked NOT NULL,\nbirthdate has a check constraint\ncompleted_orientation defaults to FALSE\ntuition has a check constraint\na check constraint ensures the scholarship amount is valid.\n\nFor example,\nINSERT INTO students (name, birthdate, completed_orientation, tuition, scholarship)\nVALUES\n('Englebert Humperdinck', CAST('1936-05-02' AS date), FALSE, 36000, 15000);\nWhereas this doesn’t work:\nINSERT INTO students (name, birthdate, completed_orientation, tuition, scholarship)\nVALUES\n('Daffy Duck', CAST('2038-01-03' AS date), TRUE, 39, 40);\n\n\n\n\nThe ALTER TABLE command allows you to change a variety of table features. This includes adding and removing columns, renaming attributes, changing constraints or attribute types, and setting column defaults. See the full documentation for more.\nA few examples using the most recent definition of products:\n\nLet’s rename product_id to just id for simplicity.\nALTER TABLE products\n      RENAME product_id TO id;\nLet’s add a brand_name column.\nALTER TABLE products ADD brand_name TEXT DEFAULT 'generic' NOT NULL;\nLet’s drop the discount column\nALTER TABLE products DROP discount;\nLet’s set a default value for brand_name.\nALTER TABLE products\n      ALTER brand_name SET DEFAULT 'generic';\n\n\n\n\nThe command is DROP TABLE.\nDROP TABLE products;\nTry it, then type \\d at the prompt.\nBeware: This command is immediate and permanent (unless you have a backup).\n\n\n\n\nAs we will see shortly, principles of good database design tell us that tables represent distinct entities with a single authoritative copy of relevant data. This is the DRY principle (Don’t Repeat Yourself) in action, in this case eliminating data redundancy.\nAn example of this in the events table are the persona and element columns, which point to information about students and components of the learning environment. We do not repeat the student’s information each time we refer to that student. Instead, we use a link to the student that points into a separate personae table. Table 2 shows a few rows of this table.\n\n\n\nTable 2: The first few rows of the personae table.\n\n\n\n\n\nid\nfirstname\nlastname\nbirthdate\naccount_balance\n\n\n\n\n1\nArnold\nPerlstein\n1988-06-02\n$1700.02\n\n\n2\nCarlos\nRamon\n1988-04-27\n$0.00\n\n\n3\nDorothy\nHudson\n1989-01-06\n$-406.79\n\n\n4\nKeesha\nFranklin\n1988-03-15\n$0.00\n\n\n\n\n\n\nBut if our databases are to stay DRY in this way, we need two things:\n\nA way to define links between tables (and thus define relationships between the corresponding entities), and to enforce that these links are valid.\nAn efficient way to combine information across these links.\n\nThe former is supplied by foreign keys and the latter by joins. We will tackle both in turn.\n\n\nA foreign key is a field (or collection of fields) in one table that uniquely specifies a row in another table. We specify foreign keys in PostgreSQL using the REFERENCES keyword when we define a column or table. A foreign key that references another table must be the value of a unique key in that table, though it is most common to reference a primary key.\nFor example, the events table was defined using REFERENCES:\nCREATE TABLE events (\n    id SERIAL PRIMARY KEY,\n    moment TIMESTAMP,\n    persona INTEGER REFERENCES personae (id),\n    -- more columns go here:\n    ...\n);\nForeign keys can also be added (and altered) as table constraints that look like FOREIGN KEY (&lt;key&gt;) REFERENCES &lt;table&gt;.\nNow try this\nINSERT INTO events (persona, element, score)\nVALUES (-1, 123, 1000);\nNotice that the insertion did not work—-and the entire transaction was rolled back—because the foreign key constraint was violated. There was no persona with ID -1.\nSo let’s fix it. Try it!\nINSERT INTO personae VALUES (-1, 'Englebert', 'Humperdinck', '1936-05-02', 100.0);\n\nINSERT INTO events (persona, element, score)\nVALUES (-1, 123, 1000);\nA foreign key is allowed to be NULL, meaning it points to nothing, unless you include a NOT NULL constraint in the column definition.\n\n\n\nSuppose we want to display features of an event with the name and course of the student who generated it. If we’ve kept to DRY design and used a foreign key for the persona column, this seems inconvenient.\nThat is the purpose of a join. For instance, we can write:\nSELECT personae.lastname, personae.firstname, events.score, events.moment\nFROM events\nJOIN personae ON events.persona = personae.id\nWHERE moment &gt; CAST('2015-03-26 08:00:00' AS timestamp)\nORDER BY moment;\nJoins incorporate additional tables into a select. This is done by appending to the from clause:\nFROM &lt;table&gt; JOIN &lt;table&gt; ON &lt;condition&gt; ...\nwhere the on condition specifies which rows of the different tables are included. And within the select, we can disambiguate columns by referring them to by &lt;table&gt;.&lt;column&gt;. Look at the example above with this in mind.\nYou can think of FROM &lt;table&gt; JOIN &lt;table&gt; as producing a new virtual table formed by the two joined together, and the SELECT operates on this. You can join this virtual table to others: FROM &lt;table&gt; JOIN &lt;table&gt; ON &lt;condition&gt; JOIN &lt;table&gt; and so on.\nAfter joining two or more tables, you can operate on the resulting table just like in any other queries, with grouping, aggregates, and so on.\nWe will start by seeing what joins mean in a simple case, using these tables:\nCREATE TABLE a (id SERIAL PRIMARY KEY, name TEXT);\nINSERT INTO a (name)\n       VALUES ('Pirate'),\n              ('Monkey'),\n              ('Ninja'),\n              ('Flying Spaghetti Monster');\n\nCREATE TABLE b (id SERIAL PRIMARY KEY, name TEXT);\nINSERT INTO b (name)\n       VALUES ('Rutabaga'),\n              ('Pirate'),\n              ('Darth Vader'),\n              ('Ninja');\nLet’s look at several kinds of joins. (There are others, but this will get across the most common types.)\n\n\nAn inner join produces the rows for which attributes in both tables match. (If you just say JOIN in SQL, you get an inner join; the word INNER is optional.)\nSELECT * FROM a INNER JOIN b on a.name = b.name;\n\n\n\nA full outer join produces the full set of rows in all tables, matching where possible but NULL otherwise.\nSELECT * FROM a FULL OUTER JOIN b on a.name = b.name;\n\n\n\nA left outer join produces all the rows from A, the table on the “left” side of the join operator, along with matching rows from B if available, or null otherwise. (LEFT JOIN is a shorthand for LEFT OUTER JOIN in PostgreSQL.)\nSELECT * FROM a LEFT OUTER JOIN b ON a.name = b.name;\n\nExercise 7 (Set operations) Write queries to:\n\nSelect all the rows of A that are not in B.\nSelect the rows of A not in B and the rows of B not in A.\n\nHint: To check if a value is null, use value IS NULL.\n\n\nSolution. We can use outer joins. When a row in one table does not match a row in the other, the second table’s entries are NULL, so we can check for null in the results:\nSELECT * FROM a LEFT OUTER JOIN b on a.name = b.name where b.id IS NULL;\nSELECT * FROM b FULL OUTER JOIN b on a.name = b.name\n    WHERE b.id IS NULL OR b.id IS NULL;\n\n\nExercise 8 (Joining educational data) Besides the events and personae tables, our database also contains an elements table providing information about each instructional element (such as a quiz question) within the system. Here is its schema:\nCREATE TABLE elements (\n       id SERIAL PRIMARY KEY,\n       content TEXT,\n       points_max INTEGER CHECK (points_max &gt;= 0),\n       module INTEGER, -- references a modules table, if we had one\n       hint TEXT,\n       max_tries INTEGER CHECK (max_tries &gt; 0)\n);\nWrite queries to:\n\nList all events, with the score they received, the name of the student completing the event, their birthdate, and the maximum number of points possible.\nThe same, but only for events where the student’s score is higher than the number of points possible.\nList all elements IDs that have never been part of an event.\nCalculate the average score for all events for each element.\nSum the scores for all students born after June 1, 1988.\n\n\n\nSolution. TODO solutions\n\n\n\n\n\n\nWhen we know we will search on certain fields regularly, it can be helpful to create an index, which speeds up those particular searches. An index tells PostgreSQL to keep a data structure (typically a tree) for a particular column so that it can identify all rows matching a query without scanning the entire table.\nFor example, PostgreSQL automatically indexes any column declared as a PRIMARY KEY. In the events table, the id column is the primary key, so a query like\nSELECT * FROM events WHERE id &gt; 200 AND id &lt; 300;\nwill use the index. PostgreSQL will search the tree (in something like \\(O(\\log\nn)\\) time, where \\(n\\) is the number of rows) and pull the matching rows of the table, rather than looping over the entire table and checking every row.\nWhile PostgreSQL automatically indexes primary keys, we can also tell it explicitly to add indexes for specific columns. For example, if we know we’ll often be searching events by start time, we could do\nCREATE INDEX event_moment_index\nON events (moment);\nEach index has a name so that we can identify it when we use DROP INDEX to remove it. Postgres also uses these names if we ask it to explain how it will conduct a query. By default, this index is based on a B-tree, but there are other index types that are useful for other things. Hash indices, for instance, are very fast for equality (SELECT * FROM foo WHERE col == 'something'). See the indexes chapter of the manual for more information on the syntax and type of indexes.\nIndices are particularly important for tables with many thousands or millions of rows. Companies often have tables with gigabytes of data, and looping through every row to find matching results would be prohibitively slow. Worse, joining two large tables requires matching up rows from the two tables, and would be even slower. A clever selection of indices can make these operations much faster.\nMuch of the engineering work in database software is in the so-called query planner, which takes your query, information about the size of each table, and the available indices to determine the most efficient way to execute the query. (Should it filter the data first, then join? Or join and then filter? Which of several indices should it use to filter first, so it has less work to do for subsequent operations?)\nIt is possible to develop an entire career around building database tables, specifying their indices, and writing queries that both solve business questions and are extremely fast for enormous databases."
  },
  {
    "objectID": "data-engineering/sql.html#connecting-to-our-database-server",
    "href": "data-engineering/sql.html#connecting-to-our-database-server",
    "title": "SQL Basics",
    "section": "",
    "text": "The Department runs a PostgreSQL server in Microsoft Azure for this course. You should receive an automated email with your username and password.\n\n\nA simple way to work with PostgreSQL in Azure is Microsoft’s Azure Data Studio. Download and install it. Once you’ve installed it, find the Extensions button on the left and search for the PostgreSQL extension by Microsoft, and install it.\nYou can now connect to our PostgreSQL server with the following information:\n\n\n\nField\nValue\n\n\n\n\nConnection type\nPostgreSQL\n\n\nServer name\npinniped.postgres.database.azure.com\n\n\nAuthentication type\nPassword\n\n\nUser name\nYour username\n\n\nPassword\nYour password\n\n\nDatabase\nSelect either your database or examples\n\n\n\nOnce connected, you’ll be able to see all the databases and tables your account can access on the left. The examples database is for examples used in class, and you can read it but not modify it. You have your own database, under your Andrew ID, that you can work in.\nWhen you connect, you’ll get a query window (or can create one with File→New Query) for writing queries. You can select at the top which database to run those queries in.\nYou can also create a notebook (based on Jupyter notebooks) to access a specific database. For example, open the list of databases on the left, right click on examples, and select “New Notebook”. You will use these notebooks for your homework.\nNote: You will not be able to connect through the eduroam wireless network, since it blocks PostgreSQL connections. If you’re on campus, always use CMU-SECURE.\n\n\n\nSQL consists of a sequence of statements.\nEach statement is built around a specific command, like SELECT or INSERT, with a variety of modifiers and optional clauses (like WHERE or LIMIT).\nCommand names and modifiers are not case-sensitive. You can write SELECT, select, or sEleCt. It is standard style to use upper case command names and lower case for table and column names.\nSQL statements can span several lines, and all SQL statements end in a semi-colon (;).\nKeep in mind: strings are delimited by single quotes 'like this', not double quotes \"like this\". To write a single quote in a string, write it twice: 'can''t' represents the string can't.\nSQL comments are lines starting with --.\n\n\n\nYour SQL homework assignments should be completed as notebooks in Azure Data Studio. Use File→New Notebook to create one, and connect it to the right database. (These are a lot like Jupyter notebooks, or the notebooks you may have used in Visual Studio Code in 36-650.)\nUse text cells to create headings labeling each problem, and to include text answers to any questions that ask for your comments. Use SQL cells for your queries. Use LIMIT clauses on queries returning lots of rows so the output isn’t enormous—we don’t want to scroll through 50 pages of results to find your next homework problem.\nSave the notebook as a Jupyter .ipynb file. When you’re ready to submit your homework, upload the notebook to Gradescope."
  },
  {
    "objectID": "data-engineering/sql.html#sec-example-db",
    "href": "data-engineering/sql.html#sec-example-db",
    "title": "SQL Basics",
    "section": "",
    "text": "To learn how to write queries, we’ll start with a database already filled with example data. This database is for a hypothetical online learning platform where students see material and then take short quizzes. We’ll start by looking at the events table, which records every time a student interacted with some element of the system, such as a quiz item. Table 1 shows a few example rows.\n\n\n\nTable 1: A few rows from the events table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntime\npersona\nelement\nlatency\nscore\nfeedback\n\n\n\n\n17\n2015-07-11 09:42:11\n3271\n97863\n329.4\n240\nConsider…\n\n\n18\n2015-07-11 09:48:37\n3271\n97864\n411.9\n1000\n\n\n\n19\n2015-07-08 11:22:01\n499\n104749\n678.2\n750\nThe mean is…\n\n\n22\n2015-07-30 08:44:22\n6742\n7623\n599.7\n800\nTry to think of…\n\n\n24\n2015-08-04 23:56:33\n1837\n424933\n421.3\n0\nPlease select…\n\n\n32\n2015-07-11 10:11:07\n499\n97863\n702.1\n820\nWhat does the…\n\n\n99\n2015-07-22 16:11:27\n24\n88213\n443.0\n1000\n\n\n\n\n\n\n\nYou can load this data into your own database. Download events.sql and load it in Azure Data Studio. You will see a file full of SQL queries that create the table. At the top, select your database connection, then press Run to run all the queries and create the table."
  },
  {
    "objectID": "data-engineering/sql.html#selecting-data",
    "href": "data-engineering/sql.html#selecting-data",
    "title": "SQL Basics",
    "section": "",
    "text": "The SELECT command is how we query the database. It is versatile and powerful command.\nThe simplest query is to look at all rows and columns of a table:\nSELECT * FROM events;\nThe * is a shorthand for “all columns.”\nSelects can include expressions, not just column names, as the quantities selected. And we can use as clauses to name (or rename) the results.\nSELECT 1 AS one;\nSELECT persona AS person FROM events;\nMost importantly, we can qualify our queries with conditions that refine the selection. We do this with the WHERE clause, which accepts a logical condition on any expression and selects only those rows that satisfy the condition. The conditional expression can include column names (even temporary ones) as variables.\nSELECT * FROM events WHERE id &gt; 20 AND id &lt; 40;\nWe can also order the output using the ORDER BY clause:\nSELECT score, element FROM events\nWHERE persona = 1150\nORDER BY element, score;\nSpecifying two columns (element, score) means the results will be sorted by the first column, and any ties broken using the second column.\nWe can limit the number of results using LIMIT:\nSELECT score, element FROM events\nWHERE persona = 1150\nORDER BY element, score\nLIMIT 10;\n\nExercise 1 (Basic SELECT queries) Craft queries to do the following in the events table:\n\nList all ids, persona, score where a score &gt; 900 occurred.\nList all persona (sorted numerically) who score &gt; 900. Can you eliminate duplicates here? (Hint: Consider SELECT DISTINCT.)\n\n\n\nSolution. \nSELECT id, persona, score\nFROM events\nWHERE score &gt; 900;\n\nSELECT DISTINCT persona\nFROM events\nWHERE score &gt; 900\nORDER BY persona;\n\n\n\nSQL queries can contain expressions, such as multiplication and addition. For example:\nSELECT 2 * score, element FROM events\nWHERE persona &lt; 1201\nLIMIT 2;\nRelational databases also typically have a library of built-in functions that can do various useful things. The chapter on built-in functions and operators in the PostgreSQL manual lists numerous such functions: mathematical functions, string concatenation and formatting, date and time operations, comparisons, …\nThese functions can be used anywhere you’d expect a value. For example:\nSELECT log(score) AS log_score, element,\n       upper(feedback) AS shouty_feedback\nFROM events\nLIMIT 10;\nYou can think of these functions as being inherently vectorized: given a column of data, they return an entire column of results, much like applying a function to a column of a Pandas data frame.\n\n\n\nEach value in a SQL database has a type controlling the operations you can perform with it: text, numbers, dates, and so on.\nJust as expressions in Python or R can “cast” data from one type to another (like as.character(7) in R, or str(7) in Python), in SQL you can cast to different data types. The CAST() function does this. For example, to convert a string specifying a timestamp to a timestamp object, we can write\nSELECT CAST('2020-10-01 11:00:00' AS TIMESTAMP);\nThe string now has the timestamp type, so functions that work on timestamps (listed in the chapter on date/time functions) can work on it:\nSELECT date_part('dow', CAST('2020-10-01 11:00:00' AS TIMESTAMP));\nThe database will automatically reject operations on the wrong type of data, or that try to insert the wrong type into a column.\nOne special value is NULL. Most data types can be NULL, which represents a missing value, must like NA in R or None in Python. By default, most types allow NULL values, so if you want to enforce that a value must always be provided, you must declare the column to be NOT NULL (see Section 1.7 below).\n\n\n\nSometimes we want to calculate something from many rows, not just one row. For example, most databases have count(), max(), min(), and sum() functions. These are all aggregate functions, meaning they take many values and produce a single value.\nFor example, let’s calculate the average score obtained by students:\nSELECT avg(score) FROM events;\nThis produces a single row: the average score. Any aggregate function takes many rows and reduces them to a single row. This is why you can’t write this:\nSELECT persona, avg(score) FROM events;\nTry it; why does Postgres complain?\nWe often want to apply aggregate functions not just to whole columns but to groups of rows within columns. This is the province of the GROUP BY clause. It groups the data according to a specific value, and aggregate functions then produce a single result per group.\nFor example, if I wanted the average score for each separate user, I could write:\nSELECT persona, avg(score) AS mean_score\nFROM events\nGROUP BY persona\nORDER BY mean_score desc;\nYou can apply conditions on the groups. While WHERE filters the individual rows, HAVING filters the groups after aggregation. For example, we can find groups with average score above 50:\nSELECT persona, avg(score) AS mean_score\nFROM events\nWHERE moment &gt; CAST('2014-10-01 11:00:00' AS timestamp)\nGROUP by persona\nHAVING avg(score) &gt; 300\nORDER BY mean_score DESC;\n\nExercise 2 (Grouping and aggregating) Using the events table,\n\nList all personas whose maximum latency is greater than 575, and calculate their average scores. Sort the results by increasing average score.\nList all persona whose average score &gt; 600. You will need to do a GROUP BY as above. (Hint: use HAVING instead of WHERE for the aggregate condition.)\nProduce a table showing how many times each instructional element was practiced. The COUNT() aggregate function counts the number of rows in each group.\nList all personas and, as a string, the month and year of their first event (by moment) in the table. (For example, “January 2014”.) Limit this to the first 10 results.\nHint: Check the PostgreSQL documentation on data type formatting to see how to convert a timestamp to a string, particularly tables 9.27 and 9.31.\n\n\n\nSolution. \nSELECT persona, avg(score) AS mean_score\nFROM events\nGROUP BY persona\nHAVING MAX(latency) &gt; 575\nORDER BY mean_score;\n\nSELECT persona\nFROM events\nGROUP BY persona\nHAVING avg(score) &gt; 600;\n\nSELECT element, count(element)\nFROM events\nGROUP by element\nORDER by element;\n\nSELECT persona, to_char(min(moment), 'FMMonth YYYY')\nFROM events\nGROUP BY persona\nLIMIT 10;"
  },
  {
    "objectID": "data-engineering/sql.html#inserting-data",
    "href": "data-engineering/sql.html#inserting-data",
    "title": "SQL Basics",
    "section": "",
    "text": "The INSERT command specifies data to insert.\nThe basic template is\nINSERT INTO &lt;tablename&gt; (&lt;column1&gt;, ..., &lt;columnk&gt;)\n       VALUES (&lt;value1&gt;, ..., &lt;valuek&gt;);\nIf the column names are excluded, then values for all columns must be provided. You can use DEFAULT in place of a value for a column with a default setting.\nYou can also insert multiple rows at once:\nINSERT INTO &lt;tablename&gt; (&lt;column1&gt;, ..., &lt;columnk&gt;)\n       VALUES (&lt;value11&gt;, ..., &lt;value1k&gt;),\n              (&lt;value21&gt;, ..., &lt;value2k&gt;),\n              ...\n              (&lt;valuem1&gt;, ..., &lt;valuemk&gt;);\nFor example:\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1211, 29353, 824, 'C', 'How do the mean and median differ?');\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1207, 29426, 1000, 'A', 'You got it!');\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1217, 29433,  842, 'C', 'Try simplifying earlier.'),\n              (1199, 29435,    0, 'B', 'Your answer was blank'),\n              (1207, 29413, 1000, 'C', 'You got it!'),\n              (1207, 29359,  200, 'A', 'A square cannot be negative');\nBy default, INSERT does not return a table of results. But you can ask it to:\nINSERT INTO events (persona, element, score, answer, feedback)\n       VALUES (1217, 29433,  842, 'C', 'Try simplifying earlier.'),\n              (1199, 29435,    0, 'B', 'Your answer was blank'),\n              (1207, 29413, 1000, 'C', 'You got it!'),\n              (1207, 29359,  200, 'A', 'A square cannot be negative')\nRETURNING id;\nThis query returns a table with 4 rows and 1 column: the id column that is automatically filled in (because it is SERIAL). You can return one or more columns or expressions in RETURNING: it takes a list of columns just like SELECT.\n\nExercise 3 (Inserting invalid data) Write a query to insert data into the events table, but provide the wrong data type for one column (such as a string instead of an integer). Report the message you receive. Which of the ACID guarantees (?@sec-acid-guarantees) is the database enforcing here?\n\n\nSolution. Postgres is enforcing consistency: the data must satisfy all defined constraints at all times.\nAny query that inserts invalid data is fine here. Note that Postgres will automatically do some kind of casts: for example, inserting the string '100' for the score is fine, because that string can be automatically converted to a number, and Postgres will not complain. Only if Postgres does not know how to do the conversion, or if the conversion fails, will it complain. Hence inserting 'ducks' into a numeric column will not work."
  },
  {
    "objectID": "data-engineering/sql.html#updating-data",
    "href": "data-engineering/sql.html#updating-data",
    "title": "SQL Basics",
    "section": "",
    "text": "The UPDATE command allows us to modify existing entries in any way we like. The basic syntax looks like this:\nUPDATE table\n    SET col1 = expression1,\n        col2 = expression2,\n        ...\n    WHERE condition;\nBeware: If you omit the WHERE clause, the update will be applied to every row.1\nThe UPDATE command can update one or more columns at once.\nAs with INSERT, you can use RETURNING with UPDATE queries. Here it returns one row per row that was modified, and you can choose the columns returned.\n\nExercise 4 (Updating data) In the events table,\n\nSet the answer for events with id &gt; 800 to the letter C.\nUpdate the scores to subtract 50 points for every hint taken (in the hints column). Ensure that the score stays above 0. Use RETURNING to get the id and the new score for each updated event.\nHint: The GREATEST() function returns the maximum of its arguments, so GREATEST(-5, 0, -2) is 0.\n\n\n\nSolution. \nUPDATE events\nSET answer = 'C'\nWHERE id &gt; 800;\n\nUPDATE events\nSET score = GREATEST(score - 50 * hints, 0)\nRETURNING id, score AS new_score;"
  },
  {
    "objectID": "data-engineering/sql.html#deleting-data",
    "href": "data-engineering/sql.html#deleting-data",
    "title": "SQL Basics",
    "section": "",
    "text": "The DELETE command allows you to remove rows from a table that satisfy a condition. The basic syntax is:\nDELETE FROM table WHERE condition;\nBeware: If you omit the WHERE clause, all rows in the table will be deleted immediately.\nAs before, you can use RETURNING to get results from DELETE. There will be one row per deleted row.\n\nExercise 5 (Gems data) The file gems.sql contains the commands needed to create a table called gems filled with some random data. Connect to your personal database. Open the file and run the queries inside to create the table.\nTry\nSELECT * FROM gems LIMIT 10;\nto see a few examples of the data in the new gems table.\nNow try writing some queries:\n\nSet the label to 'thin' for all gems with fewer than 10 facets.\nSet the label to 'wow' and the price to $100 for all gems with more than 20 facets.\n(The price column is defined to be of type money. To convert a number to money, you can write cast('100.0' as money). The money type is useful because it represents a number with fixed decimal precision; by default, this is two decimal digits. It stores these exactly, unlike floating-point numbers, which can’t represent some simple decimals.)\nDelete all gems with fewer than four facets.\n\n\n\nSolution. For reference, here is how the table was created:\nCREATE TABLE gems (label TEXT DEFAULT '',\n                   facets INTEGER DEFAULT 0,\n                   price MONEY);\n\nINSERT INTO gems\n(SELECT '', ceiling(20 * random() + 1), '1.00'::money\n FROM generate_series(1, 20) AS k);\n\nUPDATE gems SET label = ('{thin,quality,wow}'::text[])[ceil(random() * 3)];\nHere are the queries:\nUPDATE gems SET label = 'thin'\nWHERE facets &lt; 10;\n\nUPDATE gems SET label = 'wow', price = CAST('100.00' AS money)\nWHERE facets &gt;= 20;\n\nDELETE FROM gems WHERE facets &lt; 4;"
  },
  {
    "objectID": "data-engineering/sql.html#sec-creating-schemas",
    "href": "data-engineering/sql.html#sec-creating-schemas",
    "title": "SQL Basics",
    "section": "",
    "text": "Before making a table, we must choose what columns it should have, what they will be named, and what types they have.\nWe’ll discuss the selection of columns in more detail in ?@sec-schema-design. Broadly, we choose the columns to describe whatever entity is in the table: if each row is a student, the columns should be features of the students. Their names should be reasonably descriptive without being so long as to make typing queries a pain.\nTypes require careful choice. PostgreSQL supports dozens of types. The most common are:\n\nBooleans: boolean can be either true or false (or yes and no, on or off, or 1 and 0)\nNumeric\n\ninteger for integers\nnumeric for arbitrary-precision numbers (i.e. to any number of decimal places – making them slow to manipulate)\ndouble precision (or just double) for 64-bit floating point numbers\nserial for an integer column whose default value is always the last value plus 1 (great for IDs)\nmoney for monetary amounts with fixed precision\n\nStrings\n\ntext is the general-purpose text type, for strings up to any length. This should be your default choice in PostgreSQL. (However, the SQL standard does not include text, so some databases don’t have it, forcing everyone to use varchar.)\nchar(n) gives a fixed-length string of a certain length, padded with spaces, so char(10) always is 10 characters long\nvarchar(n) gives variable-length strings up to a certain length, so varchar(10) can be any length up to 10 characters.\n\nDates and times\n\ntimestamp records a specific point in time with date and time. Optionally, timestamp with time zone records the time zone associated with the event.\ndate gives a date with no time of day\ntime gives a time of day (from midnight to midnight), but no date, optionally with a time zone.\ninterval represents the interval between two timestamps, and is what you get when you subtract two timestamps\n\nEnumerated types are user-created. Each value takes one of several defined options, much like a factor in R.\n\nThere are many other types and options; view the full Postgres list for more.\n\n\n\nTo create a new table, we use the CREATE TABLE command. In its most basic form, it looks like\nCREATE TABLE name (attribute1 TYPE1, attribute2 TYPE2, ...);\nA simple products table specifying products for sale is:\nCREATE TABLE products (\n       product_id INTEGER,\n       name TEXT,\n       price DOUBLE,\n       sale_price DOUBLE\n);\nHere we have chosen five columns and specified their types, out of the list of types supported by PostgreSQL.\nHere’s a fancier version:\nCREATE TABLE products (\n       product_id SERIAL PRIMARY KEY,\n       name TEXT,\n       price NUMERIC CHECK (price &gt; 0),\n       sale_price NUMERIC CHECK (sale_price &gt; 0),\n       CHECK (price &gt; sale_price)\n);\nThe SERIAL type specifies that product_id is automatically set to an increasing integer whenever we add a row. We have also specified it is the primary key, meaning it uniquely identifies rows in this table. We’ve also set constraints on price, sale price, and their relationship.\nPostgreSQL will reject any data of the wrong type or violating the constraints:\nINSERT INTO products (name, price, sale_price)\nVALUES ('duck', 'duck', 'goose');\n\nINSERT INTO products (name, price, sale_price)\nVALUES ('kirk action figure', 50, 52);\nWe can also specify default values using DEFAULT and require entries to not be null. Here’s an even fancier products table:\nCREATE TABLE products (\n       product_id SERIAL PRIMARY KEY,\n       name TEXT NOT NULL,\n       quantity INTEGER DEFAULT 0,\n       price NUMERIC CHECK (price &gt; 0),\n       sale_price NUMERIC CHECK (sale_price &gt; 0),\n       CHECK (price &gt; sale_price)\n);\n\nExercise 6 (Creating a table) Write the CREATE TABLE command for a table called students containing the following columns:\n\nStudent ID, a unique integer\nStudent name, which cannot be null\nDate of birth (as a date without a time), which must be before the present\nA boolean flag for whether each student has completed orientation, false by default\nThe tuition charged for that student, which must be greater than 0\nThe scholarship amount that student receives, which must be less than the tuition amount, and is 0 by default\n\nYou may choose appropriate column names, and should choose appropriate data types from the many types supported by PostgreSQL. Chapter 5 of the manual gives great detail on defining tables, constraints, defaults, and other features.\nWrite an INSERT query that adds four example students to the table. Write another INSERT query that attempts to insert invalid data, proving that your definition enforces the criteria above.\n\n\nSolution. Names may differ, but the general idea is this:\nCREATE TABLE students (\n    id SERIAL PRIMARY KEY,\n    name text NOT NULL,\n    birthdate date CHECK (birthdate &lt; now()),\n    completed_orientation boolean DEFAULT FALSE,\n    tuition money CHECK (tuition &gt; CAST(0 AS money)),\n    scholarship money,\n    CHECK (scholarship &lt;= tuition)\n);\nThe crucial parts are that\n\nid is a serial primary key, so it is unique and increasing\nname is marked NOT NULL,\nbirthdate has a check constraint\ncompleted_orientation defaults to FALSE\ntuition has a check constraint\na check constraint ensures the scholarship amount is valid.\n\nFor example,\nINSERT INTO students (name, birthdate, completed_orientation, tuition, scholarship)\nVALUES\n('Englebert Humperdinck', CAST('1936-05-02' AS date), FALSE, 36000, 15000);\nWhereas this doesn’t work:\nINSERT INTO students (name, birthdate, completed_orientation, tuition, scholarship)\nVALUES\n('Daffy Duck', CAST('2038-01-03' AS date), TRUE, 39, 40);\n\n\n\n\nThe ALTER TABLE command allows you to change a variety of table features. This includes adding and removing columns, renaming attributes, changing constraints or attribute types, and setting column defaults. See the full documentation for more.\nA few examples using the most recent definition of products:\n\nLet’s rename product_id to just id for simplicity.\nALTER TABLE products\n      RENAME product_id TO id;\nLet’s add a brand_name column.\nALTER TABLE products ADD brand_name TEXT DEFAULT 'generic' NOT NULL;\nLet’s drop the discount column\nALTER TABLE products DROP discount;\nLet’s set a default value for brand_name.\nALTER TABLE products\n      ALTER brand_name SET DEFAULT 'generic';\n\n\n\n\nThe command is DROP TABLE.\nDROP TABLE products;\nTry it, then type \\d at the prompt.\nBeware: This command is immediate and permanent (unless you have a backup)."
  },
  {
    "objectID": "data-engineering/sql.html#sec-joins-and-foreign-keys",
    "href": "data-engineering/sql.html#sec-joins-and-foreign-keys",
    "title": "SQL Basics",
    "section": "",
    "text": "As we will see shortly, principles of good database design tell us that tables represent distinct entities with a single authoritative copy of relevant data. This is the DRY principle (Don’t Repeat Yourself) in action, in this case eliminating data redundancy.\nAn example of this in the events table are the persona and element columns, which point to information about students and components of the learning environment. We do not repeat the student’s information each time we refer to that student. Instead, we use a link to the student that points into a separate personae table. Table 2 shows a few rows of this table.\n\n\n\nTable 2: The first few rows of the personae table.\n\n\n\n\n\nid\nfirstname\nlastname\nbirthdate\naccount_balance\n\n\n\n\n1\nArnold\nPerlstein\n1988-06-02\n$1700.02\n\n\n2\nCarlos\nRamon\n1988-04-27\n$0.00\n\n\n3\nDorothy\nHudson\n1989-01-06\n$-406.79\n\n\n4\nKeesha\nFranklin\n1988-03-15\n$0.00\n\n\n\n\n\n\nBut if our databases are to stay DRY in this way, we need two things:\n\nA way to define links between tables (and thus define relationships between the corresponding entities), and to enforce that these links are valid.\nAn efficient way to combine information across these links.\n\nThe former is supplied by foreign keys and the latter by joins. We will tackle both in turn.\n\n\nA foreign key is a field (or collection of fields) in one table that uniquely specifies a row in another table. We specify foreign keys in PostgreSQL using the REFERENCES keyword when we define a column or table. A foreign key that references another table must be the value of a unique key in that table, though it is most common to reference a primary key.\nFor example, the events table was defined using REFERENCES:\nCREATE TABLE events (\n    id SERIAL PRIMARY KEY,\n    moment TIMESTAMP,\n    persona INTEGER REFERENCES personae (id),\n    -- more columns go here:\n    ...\n);\nForeign keys can also be added (and altered) as table constraints that look like FOREIGN KEY (&lt;key&gt;) REFERENCES &lt;table&gt;.\nNow try this\nINSERT INTO events (persona, element, score)\nVALUES (-1, 123, 1000);\nNotice that the insertion did not work—-and the entire transaction was rolled back—because the foreign key constraint was violated. There was no persona with ID -1.\nSo let’s fix it. Try it!\nINSERT INTO personae VALUES (-1, 'Englebert', 'Humperdinck', '1936-05-02', 100.0);\n\nINSERT INTO events (persona, element, score)\nVALUES (-1, 123, 1000);\nA foreign key is allowed to be NULL, meaning it points to nothing, unless you include a NOT NULL constraint in the column definition.\n\n\n\nSuppose we want to display features of an event with the name and course of the student who generated it. If we’ve kept to DRY design and used a foreign key for the persona column, this seems inconvenient.\nThat is the purpose of a join. For instance, we can write:\nSELECT personae.lastname, personae.firstname, events.score, events.moment\nFROM events\nJOIN personae ON events.persona = personae.id\nWHERE moment &gt; CAST('2015-03-26 08:00:00' AS timestamp)\nORDER BY moment;\nJoins incorporate additional tables into a select. This is done by appending to the from clause:\nFROM &lt;table&gt; JOIN &lt;table&gt; ON &lt;condition&gt; ...\nwhere the on condition specifies which rows of the different tables are included. And within the select, we can disambiguate columns by referring them to by &lt;table&gt;.&lt;column&gt;. Look at the example above with this in mind.\nYou can think of FROM &lt;table&gt; JOIN &lt;table&gt; as producing a new virtual table formed by the two joined together, and the SELECT operates on this. You can join this virtual table to others: FROM &lt;table&gt; JOIN &lt;table&gt; ON &lt;condition&gt; JOIN &lt;table&gt; and so on.\nAfter joining two or more tables, you can operate on the resulting table just like in any other queries, with grouping, aggregates, and so on.\nWe will start by seeing what joins mean in a simple case, using these tables:\nCREATE TABLE a (id SERIAL PRIMARY KEY, name TEXT);\nINSERT INTO a (name)\n       VALUES ('Pirate'),\n              ('Monkey'),\n              ('Ninja'),\n              ('Flying Spaghetti Monster');\n\nCREATE TABLE b (id SERIAL PRIMARY KEY, name TEXT);\nINSERT INTO b (name)\n       VALUES ('Rutabaga'),\n              ('Pirate'),\n              ('Darth Vader'),\n              ('Ninja');\nLet’s look at several kinds of joins. (There are others, but this will get across the most common types.)\n\n\nAn inner join produces the rows for which attributes in both tables match. (If you just say JOIN in SQL, you get an inner join; the word INNER is optional.)\nSELECT * FROM a INNER JOIN b on a.name = b.name;\n\n\n\nA full outer join produces the full set of rows in all tables, matching where possible but NULL otherwise.\nSELECT * FROM a FULL OUTER JOIN b on a.name = b.name;\n\n\n\nA left outer join produces all the rows from A, the table on the “left” side of the join operator, along with matching rows from B if available, or null otherwise. (LEFT JOIN is a shorthand for LEFT OUTER JOIN in PostgreSQL.)\nSELECT * FROM a LEFT OUTER JOIN b ON a.name = b.name;\n\nExercise 7 (Set operations) Write queries to:\n\nSelect all the rows of A that are not in B.\nSelect the rows of A not in B and the rows of B not in A.\n\nHint: To check if a value is null, use value IS NULL.\n\n\nSolution. We can use outer joins. When a row in one table does not match a row in the other, the second table’s entries are NULL, so we can check for null in the results:\nSELECT * FROM a LEFT OUTER JOIN b on a.name = b.name where b.id IS NULL;\nSELECT * FROM b FULL OUTER JOIN b on a.name = b.name\n    WHERE b.id IS NULL OR b.id IS NULL;\n\n\nExercise 8 (Joining educational data) Besides the events and personae tables, our database also contains an elements table providing information about each instructional element (such as a quiz question) within the system. Here is its schema:\nCREATE TABLE elements (\n       id SERIAL PRIMARY KEY,\n       content TEXT,\n       points_max INTEGER CHECK (points_max &gt;= 0),\n       module INTEGER, -- references a modules table, if we had one\n       hint TEXT,\n       max_tries INTEGER CHECK (max_tries &gt; 0)\n);\nWrite queries to:\n\nList all events, with the score they received, the name of the student completing the event, their birthdate, and the maximum number of points possible.\nThe same, but only for events where the student’s score is higher than the number of points possible.\nList all elements IDs that have never been part of an event.\nCalculate the average score for all events for each element.\nSum the scores for all students born after June 1, 1988.\n\n\n\nSolution. TODO solutions"
  },
  {
    "objectID": "data-engineering/sql.html#indices",
    "href": "data-engineering/sql.html#indices",
    "title": "SQL Basics",
    "section": "",
    "text": "When we know we will search on certain fields regularly, it can be helpful to create an index, which speeds up those particular searches. An index tells PostgreSQL to keep a data structure (typically a tree) for a particular column so that it can identify all rows matching a query without scanning the entire table.\nFor example, PostgreSQL automatically indexes any column declared as a PRIMARY KEY. In the events table, the id column is the primary key, so a query like\nSELECT * FROM events WHERE id &gt; 200 AND id &lt; 300;\nwill use the index. PostgreSQL will search the tree (in something like \\(O(\\log\nn)\\) time, where \\(n\\) is the number of rows) and pull the matching rows of the table, rather than looping over the entire table and checking every row.\nWhile PostgreSQL automatically indexes primary keys, we can also tell it explicitly to add indexes for specific columns. For example, if we know we’ll often be searching events by start time, we could do\nCREATE INDEX event_moment_index\nON events (moment);\nEach index has a name so that we can identify it when we use DROP INDEX to remove it. Postgres also uses these names if we ask it to explain how it will conduct a query. By default, this index is based on a B-tree, but there are other index types that are useful for other things. Hash indices, for instance, are very fast for equality (SELECT * FROM foo WHERE col == 'something'). See the indexes chapter of the manual for more information on the syntax and type of indexes.\nIndices are particularly important for tables with many thousands or millions of rows. Companies often have tables with gigabytes of data, and looping through every row to find matching results would be prohibitively slow. Worse, joining two large tables requires matching up rows from the two tables, and would be even slower. A clever selection of indices can make these operations much faster.\nMuch of the engineering work in database software is in the so-called query planner, which takes your query, information about the size of each table, and the available indices to determine the most efficient way to execute the query. (Should it filter the data first, then join? Or join and then filter? Which of several indices should it use to filter first, so it has less work to do for subsequent operations?)\nIt is possible to develop an entire career around building database tables, specifying their indices, and writing queries that both solve business questions and are extremely fast for enormous databases."
  },
  {
    "objectID": "data-engineering/sql.html#footnotes",
    "href": "data-engineering/sql.html#footnotes",
    "title": "SQL Basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI once accidentally omitted the WHERE clause on an UPDATE query to change a user’s password in a database, and hence changed everyone’s password. On a live website with thousands of users. I had to very frantically get the previous day’s backup and figure out how to reset passwords to those contained in the backup.↩︎"
  },
  {
    "objectID": "large-scale-data/spark-cloud.html",
    "href": "large-scale-data/spark-cloud.html",
    "title": "Spark in the Cloud",
    "section": "",
    "text": "As we saw previously (?@sec-spark), Apache Spark is designed for distributed data analysis: splitting up a data analysis task to operate on chunks of the data that may be hosted on many different machines. In a cloud computing context, that means the data is often hosted on many rented machines, or stored in the cloud provider’s object storage system and analyzed by many machines at a time.\nFirst, let’s return to ?@sec-spark-architecture and recall how Spark is set up and runs on multiple machines.\nNow that we recall the details about drivers and executors, let’s talk about how we can write Spark jobs that run in the cloud, instead of just on our laptops.\nPreviously we used pyspark, a program that opened an interactive Python shell and let us type commands and see their output. That’s great to test things out, or for a quick interactive data analysis, but it’s not so good for a big data analysis job requiring many hours to run. Let’s see how to submit jobs at the command line.\n\n\nWhen installed on a computer, Spark provides a number of utility programs. Let’s say Spark is installed in the $SPARK_HOME – this is a shell notation for an environment variable, so the actual location is stored in a variable. (If you log in to our Spark server via SSH, this variable should already be defined in your shell; try running echo $SPARK_HOME.) Spark’s programs are stored in $SPARK_HOME/bin.\nIn there, you’ll find the pyspark program you used previously. You’ll also find spark-submit, the tool for submitting scripts to a Spark cluster.\nAs we described before, Spark can run Python, Java, or Scala programs, but we’ll be working in Python. Here’s a simple usage of spark-submit for a Python script:\n$SPARK_HOME/bin/spark-submit \\\n    --master spark://localhost:7077 \\\n    --deploy-mode client \\\n    some-analysis-script.py\nThis sends some-analysis-script.py to Spark. Specifically:\n\nWe choose the client deployment mode, meaning that our script is run on the local machine and sends Spark operations to the cluster.\nThe script becomes the Spark driver. It loads the SparkContext and contains all the code telling Spark what analyses to do (what data to load, what aggregations and calculations to do, etc.)\nThe script connects to the specified Spark cluster master. Here we’ve specified spark://localhost:7077, meaning the script will connect to a Spark cluster running on the same machine (localhost) on port 7077. The master is part of the Spark cluster manager, and is responsible for creating Spark executors to actually do the data analysis.\n\nThere are many other options, but we’ll consider them later if we need them.\n\n\n\nLet’s suppose we have a Spark cluster with five machines:\n\nspark-users: Has accounts for all the data analysts who need to use Spark, and lets them log in to run spark-submit or pyspark.\nspark-master: Runs the cluster manager.\nspark-worker-1 through spark-worker-3: Members of the cluster that run executors when needed.\n\nEach of these is a separate machine with its own operating system and files.\n\n\n\nNow let’s suppose we log in to the spark-users server and use spark-submit to run the following Python file:\nfrom pyspark.sql import SparkSession\n\n# Create the SparkSession, which makes this script the driver and\n# connects to the cluster manager to get executors\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .getOrCreate()\n\n# Load data. The path is a location on the *executor* servers, so\n# the data must be available there\ndf = spark.read.json(\"examples/src/main/resources/people.json\")\n\n# Do an action\ndf.groupBy(\"age\").count()\nIn the “client” deploy-mode, this Python script runs on spark-users, so it has access to the Python libraries available on that machine.\n\n\nspark-submit will connect to the cluster manager (as specified by the --master command-line argument) and request executors on which to run this application. We can control how many executors it will request from the cluster manager. For example, if we include --total-executor-cores 2 when calling spark-submit, our job will only get two CPUs on the cluster to run on; if we do not include the option, it will request all currently available cores, however many that may be.\nWe can also request specific amounts of memory, if that should be necessary. By default, each executor gets 1 GB of memory, but we can ask for different amounts with the --executor-memory command-line argument. For example, use --executor-memory 10G to request 10 GB for each executor. This might be necessary if you’re doing enormous joins on large datasets.\nBe careful, though: you’re limited by what is available in your cluster. If you request more cores than are available, you’ll only get what’s available. If someone else is currently using all the cores, you may see a message like this:\nWARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\nThis could happen because there are no cores available at all, or because there is insufficient memory available. In standalone cluster mode, spark-submit will simply retry every 15 seconds until sufficient resources are available; other cluster systems, like Mesos or YARN, have queuing systems where jobs queue up to run and are served in order.\n\n\n\nRecall that in Spark, nothing is actually calculated until we perform an action. Actions include things like printing a data frame or writing it to a file. Until then, all of our operations (groupBy(), filter(), and so on) are collected together.\nWhen we perform the action, Spark breaks those operations into stages. A stage is a group of operations that can be done simultaneously on many machines. After each stage, data may be shuffled between machines, putting it where it is needed for the next stage of operations.\nHowever, the output of this action doesn’t go anywhere visible to us: spark-submit provides lots of debugging output, but not the values printed from the script. We’ll need to write output to data files as needed. We’ll discuss files and data management next.\n\n\n\n\nBack in ?@sec-distributed-data, we talked about the idea of distributed data. A distributed file system looks like any other file system from the outside – with files stored in directories inside other directories – but stores its data on multiple machines, often redundantly, and fetches data from machines as needed.\nWith Spark, a common choice is the Hadoop Distributed File System. HDFS is designed to work when spread across thousands of machines for enormous datasets, but it presents a file system that looks much like any other.\nIt’s organized by using a single NameNode and multiple DataNodes. The NameNode is a machine that keeps track of which files are stored in the filesystem and which machines they live on; the DataNodes are the machines that actually store the data. A client trying to open a file asks the NameNode which DataNodes have it, and then fetches the contents from the DataNodes. The NameNode periodically checks the status of the DataNodes to find out if any of them has gone down or failed.\nTypically, then, a company using HDFS would have dozens or hundreds of servers, each with as many hard drives as can fit, serving as DataNodes. In a basic setup, there is only one NameNode, so its failure would prevent any user from accessing files; since Hadoop 2.0, it’s been possible to set up a “High Availability” cluster that has multiple NameNodes, one serving requests and others waiting in reserve in case the primary fails.\nTo access data on HDFS from Spark, we need the address of the HDFS NameNode. We can then provide a path that begins with hdfs://. For example, we could run\ndf = spark.read.parquet(\"hdfs://10.0.0.4:9000/lending-club.parquet\")\nto read a Parquet file called lending-club.parquet from the NameNode at 10.0.0.4:9000.\nWe can also view and manipulate HDFS files from the command line. The program $HADOOP_HOME/bin/hdfs has a sub-command dfs to manipulate the file system. For example,\n$HADOOP_HOME/bin/hdfs dfs -ls /\nlists the files in the root directory. Run $HADOOP_HOME/bin/hdfs dfs to see a list of all available commands. The -copyFromLocal and -copyToLocal commands may be particularly useful, as they allow you to copy a file from your local filesystem into HDFS, and to copy a file from HDFS back into your local filesystem.\n\n\n\nTODO Update to Databricks\nWe have a class Spark cluster set up on Azure. It has several machines:\n\nmsp-cool-stuff: Configured as the Spark cluster manager. You’ll log in to this machine to run spark-submit or pyspark. You can log in via SSH following the instructions in the email you received.\nmsp-spark-worker-1 through msp-spark-worker-3: Spark workers. Each has 8 cores and 32 GB of RAM.\n\nThis means our cluster has 24 cores and 96 GB available for student use.\nThe cluster runs HDFS at hdfs://10.0.0.4:9000/. Each student has a directory in HDFS that matches your username. If your username is jdoe, you have access to a directory /jdoe/, and can write results to that directory. So if you need to write output files, write them to paths beginning with hdfs://10.0.0.4:9000/yourusername/.\nSome basic rules for use:\n\nIf you run the PySpark shell, set --total-executor-cores 2. Otherwise you will get all the available cores; if nobody else is using the cluster at the time, you’ll get the entire cluster, and nobody will be able to use it until you exit PySpark.\nYou can hence start PySpark by running:\n$SPARK_HOME/bin/pyspark \\\n    --master spark://10.0.0.4:7077 \\\n    --total-executor-cores 2\nIf you’re submitting a job with spark-submit, you can use the defaults and request all available cores – but do not allow your job to take more than a few minutes, so other students do not have to wait. (You can use Ctrl-C to stop the job.) If other students are using the cluster, you may have to wait for their jobs to finish before your job can start. Just leave spark-submit open until it gets resources and starts running your job.\nYou run spark-submit with the command\n$SPARK_HOME/bin/spark-submit \\\n    --master spark://10.0.0.4:7077 \\\n    --deploy-mode client \\\n    your-script.py\n\n\n\n\nTODO"
  },
  {
    "objectID": "large-scale-data/spark-cloud.html#submitting-a-job",
    "href": "large-scale-data/spark-cloud.html#submitting-a-job",
    "title": "Spark in the Cloud",
    "section": "",
    "text": "When installed on a computer, Spark provides a number of utility programs. Let’s say Spark is installed in the $SPARK_HOME – this is a shell notation for an environment variable, so the actual location is stored in a variable. (If you log in to our Spark server via SSH, this variable should already be defined in your shell; try running echo $SPARK_HOME.) Spark’s programs are stored in $SPARK_HOME/bin.\nIn there, you’ll find the pyspark program you used previously. You’ll also find spark-submit, the tool for submitting scripts to a Spark cluster.\nAs we described before, Spark can run Python, Java, or Scala programs, but we’ll be working in Python. Here’s a simple usage of spark-submit for a Python script:\n$SPARK_HOME/bin/spark-submit \\\n    --master spark://localhost:7077 \\\n    --deploy-mode client \\\n    some-analysis-script.py\nThis sends some-analysis-script.py to Spark. Specifically:\n\nWe choose the client deployment mode, meaning that our script is run on the local machine and sends Spark operations to the cluster.\nThe script becomes the Spark driver. It loads the SparkContext and contains all the code telling Spark what analyses to do (what data to load, what aggregations and calculations to do, etc.)\nThe script connects to the specified Spark cluster master. Here we’ve specified spark://localhost:7077, meaning the script will connect to a Spark cluster running on the same machine (localhost) on port 7077. The master is part of the Spark cluster manager, and is responsible for creating Spark executors to actually do the data analysis.\n\nThere are many other options, but we’ll consider them later if we need them."
  },
  {
    "objectID": "large-scale-data/spark-cloud.html#an-example-spark-cluster",
    "href": "large-scale-data/spark-cloud.html#an-example-spark-cluster",
    "title": "Spark in the Cloud",
    "section": "",
    "text": "Let’s suppose we have a Spark cluster with five machines:\n\nspark-users: Has accounts for all the data analysts who need to use Spark, and lets them log in to run spark-submit or pyspark.\nspark-master: Runs the cluster manager.\nspark-worker-1 through spark-worker-3: Members of the cluster that run executors when needed.\n\nEach of these is a separate machine with its own operating system and files."
  },
  {
    "objectID": "large-scale-data/spark-cloud.html#an-example-job",
    "href": "large-scale-data/spark-cloud.html#an-example-job",
    "title": "Spark in the Cloud",
    "section": "",
    "text": "Now let’s suppose we log in to the spark-users server and use spark-submit to run the following Python file:\nfrom pyspark.sql import SparkSession\n\n# Create the SparkSession, which makes this script the driver and\n# connects to the cluster manager to get executors\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .getOrCreate()\n\n# Load data. The path is a location on the *executor* servers, so\n# the data must be available there\ndf = spark.read.json(\"examples/src/main/resources/people.json\")\n\n# Do an action\ndf.groupBy(\"age\").count()\nIn the “client” deploy-mode, this Python script runs on spark-users, so it has access to the Python libraries available on that machine.\n\n\nspark-submit will connect to the cluster manager (as specified by the --master command-line argument) and request executors on which to run this application. We can control how many executors it will request from the cluster manager. For example, if we include --total-executor-cores 2 when calling spark-submit, our job will only get two CPUs on the cluster to run on; if we do not include the option, it will request all currently available cores, however many that may be.\nWe can also request specific amounts of memory, if that should be necessary. By default, each executor gets 1 GB of memory, but we can ask for different amounts with the --executor-memory command-line argument. For example, use --executor-memory 10G to request 10 GB for each executor. This might be necessary if you’re doing enormous joins on large datasets.\nBe careful, though: you’re limited by what is available in your cluster. If you request more cores than are available, you’ll only get what’s available. If someone else is currently using all the cores, you may see a message like this:\nWARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\nThis could happen because there are no cores available at all, or because there is insufficient memory available. In standalone cluster mode, spark-submit will simply retry every 15 seconds until sufficient resources are available; other cluster systems, like Mesos or YARN, have queuing systems where jobs queue up to run and are served in order.\n\n\n\nRecall that in Spark, nothing is actually calculated until we perform an action. Actions include things like printing a data frame or writing it to a file. Until then, all of our operations (groupBy(), filter(), and so on) are collected together.\nWhen we perform the action, Spark breaks those operations into stages. A stage is a group of operations that can be done simultaneously on many machines. After each stage, data may be shuffled between machines, putting it where it is needed for the next stage of operations.\nHowever, the output of this action doesn’t go anywhere visible to us: spark-submit provides lots of debugging output, but not the values printed from the script. We’ll need to write output to data files as needed. We’ll discuss files and data management next."
  },
  {
    "objectID": "large-scale-data/spark-cloud.html#sec-working-with-hdfs",
    "href": "large-scale-data/spark-cloud.html#sec-working-with-hdfs",
    "title": "Spark in the Cloud",
    "section": "",
    "text": "Back in ?@sec-distributed-data, we talked about the idea of distributed data. A distributed file system looks like any other file system from the outside – with files stored in directories inside other directories – but stores its data on multiple machines, often redundantly, and fetches data from machines as needed.\nWith Spark, a common choice is the Hadoop Distributed File System. HDFS is designed to work when spread across thousands of machines for enormous datasets, but it presents a file system that looks much like any other.\nIt’s organized by using a single NameNode and multiple DataNodes. The NameNode is a machine that keeps track of which files are stored in the filesystem and which machines they live on; the DataNodes are the machines that actually store the data. A client trying to open a file asks the NameNode which DataNodes have it, and then fetches the contents from the DataNodes. The NameNode periodically checks the status of the DataNodes to find out if any of them has gone down or failed.\nTypically, then, a company using HDFS would have dozens or hundreds of servers, each with as many hard drives as can fit, serving as DataNodes. In a basic setup, there is only one NameNode, so its failure would prevent any user from accessing files; since Hadoop 2.0, it’s been possible to set up a “High Availability” cluster that has multiple NameNodes, one serving requests and others waiting in reserve in case the primary fails.\nTo access data on HDFS from Spark, we need the address of the HDFS NameNode. We can then provide a path that begins with hdfs://. For example, we could run\ndf = spark.read.parquet(\"hdfs://10.0.0.4:9000/lending-club.parquet\")\nto read a Parquet file called lending-club.parquet from the NameNode at 10.0.0.4:9000.\nWe can also view and manipulate HDFS files from the command line. The program $HADOOP_HOME/bin/hdfs has a sub-command dfs to manipulate the file system. For example,\n$HADOOP_HOME/bin/hdfs dfs -ls /\nlists the files in the root directory. Run $HADOOP_HOME/bin/hdfs dfs to see a list of all available commands. The -copyFromLocal and -copyToLocal commands may be particularly useful, as they allow you to copy a file from your local filesystem into HDFS, and to copy a file from HDFS back into your local filesystem."
  },
  {
    "objectID": "large-scale-data/spark-cloud.html#sec-using-spark-cluster",
    "href": "large-scale-data/spark-cloud.html#sec-using-spark-cluster",
    "title": "Spark in the Cloud",
    "section": "",
    "text": "TODO Update to Databricks\nWe have a class Spark cluster set up on Azure. It has several machines:\n\nmsp-cool-stuff: Configured as the Spark cluster manager. You’ll log in to this machine to run spark-submit or pyspark. You can log in via SSH following the instructions in the email you received.\nmsp-spark-worker-1 through msp-spark-worker-3: Spark workers. Each has 8 cores and 32 GB of RAM.\n\nThis means our cluster has 24 cores and 96 GB available for student use.\nThe cluster runs HDFS at hdfs://10.0.0.4:9000/. Each student has a directory in HDFS that matches your username. If your username is jdoe, you have access to a directory /jdoe/, and can write results to that directory. So if you need to write output files, write them to paths beginning with hdfs://10.0.0.4:9000/yourusername/.\nSome basic rules for use:\n\nIf you run the PySpark shell, set --total-executor-cores 2. Otherwise you will get all the available cores; if nobody else is using the cluster at the time, you’ll get the entire cluster, and nobody will be able to use it until you exit PySpark.\nYou can hence start PySpark by running:\n$SPARK_HOME/bin/pyspark \\\n    --master spark://10.0.0.4:7077 \\\n    --total-executor-cores 2\nIf you’re submitting a job with spark-submit, you can use the defaults and request all available cores – but do not allow your job to take more than a few minutes, so other students do not have to wait. (You can use Ctrl-C to stop the job.) If other students are using the cluster, you may have to wait for their jobs to finish before your job can start. Just leave spark-submit open until it gets resources and starts running your job.\nYou run spark-submit with the command\n$SPARK_HOME/bin/spark-submit \\\n    --master spark://10.0.0.4:7077 \\\n    --deploy-mode client \\\n    your-script.py"
  },
  {
    "objectID": "large-scale-data/spark-cloud.html#exercises",
    "href": "large-scale-data/spark-cloud.html#exercises",
    "title": "Spark in the Cloud",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "large-scale-data/spark-ml.html",
    "href": "large-scale-data/spark-ml.html",
    "title": "Machine Learning in Spark",
    "section": "",
    "text": "Besides managing large datasets and applying SQL-style operations to them, we data scientists often want to use large datasets to fit statistical models. Sometimes you will use Spark to extract features from a huge dataset, then fit models to the features on your own computer—the hard part is extracting the relevant features from the enormous dataset, not fitting the model. But sometimes the model is to be fit to too much data, and you’d like to fit it within Spark as well. That’s where MLlib comes in.\nMLlib is a collection of regression, classification, clustering, and related methods implemented within Spark and accessible within Spark applications. Within one Spark application, you can write all the necessary steps:\n\nLoad the data\nExtract features from the data\nTransform the features, scale them, recode them, etc.\nSplit the data into training, test, and evaluation sets, as needed\nSpecify models to fit\nFit the models\nEvaluate the model performance\n\nTo automate much of this, we can create a Spark pipeline, which represents the sequence of operations. The pipeline system makes it easy to fit multiple models, with different tuning parameters and settings, to the same data so you can compare their results.\n\n\nThere are two versions of MLlib:\n\nthe old version using Resilient Distributed Datasets (RDDs), which Spark used before it added the DataFrame system\nthe new version using DataFrames\n\nWe will discuss the new version, which you can get by importing from spark.ml. The old version is under spark.mllib. The new version supports nearly all the features of the old one, and will get all the new features and updates; but you will still find examples and code using the old one, which can be confusing.\nLet’s do a simple example to illustrate the new system, following @SparkGuide, chapter 24. (Their sample data and code is available.) We’ll start by loading the data:\nfrom pyspark import SparkFiles\n\ndata_url = \"https://www.refsmmat.com/courses/msp-computing/data/simple-ml.json\"\nsc.addFile(data_url)\n\ndf = spark.read.json(SparkFiles.get(\"simple-ml.json\"))\nSpark’s ML pipelines are based around two core concepts: Transformers and Estimators. These are Python classes with certain basic methods; all the operations for transforming data, fitting models, and evaluating error metrics are based on classes that inherit from either Transformer or Estimator. They implement certain basic operations:\n\nTransformer:\n\n.transform(dataset): Takes a dataset, transforms it in some way, and returns a new dataset.\n\nEstimator:\n\n.fit(dataset): Fits this estimator to a dataset and returns one or more Transformer objects.\n\n\nWe will see how an entire pipeline can be built from these two basic methods.\n\n\n\n\n\nOften we must transform our source data before using it to fit models. Perhaps we need to aggregate data to produce new features, or do transformations, or rename columns, or whatever.\nThe most direct way to do this in Spark is with a SQLTransformer, which lets you use arbitrary SQL to transform a data frame. A Transformer object can take any dataset and return a new one, so our SQL query must take a data frame and return a new one. That implies we must use a SELECT.\nfrom pyspark.ml.feature import SQLTransformer\n\ntransformation = SQLTransformer(\n    statement=\"SELECT persona, SUM(score) FROM __THIS__ GROUP BY persona\"\n)\nNotice we refer to the table as __THIS__, which always refers to the data frame passed to transformation.transform().\nThis may seem redundant: why use a SQLTransformer when you can just use spark.sql() to do any data manipulation you need? But as we’ll see, Spark has convenient features for working for pipelines composed exclusively of Transformer and Estimator objects, so expressing this task as a Transformer will have benefits.\n\n\n\nA typical Spark DataFrame has various columns for different features. Most of Spark’s ML methods, however, expect to have one numerical column for the response variable and one numerical column containing a vector of features, not multiple separate columns of features.\nAlso, unlike in R, Spark’s ML systems won’t do automatic coding of design matrices for you. That is, if you have several categorical (factor) variables, R’s lm() would automatically produce the necessary dummy variables (contrasts) based on your model formula. If your formula is y ~ x + some_factor, R will create a design matrix \\(X\\) containing columns for the intercept, x, and the levels of the factor. Similarly, if we specify interactions, R will add columns with the correct products.\nIn Spark, we need to do this step manually. Fortunately Spark provides RFormula, which understands the basics of R’s formula syntax.\nfrom pyspark.ml.feature import RFormula\n\nsupervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\nThe supervised object is an Estimator: it must be fit to data before it can be used, since it must examine the data to see how many factor levels there are before it can create the right columns. Once it’s fit, we get a Transformer, and this can transform the data to produce the new columns.\nfittedRF = supervised.fit(df)\npreparedDF = fittedRF.transform(df)\nNow the prepared data frame contains a features column with the transformed data. The label column contains the response (lab) coded as 0 or 1. We could control the names of these columns with the featuresCol and labelCol arguments to RFormula(), if we needed to.\nSometimes your data has missing values for some rows. In R, when you use lm() or glm() to fit a model to data with NA values for some predictors, it simply skips the affected rows. By default, however, Spark’s modeling functions throw an error when they encounter null, Spark’s equivalent of NA. With the handleInvalid argument to RFormula, we can set a different option. For example, by setting handleInvalid=\"skip\", we can tell RFormula that when it transforms the data, it should throw away (skip) rows with null values for any of the variables in the formula.\n\n\n\nIf we’re using a train/test split for model evaluation, we can do that now:\ntrain, test = preparedDF.randomSplit([0.7, 0.3])\n\n\n\nThe pyspark.ml.feature module contains a number of tools for manipulating and rescaling features.\nSome machine learning methods require us to rescale features first. (For example, when using the lasso or ridge regression, it’s good to put all features on the same scale so the penalty applies to all features equally.) Using MaxAbsScaler, MinMaxScaler, RobustScaler, or StandardScaler, we can rescale all our features.\nFor example, StandardScaler makes all features have mean 0 and variance 1. It expects the features to already be prepared as a vector of features in a single column, like RFormula creates for us. StandardScaler is an Estimator, so it must be fit on data (to calculate the means and variances) before it is used to transform (standardize) the data. It takes an inputCol argument naming the column to standardize and outputCol naming the new, standardized column to create; the other scalers all work in a similar way.\nTo use it, we first create the scaler and then fit it:\nfrom pyspark.ml.feature import StandardScaler\n\n# by default, StandardScaler does not center the data; set withMean=True to\n# center (subtract the mean)\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n                        withMean=True)\n\nfitted_scaler = scaler.fit(train)\n\ntrain = fitted_scaler.transform(train)\nNow train contains a scaledFeatures column where each feature has mean 0 and variance 1.\nNotice that we are training the scaler using the training data, not the entire dataset. This is important because it ensures that we do not use the test data in any way, even for scaling, when fitting models. We can later use the fitted_scaler to rescale the test data.\n\n\n\n\nOnce the data is prepared, we can fit a model. Spark has a whole range of supported models for classification, clustering, and regression. In this toy example, we’d like to do logistic regression to predict the binary response.\nfrom pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"scaledFeatures\")\nThis lr object represents an abstract logistic regression with particular columns for features and labels (responses). We must tell it where to find the true labels (labelCol) and where to find the features (featuresCol); by default, it looks in columns named label and features, matching the default column names created by RFormula.\nlr is again an Estimator object. To fit this to the data, we must run\nfittedLR = lr.fit(train)\nNow Spark has fit the model. We can get its coefficients with fittedLR.coefficients.\nNote that logistic regression in Spark is not just ordinary logistic regression: it supports lasso and ridge penalization through the elastic net, and it can support various other constraints on the coefficients. Using lr.explainParams() you can have Spark explain all the available options.\nFitted models in Spark all have a common interface: they are Transformer objects that can transform a dataset to a new dataset. In models, the transformation is adding a column of predictions; in RFormula, as we saw, the transformation is adding a column of features.\n\n\n\nOften you will want to try multiple models with multiple parameter settings, fitting them all to the same data and evaluating them using common metrics. Instead of writing code to do this manually, we can use Spark’s pipeline system, which lets us specify a pipeline of tasks to complete with a range of parameter settings.\nLet’s start with a random split of the original dataset (not the prepared data).\ntrain, test = df.randomSplit([0.7, 0.3])\nNow let’s specify our pipeline. We will first prepare the data with a model formula, as before, and then we will fit a logistic regression. Notice here we have not specified the actual formula or regression options:\nfrom pyspark.ml import Pipeline\n\nrForm = RFormula()\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n\npipeline = Pipeline().setStages([rForm, lr])\nEach stage in the pipeline can be any kind of Estimator or Transformer. The pipeline itself is an Estimator. We use it in two steps:\n\npipeline.fit(dataset): Provides the dataset to the first stage. If it’s an Estimator, call the fit(dataset) method, then call transform(dataset) on the fitted object. If it’s a Transformer, call transform(dataset). Take the output and repeat on the next stage of the pipeline. This returns a PipelineModel, which is a Transformer containing all the fitted stages.\npipelinemodel.transform(dataset): Each stage of the pipeline is now a Transformer, because it was previously fit to data. Call `transform(dataset) on each stage in turn to produce the pipeline output.\n\nTo set the options for each pipeline stage, we can use ParamGridBuilder. This lets us specify a grid of options—a grid because when we use this in our pipeline, it will try every combination of the options.\nfrom pyspark.ml.tuning import ParamGridBuilder\n\nparams = ParamGridBuilder() \\\n  .addGrid(rForm.formula, [\n    \"lab ~ . + color:value1\",\n    \"lab ~ . + color:value1 + color:value2\"]) \\\n  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n  .addGrid(lr.regParam, [0.1, 2.0]) \\\n  .build()\nThis grid represents \\(2 \\times 3 \\times 2 = 12\\) different models to try.\n\n\n\nNow that we have specified the pipeline and the range of parameter options, we must tell Spark how we plan to evaluate our models. There is a basic Evaluator class to support evaluation, with several specific kinds of evaluator available. The BinaryClassificationEvaluator lets us choose from a variety of classification metrics; here we will use the AUC:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator() \\\n  .setMetricName(\"areaUnderROC\") \\\n  .setRawPredictionCol(\"prediction\") \\\n  .setLabelCol(\"label\")\nNotice we have also told it which column the predictions will be in and where the true labels are, so it can compare prediction to truth.\nNow we need a strategy for Spark to evaluate each model. One way is a train/validation split: split the data into two pieces, train on the training piece, and evaluate on the test piece. This is all on the original test set created with df.randomSplit(), since we are saving the original test set for final evaluation.\nfrom pyspark.ml.tuning import TrainValidationSplit\n\ntvs = TrainValidationSplit() \\\n  .setTrainRatio(0.75) \\\n  .setEstimatorParamMaps(params) \\\n  .setEstimator(pipeline) \\\n  .setEvaluator(evaluator)\ntvs is now an object that can be fit to data, and will produce a model object representing the fitted model:\ntvsFitted = tvs.fit(train)\nThis sets the parameters in the pipeline object, calls fit() on it, gets the predictions via transform(), evaluates them, and repeats for all parameter combinations. tvsFitted is now a TrainValidationSplitModel object, which is a Transformer. We use its transform() method to make predictions with the best model (as evaluated by our evaluator) on our final held-out test set, then get the final AUC:\nevaluator.evaluate(tvsFitted.transform(test))\nWe might also want to get the tuning parameters of the best model. We had TrainValidationSplit estimate an entire pipeline, and so the “best model” is actually a PipelineModel from fitting our pipeline. This is stored in tvsFitted.bestModel.\nThis object contains a .stages attribute giving all the stages; in our case, this is a list of the fitted RFormula (rform) and the fitted logistic regression (lr). So we can get parameters from the logistic regression:\ntvsFitted.bestModel.stages[1].getRegParam()\ntvsFitted.bestModel.stages[1].getElasticNetParam()\nBesides TrainValidationSplit, there is also a CrossValidator that works the same way, but does K-fold cross-validation to get the error estimates.\n\n\n\nWe’ve seen a lot of pieces: Estimators, transformers, tools for coding variables, tools for scaling and transforming, models, evaluators, hyperparameter tuning, and so on. It’s hard to see how this all fits together.\nThe basic pipeline is meant to look like this:\n\n\n\n\n\nflowchart TD\n    data[Data]\n    rform[Make feature/label columns]\n    scale[Scale and transform data]\n    fit[Fit model]\n    predict[Make predictions]\n\n    data --&gt; rform\n    rform --&gt; scale\n    scale --&gt; fit\n    fit --&gt; predict\n\n\n\n\n\n\nEach step in this pipeline is one or more Transformer or Estimator objects, and the entire pipeline is represented as a Pipeline object with multiple stages. A Pipeline is itself an Estimator: the entire pipeline can be fit to data and then used to transform new data.\nBut pipelines have parameters that need to be tuned to optimize some evaluation metric. And we need some kind of train/test split to evaluate fairly. So we use a tuner like TrainValidationSplit:\n\n\n\n\n\nflowchart TD\n    split[\"Split training and test data &lt;br&gt;(TrainValidationSplit)\"]\n    subgraph pipeline[\"Pipeline() object\"]\n        rform[Make feature/label columns]\n        scale[Scale and transform data]\n        fit[Fit model]\n\n        rform --&gt; scale\n        scale --&gt; fit\n    end\n\n    split--&gt;|Training data|pipeline\n\n    fitted[\"Fitted pipeline&lt;br&gt;(PipelineModel)\"]\n    fit --&gt; fitted\n    split--&gt;|Test data|fitted\n\n    predictions[Predictions]\n    fitted --&gt; predictions\n\n    eval[Evaluator]\n    predictions --&gt; eval\n\n\n\n\n\n\nThis may need to be repeated for each combination of parameters we want to test, so we can choose the best.\n\n\n\nIf you’re working on a model for a product, once you arrive at a final model, you’re going to want to be able to use it later. Fitted models (such as fitted PipelineModel objects) have a write() method that can be used to save them:\ntvsFitted.write().save(\"path/to/fitted-logit-model\")\nAs with all Spark operations that involve reading and writing files, the Spark executors write most of the crucial data, so the path provided should be one that all executors have access to, such as a directory stored in HDFS or a shared filesystem.\nSince we saved a PipelineModel, we can also load the saved model back with PipelineModel.\nfrom pyspark.ml import PipelineModel\n\nreloaded_model = PipelineModel.load(\"path/to/fitted-logit-model\")\nThis is now a PipelineModel object we can use just like tvsFitted above, except we could have exited Spark in between saving the model and loading it. Now you can work with the same model across many Spark sessions or reuse it in many applications.\n\n\n\n\nExercise 1 (Galaxy mass prediction) The hive_metastore.default.buzzard_dc_1 table contains data from a physical simulation of 111,172 galaxies. The data is described here. Review the description so you’re familiar with the data.\nWe would like to predict galaxy stellar masses (log.mass) using redshift and the magnitude variables u, g, r, i, z, and y.\nNote that Spark uses . to refer to entries in structs (records), so log.mass will be interpreted as accessing the mass field of the log object. You must rename the variables and remove any dots (even in the columns you do not use, like u.err) to avoid confusing it.\nFirst, set aside a final test set of 30% of the data, randomly selected. (Use the randomSplit() method of DataFrame objects to do so.) We will use this to evaluate your pipelines.\nNext, build three pipelines to predict log.mass. Each has parameters you should tune with 5-fold cross-validation:\n\nLinear regression. Tune the regularization parameters, and also consider using either (a) a linear function of the original features, (b) a quadratic function of them, or (c) a cubic function of them. The PolynomialExpansion transformer may be helpful.\nA decision tree (using pyspark.ml.regression.DecisionTreeRegressor). Tune the maximum depth and maximum number of bins.\nA random forest (using pyspark.ml.regression.RandomForestRegressor). Tune the maximum tree depth and the maximum number of bins.\n\nTune each to select the model with the best mean absolute error (MAE). Report the best predictor, its MAE, and its tuning parameters.\n\n\nExercise 2 (Machine learning for LendingClub) For this exercise, we’ll continue using the LendingClub data from ?@exr-spark-lendingclub.\nIf you were an investor deciding which loans to fund, you’d be interested in predicting whether the borrower would pay the loan back on time. You might split loans into three categories:\n\nLoans fully paid off, with no late payments\nLoans paid back, but late\nLoans never fully paid back\n\nThe loan_status variable records the status of loans. There are several possible codes:\n\nFully Paid: The loan has been paid off\nCurrent: The borrower has been paying on time, but still has more payments to make\nIn Grace Period: The borrower is up to 15 days late in their payments; no late fees are charged until they’re over 15 days late\nLate (16-30 days): The borrower is 16-30 days late in their payments and will be charged a late fee\nLate (31-120 days): The borrower is 31-120 days late in their payments\nDefault: The borrower is more than 120 days late in payments, despite repeated reminders from LendingClub. LendingClub will now begin closing the loan\nCharged Off: The borrower has defaulted on the loan and LendingClub no longer expects any further payments to be made. They may sell the loan to a debt collection agency, or the borrower may have declared bankruptcy, in which case a court will decide how much money LendingClub may receive.\n\nThe status gives the current status (at the time this data was downloaded), but even a current or fully paid loan may have been late in the past and charged a late fee. The total_rec_late_fee column records the total late fees paid by the borrower.\nCreate a notebook that completes the following tasks.\nCreate a new column categorizing loans into the three categories above. You’ll have to filter out loans that are current and have not been paid off yet, because we do not know their ultimate status – they may become late or default later.\nAn investor may want to predict this categorization using information available at the time the borrower applies for the loan, such as: annual income (annual_inc), the number of times they’ve been delinquent on other accounts in the past two years (delinq_2yrs), their reason for asking for the loan (purpose), how many mortgages the borrower has (mort_acc), how many bankruptcies the borrower has had (pub_rec_bankruptcies), the borrower’s FICO credit score (fico_range_high and fico_range_low), and the amount they owe on credit cards (max_bal_bc). (There are many other covariates in the data, but we should start simple.)\nExtract these features from the data. Set aside 30% of the data to use as a test set.\nOn the training set, use Spark’s MinMaxScaler to scale all variables to be on the same scale. Then fit three classifiers to predict the loan categorization:\n\nA decision tree (DecisionTreeClassifier), using the default tuning parameters\nA random forest (RandomForestClassifier), using the default tuning parameters (which fit 20 trees with a depth of 5)\nA multilayer perceptron with three layers of 40 nodes each (MultilayerPerceptronClassifier). The layers argument lets you specify the size of each layer, including the input and output layers; here you should have 20 inputs (from your predictors, including dummies for categorical variables) and 3 outputs (for the 3 classes).\n\nEvaluate the three classifiers on the test set and report their accuracy and F1 scores. (With MulticlassClassificationEvaluator, you’ll need metricName=\"f1\" and metricName=\"accuracy\".)\nTurn in your notebook that does this complete task, starting with loading the data and ending with saving the test set metrics to a file. Include a table of accuracy and F1 scores.\nHints: See ?@sec-creating-dataframes to see how to create a data frame containing the evaluation results, so you can save it to a file. Review ?@sec-spark-expressions, and following sections, to see how to manipulate and create columns in Spark."
  },
  {
    "objectID": "large-scale-data/spark-ml.html#basic-setup",
    "href": "large-scale-data/spark-ml.html#basic-setup",
    "title": "Machine Learning in Spark",
    "section": "",
    "text": "There are two versions of MLlib:\n\nthe old version using Resilient Distributed Datasets (RDDs), which Spark used before it added the DataFrame system\nthe new version using DataFrames\n\nWe will discuss the new version, which you can get by importing from spark.ml. The old version is under spark.mllib. The new version supports nearly all the features of the old one, and will get all the new features and updates; but you will still find examples and code using the old one, which can be confusing.\nLet’s do a simple example to illustrate the new system, following @SparkGuide, chapter 24. (Their sample data and code is available.) We’ll start by loading the data:\nfrom pyspark import SparkFiles\n\ndata_url = \"https://www.refsmmat.com/courses/msp-computing/data/simple-ml.json\"\nsc.addFile(data_url)\n\ndf = spark.read.json(SparkFiles.get(\"simple-ml.json\"))\nSpark’s ML pipelines are based around two core concepts: Transformers and Estimators. These are Python classes with certain basic methods; all the operations for transforming data, fitting models, and evaluating error metrics are based on classes that inherit from either Transformer or Estimator. They implement certain basic operations:\n\nTransformer:\n\n.transform(dataset): Takes a dataset, transforms it in some way, and returns a new dataset.\n\nEstimator:\n\n.fit(dataset): Fits this estimator to a dataset and returns one or more Transformer objects.\n\n\nWe will see how an entire pipeline can be built from these two basic methods."
  },
  {
    "objectID": "large-scale-data/spark-ml.html#transforming-data",
    "href": "large-scale-data/spark-ml.html#transforming-data",
    "title": "Machine Learning in Spark",
    "section": "",
    "text": "Often we must transform our source data before using it to fit models. Perhaps we need to aggregate data to produce new features, or do transformations, or rename columns, or whatever.\nThe most direct way to do this in Spark is with a SQLTransformer, which lets you use arbitrary SQL to transform a data frame. A Transformer object can take any dataset and return a new one, so our SQL query must take a data frame and return a new one. That implies we must use a SELECT.\nfrom pyspark.ml.feature import SQLTransformer\n\ntransformation = SQLTransformer(\n    statement=\"SELECT persona, SUM(score) FROM __THIS__ GROUP BY persona\"\n)\nNotice we refer to the table as __THIS__, which always refers to the data frame passed to transformation.transform().\nThis may seem redundant: why use a SQLTransformer when you can just use spark.sql() to do any data manipulation you need? But as we’ll see, Spark has convenient features for working for pipelines composed exclusively of Transformer and Estimator objects, so expressing this task as a Transformer will have benefits.\n\n\n\nA typical Spark DataFrame has various columns for different features. Most of Spark’s ML methods, however, expect to have one numerical column for the response variable and one numerical column containing a vector of features, not multiple separate columns of features.\nAlso, unlike in R, Spark’s ML systems won’t do automatic coding of design matrices for you. That is, if you have several categorical (factor) variables, R’s lm() would automatically produce the necessary dummy variables (contrasts) based on your model formula. If your formula is y ~ x + some_factor, R will create a design matrix \\(X\\) containing columns for the intercept, x, and the levels of the factor. Similarly, if we specify interactions, R will add columns with the correct products.\nIn Spark, we need to do this step manually. Fortunately Spark provides RFormula, which understands the basics of R’s formula syntax.\nfrom pyspark.ml.feature import RFormula\n\nsupervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\nThe supervised object is an Estimator: it must be fit to data before it can be used, since it must examine the data to see how many factor levels there are before it can create the right columns. Once it’s fit, we get a Transformer, and this can transform the data to produce the new columns.\nfittedRF = supervised.fit(df)\npreparedDF = fittedRF.transform(df)\nNow the prepared data frame contains a features column with the transformed data. The label column contains the response (lab) coded as 0 or 1. We could control the names of these columns with the featuresCol and labelCol arguments to RFormula(), if we needed to.\nSometimes your data has missing values for some rows. In R, when you use lm() or glm() to fit a model to data with NA values for some predictors, it simply skips the affected rows. By default, however, Spark’s modeling functions throw an error when they encounter null, Spark’s equivalent of NA. With the handleInvalid argument to RFormula, we can set a different option. For example, by setting handleInvalid=\"skip\", we can tell RFormula that when it transforms the data, it should throw away (skip) rows with null values for any of the variables in the formula.\n\n\n\nIf we’re using a train/test split for model evaluation, we can do that now:\ntrain, test = preparedDF.randomSplit([0.7, 0.3])\n\n\n\nThe pyspark.ml.feature module contains a number of tools for manipulating and rescaling features.\nSome machine learning methods require us to rescale features first. (For example, when using the lasso or ridge regression, it’s good to put all features on the same scale so the penalty applies to all features equally.) Using MaxAbsScaler, MinMaxScaler, RobustScaler, or StandardScaler, we can rescale all our features.\nFor example, StandardScaler makes all features have mean 0 and variance 1. It expects the features to already be prepared as a vector of features in a single column, like RFormula creates for us. StandardScaler is an Estimator, so it must be fit on data (to calculate the means and variances) before it is used to transform (standardize) the data. It takes an inputCol argument naming the column to standardize and outputCol naming the new, standardized column to create; the other scalers all work in a similar way.\nTo use it, we first create the scaler and then fit it:\nfrom pyspark.ml.feature import StandardScaler\n\n# by default, StandardScaler does not center the data; set withMean=True to\n# center (subtract the mean)\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n                        withMean=True)\n\nfitted_scaler = scaler.fit(train)\n\ntrain = fitted_scaler.transform(train)\nNow train contains a scaledFeatures column where each feature has mean 0 and variance 1.\nNotice that we are training the scaler using the training data, not the entire dataset. This is important because it ensures that we do not use the test data in any way, even for scaling, when fitting models. We can later use the fitted_scaler to rescale the test data."
  },
  {
    "objectID": "large-scale-data/spark-ml.html#fitting-models",
    "href": "large-scale-data/spark-ml.html#fitting-models",
    "title": "Machine Learning in Spark",
    "section": "",
    "text": "Once the data is prepared, we can fit a model. Spark has a whole range of supported models for classification, clustering, and regression. In this toy example, we’d like to do logistic regression to predict the binary response.\nfrom pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"scaledFeatures\")\nThis lr object represents an abstract logistic regression with particular columns for features and labels (responses). We must tell it where to find the true labels (labelCol) and where to find the features (featuresCol); by default, it looks in columns named label and features, matching the default column names created by RFormula.\nlr is again an Estimator object. To fit this to the data, we must run\nfittedLR = lr.fit(train)\nNow Spark has fit the model. We can get its coefficients with fittedLR.coefficients.\nNote that logistic regression in Spark is not just ordinary logistic regression: it supports lasso and ridge penalization through the elastic net, and it can support various other constraints on the coefficients. Using lr.explainParams() you can have Spark explain all the available options.\nFitted models in Spark all have a common interface: they are Transformer objects that can transform a dataset to a new dataset. In models, the transformation is adding a column of predictions; in RFormula, as we saw, the transformation is adding a column of features."
  },
  {
    "objectID": "large-scale-data/spark-ml.html#making-pipelines",
    "href": "large-scale-data/spark-ml.html#making-pipelines",
    "title": "Machine Learning in Spark",
    "section": "",
    "text": "Often you will want to try multiple models with multiple parameter settings, fitting them all to the same data and evaluating them using common metrics. Instead of writing code to do this manually, we can use Spark’s pipeline system, which lets us specify a pipeline of tasks to complete with a range of parameter settings.\nLet’s start with a random split of the original dataset (not the prepared data).\ntrain, test = df.randomSplit([0.7, 0.3])\nNow let’s specify our pipeline. We will first prepare the data with a model formula, as before, and then we will fit a logistic regression. Notice here we have not specified the actual formula or regression options:\nfrom pyspark.ml import Pipeline\n\nrForm = RFormula()\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n\npipeline = Pipeline().setStages([rForm, lr])\nEach stage in the pipeline can be any kind of Estimator or Transformer. The pipeline itself is an Estimator. We use it in two steps:\n\npipeline.fit(dataset): Provides the dataset to the first stage. If it’s an Estimator, call the fit(dataset) method, then call transform(dataset) on the fitted object. If it’s a Transformer, call transform(dataset). Take the output and repeat on the next stage of the pipeline. This returns a PipelineModel, which is a Transformer containing all the fitted stages.\npipelinemodel.transform(dataset): Each stage of the pipeline is now a Transformer, because it was previously fit to data. Call `transform(dataset) on each stage in turn to produce the pipeline output.\n\nTo set the options for each pipeline stage, we can use ParamGridBuilder. This lets us specify a grid of options—a grid because when we use this in our pipeline, it will try every combination of the options.\nfrom pyspark.ml.tuning import ParamGridBuilder\n\nparams = ParamGridBuilder() \\\n  .addGrid(rForm.formula, [\n    \"lab ~ . + color:value1\",\n    \"lab ~ . + color:value1 + color:value2\"]) \\\n  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n  .addGrid(lr.regParam, [0.1, 2.0]) \\\n  .build()\nThis grid represents \\(2 \\times 3 \\times 2 = 12\\) different models to try."
  },
  {
    "objectID": "large-scale-data/spark-ml.html#evaluating-models",
    "href": "large-scale-data/spark-ml.html#evaluating-models",
    "title": "Machine Learning in Spark",
    "section": "",
    "text": "Now that we have specified the pipeline and the range of parameter options, we must tell Spark how we plan to evaluate our models. There is a basic Evaluator class to support evaluation, with several specific kinds of evaluator available. The BinaryClassificationEvaluator lets us choose from a variety of classification metrics; here we will use the AUC:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator() \\\n  .setMetricName(\"areaUnderROC\") \\\n  .setRawPredictionCol(\"prediction\") \\\n  .setLabelCol(\"label\")\nNotice we have also told it which column the predictions will be in and where the true labels are, so it can compare prediction to truth.\nNow we need a strategy for Spark to evaluate each model. One way is a train/validation split: split the data into two pieces, train on the training piece, and evaluate on the test piece. This is all on the original test set created with df.randomSplit(), since we are saving the original test set for final evaluation.\nfrom pyspark.ml.tuning import TrainValidationSplit\n\ntvs = TrainValidationSplit() \\\n  .setTrainRatio(0.75) \\\n  .setEstimatorParamMaps(params) \\\n  .setEstimator(pipeline) \\\n  .setEvaluator(evaluator)\ntvs is now an object that can be fit to data, and will produce a model object representing the fitted model:\ntvsFitted = tvs.fit(train)\nThis sets the parameters in the pipeline object, calls fit() on it, gets the predictions via transform(), evaluates them, and repeats for all parameter combinations. tvsFitted is now a TrainValidationSplitModel object, which is a Transformer. We use its transform() method to make predictions with the best model (as evaluated by our evaluator) on our final held-out test set, then get the final AUC:\nevaluator.evaluate(tvsFitted.transform(test))\nWe might also want to get the tuning parameters of the best model. We had TrainValidationSplit estimate an entire pipeline, and so the “best model” is actually a PipelineModel from fitting our pipeline. This is stored in tvsFitted.bestModel.\nThis object contains a .stages attribute giving all the stages; in our case, this is a list of the fitted RFormula (rform) and the fitted logistic regression (lr). So we can get parameters from the logistic regression:\ntvsFitted.bestModel.stages[1].getRegParam()\ntvsFitted.bestModel.stages[1].getElasticNetParam()\nBesides TrainValidationSplit, there is also a CrossValidator that works the same way, but does K-fold cross-validation to get the error estimates."
  },
  {
    "objectID": "large-scale-data/spark-ml.html#reviewing-the-pipeline",
    "href": "large-scale-data/spark-ml.html#reviewing-the-pipeline",
    "title": "Machine Learning in Spark",
    "section": "",
    "text": "We’ve seen a lot of pieces: Estimators, transformers, tools for coding variables, tools for scaling and transforming, models, evaluators, hyperparameter tuning, and so on. It’s hard to see how this all fits together.\nThe basic pipeline is meant to look like this:\n\n\n\n\n\nflowchart TD\n    data[Data]\n    rform[Make feature/label columns]\n    scale[Scale and transform data]\n    fit[Fit model]\n    predict[Make predictions]\n\n    data --&gt; rform\n    rform --&gt; scale\n    scale --&gt; fit\n    fit --&gt; predict\n\n\n\n\n\n\nEach step in this pipeline is one or more Transformer or Estimator objects, and the entire pipeline is represented as a Pipeline object with multiple stages. A Pipeline is itself an Estimator: the entire pipeline can be fit to data and then used to transform new data.\nBut pipelines have parameters that need to be tuned to optimize some evaluation metric. And we need some kind of train/test split to evaluate fairly. So we use a tuner like TrainValidationSplit:\n\n\n\n\n\nflowchart TD\n    split[\"Split training and test data &lt;br&gt;(TrainValidationSplit)\"]\n    subgraph pipeline[\"Pipeline() object\"]\n        rform[Make feature/label columns]\n        scale[Scale and transform data]\n        fit[Fit model]\n\n        rform --&gt; scale\n        scale --&gt; fit\n    end\n\n    split--&gt;|Training data|pipeline\n\n    fitted[\"Fitted pipeline&lt;br&gt;(PipelineModel)\"]\n    fit --&gt; fitted\n    split--&gt;|Test data|fitted\n\n    predictions[Predictions]\n    fitted --&gt; predictions\n\n    eval[Evaluator]\n    predictions --&gt; eval\n\n\n\n\n\n\nThis may need to be repeated for each combination of parameters we want to test, so we can choose the best."
  },
  {
    "objectID": "large-scale-data/spark-ml.html#saving-models-for-later",
    "href": "large-scale-data/spark-ml.html#saving-models-for-later",
    "title": "Machine Learning in Spark",
    "section": "",
    "text": "If you’re working on a model for a product, once you arrive at a final model, you’re going to want to be able to use it later. Fitted models (such as fitted PipelineModel objects) have a write() method that can be used to save them:\ntvsFitted.write().save(\"path/to/fitted-logit-model\")\nAs with all Spark operations that involve reading and writing files, the Spark executors write most of the crucial data, so the path provided should be one that all executors have access to, such as a directory stored in HDFS or a shared filesystem.\nSince we saved a PipelineModel, we can also load the saved model back with PipelineModel.\nfrom pyspark.ml import PipelineModel\n\nreloaded_model = PipelineModel.load(\"path/to/fitted-logit-model\")\nThis is now a PipelineModel object we can use just like tvsFitted above, except we could have exited Spark in between saving the model and loading it. Now you can work with the same model across many Spark sessions or reuse it in many applications."
  },
  {
    "objectID": "large-scale-data/spark-ml.html#exercises",
    "href": "large-scale-data/spark-ml.html#exercises",
    "title": "Machine Learning in Spark",
    "section": "",
    "text": "Exercise 1 (Galaxy mass prediction) The hive_metastore.default.buzzard_dc_1 table contains data from a physical simulation of 111,172 galaxies. The data is described here. Review the description so you’re familiar with the data.\nWe would like to predict galaxy stellar masses (log.mass) using redshift and the magnitude variables u, g, r, i, z, and y.\nNote that Spark uses . to refer to entries in structs (records), so log.mass will be interpreted as accessing the mass field of the log object. You must rename the variables and remove any dots (even in the columns you do not use, like u.err) to avoid confusing it.\nFirst, set aside a final test set of 30% of the data, randomly selected. (Use the randomSplit() method of DataFrame objects to do so.) We will use this to evaluate your pipelines.\nNext, build three pipelines to predict log.mass. Each has parameters you should tune with 5-fold cross-validation:\n\nLinear regression. Tune the regularization parameters, and also consider using either (a) a linear function of the original features, (b) a quadratic function of them, or (c) a cubic function of them. The PolynomialExpansion transformer may be helpful.\nA decision tree (using pyspark.ml.regression.DecisionTreeRegressor). Tune the maximum depth and maximum number of bins.\nA random forest (using pyspark.ml.regression.RandomForestRegressor). Tune the maximum tree depth and the maximum number of bins.\n\nTune each to select the model with the best mean absolute error (MAE). Report the best predictor, its MAE, and its tuning parameters.\n\n\nExercise 2 (Machine learning for LendingClub) For this exercise, we’ll continue using the LendingClub data from ?@exr-spark-lendingclub.\nIf you were an investor deciding which loans to fund, you’d be interested in predicting whether the borrower would pay the loan back on time. You might split loans into three categories:\n\nLoans fully paid off, with no late payments\nLoans paid back, but late\nLoans never fully paid back\n\nThe loan_status variable records the status of loans. There are several possible codes:\n\nFully Paid: The loan has been paid off\nCurrent: The borrower has been paying on time, but still has more payments to make\nIn Grace Period: The borrower is up to 15 days late in their payments; no late fees are charged until they’re over 15 days late\nLate (16-30 days): The borrower is 16-30 days late in their payments and will be charged a late fee\nLate (31-120 days): The borrower is 31-120 days late in their payments\nDefault: The borrower is more than 120 days late in payments, despite repeated reminders from LendingClub. LendingClub will now begin closing the loan\nCharged Off: The borrower has defaulted on the loan and LendingClub no longer expects any further payments to be made. They may sell the loan to a debt collection agency, or the borrower may have declared bankruptcy, in which case a court will decide how much money LendingClub may receive.\n\nThe status gives the current status (at the time this data was downloaded), but even a current or fully paid loan may have been late in the past and charged a late fee. The total_rec_late_fee column records the total late fees paid by the borrower.\nCreate a notebook that completes the following tasks.\nCreate a new column categorizing loans into the three categories above. You’ll have to filter out loans that are current and have not been paid off yet, because we do not know their ultimate status – they may become late or default later.\nAn investor may want to predict this categorization using information available at the time the borrower applies for the loan, such as: annual income (annual_inc), the number of times they’ve been delinquent on other accounts in the past two years (delinq_2yrs), their reason for asking for the loan (purpose), how many mortgages the borrower has (mort_acc), how many bankruptcies the borrower has had (pub_rec_bankruptcies), the borrower’s FICO credit score (fico_range_high and fico_range_low), and the amount they owe on credit cards (max_bal_bc). (There are many other covariates in the data, but we should start simple.)\nExtract these features from the data. Set aside 30% of the data to use as a test set.\nOn the training set, use Spark’s MinMaxScaler to scale all variables to be on the same scale. Then fit three classifiers to predict the loan categorization:\n\nA decision tree (DecisionTreeClassifier), using the default tuning parameters\nA random forest (RandomForestClassifier), using the default tuning parameters (which fit 20 trees with a depth of 5)\nA multilayer perceptron with three layers of 40 nodes each (MultilayerPerceptronClassifier). The layers argument lets you specify the size of each layer, including the input and output layers; here you should have 20 inputs (from your predictors, including dummies for categorical variables) and 3 outputs (for the 3 classes).\n\nEvaluate the three classifiers on the test set and report their accuracy and F1 scores. (With MulticlassClassificationEvaluator, you’ll need metricName=\"f1\" and metricName=\"accuracy\".)\nTurn in your notebook that does this complete task, starting with loading the data and ending with saving the test set metrics to a file. Include a table of accuracy and F1 scores.\nHints: See ?@sec-creating-dataframes to see how to create a data frame containing the evaluation results, so you can save it to a file. Review ?@sec-spark-expressions, and following sections, to see how to manipulate and create columns in Spark."
  },
  {
    "objectID": "large-scale-data/web-scraping.html",
    "href": "large-scale-data/web-scraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "Sometimes, the data you want isn’t available through a convenient API or as a downloadable file. It’s presented in a web page, but they don’t provide any way to get the data out of that page.\nBut the data is right there in the page, so surely we can get it out somehow! That process is called scraping.\nTo scrape data from a website, we must:\n\nConnect to the right HTTP server.\nMake an HTTP request for the right Web page.\nParse the HTML representing the page.\nExtract the relevant text and data from the parsed HTML.\n\nWe’ve already seen how to do Step 2 in ?@sec-http. Let’s fill in the other pieces. It’s helpful to understand how the Internet works first, if only to better understand the kinds of error messages you’ll eventually run into.\n\n\nThe Internet is the collective name for all the computers and routers connected together in one big system that we use. Email, Web sites, video calls, your Internet-connected doorbell – they all use the Internet to communicate with other systems, even if they do not use HTTP or Web pages as we know them.\n\n\nA machine connected to the Internet is a host. Each host has a hostname. On a Mac or Linux machine, you can run the hostname command to see your computer’s hostname; mine is currently odobenus.local.\nThat hostname means I’m on a network called local and my computer is called odobenus, apparently because I had a fondness for walruses when I first named it.\nHostnames can be assembled into domain names, which identify hosts on the Internet. For example, andrew.cmu.edu is a fully qualified domain name. Domain names are hierarchical, separated by dots, and are read right-to-left:\n\n[root zone]: Conceptually, every fully qualified domain name is part of the root zone, which is controlled by the Internet Corporation for Assigned Names and Numbers (ICANN). (ICANN was originally run under contract with the US Department of Commerce, but since 2016 it is independent, operating with input from 111 countries.)\nedu: A top-level domain (TLD) controlled by the organization Educause.\ncmu: Carnegie Mellon’s domain name.\nandrew: A specific host in Carnegie Mellon’s network.\n\nTop-level domains are created under ICANN’s authority, granting specific organizations authority to operate specific TLDs. Those TLDs (like .edu or .org) then sell name registrations to organizations like CMU. CMU then can create its own domains underneath its cmu.edu registration, acting as its own registration authority for names like stat.cmu.edu and www.cmu.edu.\n(Incidentally, it is quite important that domain names are hierarchical. This is how you know that securepayments.yourbank.com is run by the same people who run yourbank.com, whereas securepayments.yourbank.com.tech.mafia.ru is run by the Russian Mafia, despite containing the same substring. Phishing sites will often use this trick to try to confuse you; remember to read right-to-left!)\nNot every computer has a publicly accessible fully qualified domain name. My laptop currently does not, for example; outside of my local network, the hostname odobenus.local means nothing to anyone.\nDomain names have to follow certain rules: for example, they do not contain spaces or underscore characters. Note that http://www.stat.cmu.edu or andrew.cmu.edu/foo/bar are not domain names; they are Uniform Resource Locators and refer to specific services hosted on specific domain names.\n\n\n\nNow, a domain name doesn’t get you much. If I want my computer to send something to the host at andrew.cmu.edu, how am I supposed to do that? I need a way to know which physical machine to deliver to.\nInternet routers and switches don’t know how to find domain names, but they do understand IP addresses. An IP address is a numerical address; every machine connected to the public Internet has an IP address. (We’ll skip NAT for now.)\nIPv4 uses addresses like 172.16.254.1, with four parts each containing an 8-bit (0 to 255) number; IPv6, the successor, uses addresses like 2001:db8:0:1234:0:567:8:1, with eight parts each containing a 16-bit number encoded in hexadecimal. (The encoding is just for humans to look at; computers just look at the full 32-bit or 128-bit numbers.)\nCrucially, IP addresses are hierarchical as well. For example, Carnegie Mellon owns the entire block of addresses from 128.2.0.0 to 128.2.255.255, which includes every address beginning with 128.2. The details are a bit out of scope here, but using BGP, CMU’s routers announce to other routers they’re connected to, “Hey, I know how to deliver to any address starting with 128.2”, and those routers advertise to their neighbors “Hey, I have a way to get to 128.2,” and so on, and so every router on the Internet knows someone who knows someone who knows someone who can deliver the message. This involves something called the Border Gateway Protocol.\nThe Internet is, basically, a big extended family where if you get a parking ticket, your sister says “oh, talk to our aunt, she knows the sister of a guy who was roommates with the cousin of the county clerk”, and the message gets passed from step to step until it gets to the right person.\nBut I don’t want to type in 128.2.12.64 to get the website at stat.cmu.edu. I want to just type in stat.cmu.edu. How do I do that?\n\n\n\nThe answer: the Domain Name System. When I type in stat.cmu.edu, my computer sends a DNS query to its friendly neighborhood DNS resolver (usually run by your Internet Service Provider). The DNS resolver follows several steps:\n\n[root zone]: It asks DNS servers run by the root zone where to find the DNS servers for .edu. A master list of root DNS servers and their IP addresses has to be distributed manually; several organizations maintain root zone servers on behalf of ICANN. The root zone server responds with the IP address of a DNS server for .edu, such as 2001:501:b1f9::30.\nedu: It asks the DNS server for .edu where to find the DNS server for cmu.edu, and receives a response like 128.237.148.168.\ncmu: It asks the DNS server for cmu.edu where to find stat.cmu.edu, and receives a response like 128.2.12.64.\n\nObviously this involves a lot of back-and-forth communication, so resolvers usually have a cache. Every DNS server gives a time-to-live for its responses, indicating how long they can be relied upon; the resolver saves all responses it has received for that time period, so subsequent requests can be given the same answer.\nTypical TTLs are on the range of hours to a day or two, which is why sometimes after website maintenance it can take a while for your access to be restored – your resolver might have an old invalid DNS response cached.\nWhen you see error messages like “example.com could not be found”, your computer could not find a record for it in the Domain Name System.\nSome systems distribute data via DNS. Some organizations run DNSBLs or RBLs (DNS Blackhole List or Real-time Blackhole List), which provide ways to query if a domain name or host is known to be involved in spam mail. If your email server receives a message from 192.168.42.23, it might do a DNS query for 23.42.168.192.dnsbl.example.net; if the example.net RBL indicates that this IP is known to send spam, it will return an IP address, otherwise it will return a message that no such record is known.\n\n\n\nOnce you have received the IP address corresponding to a domain name, how do you communicate with it?\nThere are layers here; seven layers, specifically. Your computer needs a physical connection to other computers, either by cable or via electromagnetic field; communication over that connection has to be coordinated; messages have to be sent over that connection; those messages need to be reassembled into larger pieces with meaning; and those pieces need to be given meaning.\nWe’ll skip the physical layers. Suffice to say that Ethernet cables or Wi-Fi connections are involved.\nLet’s talk about how we send messages and reassemble them. There are several ways to do this, but the one most often used is the Transmission Control Protocol.\nTCP is a way of delivering messages between machines. It does not discriminate on the content of those messages; they may involve emails, video calls, Web pages, DNS queries, World of Warcraft game data, or anything else. TCP does not care or know anything about the specific uses of the messages.\nTCP merely provides a way to deliver messages. It does so in steps:\n\nYour computer sends a TCP message asking to connect to a particular server. That server replies with a message saying “Sounds good to me”, and your computer replies with a message saying “Great, let’s get to it”. This is the connection handshake. It’s what’s happening when you see “connecting to host…” messages.\nYour computer takes the content of the message it wants to send and breaks it up into small pieces, called packets. Each packet is stamped with its destination IP address and a number indicating the order they’re supposed to be reassembled in. Your computer sends these to be delivered to the server.\nAs the server receives the packets, it sends acknowledgments (“ACK”). If a packet is lost, your computer can re-send it.\nThe server may reply with messages of its own, delivered in the same way, and messages can flow back and forth until one computer decides to close the connection and sends a “FIN” message (which has to be ACKed and matched with another FIN from the other computer).\n\nTCP is meant to be robust: if packets are lost (which happens surprisingly often!) they are re-sent, and packets can arrive in any order and be reassembled.\nWhen you see error messages like “Could not connect to host” or “Connection refused”, there was a problem at step 1: the server failed to reply to your TCP connection request. Maybe there is no server actually listening at that IP address, or that computer is temporarily disconnected.\nNow, if we connect to a server and make an HTTP request, what do we get?\n\n\n\n\nHTML, the HyperText Markup Language, is a way of marking up text with various attributes and features to define a structure – a structure of paragraphs, boxes, headers, and so on, which can be given colors and styles and sizes with another language, CSS (Cascading Style Sheets).\nFor our purposes we don’t need to worry about CSS, since we don’t care what a page looks like, just what’s included in its HTML.\nHTML defines a hierarchical tree structure. Let’s look at a minimal example:\n&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;title&gt;An Awesome Website&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1 id=\"awesome\"&gt;Awesome Website&lt;/h1&gt;\n    &lt;p id=\"intro\"&gt;\n      This is a paragraph of text linking\n      to &lt;a href=\"http://example.com/page\"&gt;another Web page.&lt;/a&gt;\n    &lt;p&gt;This is another paragraph to introduce data &amp; stuff:\n      &lt;table class=\"datatable\" id=\"bigdata\"&gt;\n        &lt;thead&gt;\n          &lt;tr&gt;&lt;th&gt;Key&lt;/th&gt;&lt;th&gt;Value&lt;/th&gt;&lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n          &lt;tr id=\"importantrow\"&gt;&lt;td&gt;Foo&lt;/td&gt;&lt;td&gt;Bar&lt;/td&gt;&lt;/tr&gt;\n          &lt;tr&gt;&lt;td&gt;Baz&lt;/td&gt;&lt;td&gt;Bam&lt;/td&gt;&lt;/tr&gt;\n        &lt;/tbody&gt;\n      &lt;/table&gt;\n      &lt;!-- This is a comment and is ignored --&gt;\n      &lt;p&gt;Notice the use of &lt;thead&gt; and &lt;tbody&gt; tags, which are\n      actually optional.\n  &lt;/body&gt;\n&lt;/html&gt;\nNotice some features of HTML:\n\nTags: Tags are named inside square brackets, like &lt;h1&gt;. There is a set of tags with predefined meanings, like &lt;p&gt; for paragraphs and &lt;table&gt; for tables. Tag names are case-insensitive, so &lt;P&gt; and &lt;p&gt; mean the same thing.\nTag hierarchy: Tags enclose text and other tags: tags open with a form like &lt;p&gt; and close with &lt;/p&gt;, and everything in between is enclosed in those tags. All tags have to be closed, except those that don’t. (Since people writing web pages were very bad at remembering to close tags, browsers now have standard rules for inferring when you meant to close a tag; notice the paragraphs above aren’t closed. Tables can’t be inside paragraphs, so the previous &lt;p&gt; does not contain the &lt;table&gt;.)\nAttributes: Tags can have attributes, which are key-value pairs describing the content inside them. Many attributes have specific meaning: id is used for unique identifiers for elements, which can be used in JavaScript or CSS to modify those elements, and a class can be assigned to many elements which should somehow behave in the same way.\nEscaping: Characters like &lt;, &gt;, and & have specific meanings in HTML. If you want to write &lt; without it starting a new tag, you have to escape it by writing &lt;. There are many escapes, like &copy; for the copyright symbol, and numeric escapes for specifying arbitrary Unicode characters. These are called “HTML entities”.\nWhitespace: Whitespace has no meaning in HTML. You could put the above HTML document on one line or four hundred, if you’d like. Spaces are collapsed: writing two spaces between words is the same as writing one space between words. Hence lines that wrap, above, don’t have excess spaces because of the indentation.\n\nConceptually, HTML tags form a tree. Each tag has a parent (the enclosing tag) and children (the tags within). For example, above, the &lt;body&gt; tag’s parent is &lt;html&gt;, while its direct children are &lt;h1&gt;, &lt;table&gt;, and &lt;p&gt; tags.\n\n\nTo pull information out of an HTML document, we may need only the contents of a few specific tags. There are several ways to do this, but they come down to needing a selector: some specification of the type or name of the tag we want.\nThere are several common types of selector. The simplest is the CSS selector, used when making Cascading Style Sheets. A CSS selector might look like this: .datatable tr#importantrow td.\nThat means:\n\n.datatable: Any element with the class attribute “datatable”.\ntr#importantrow: Any tr element with the id attribute “importantrow”.\ntd: Any &lt;td&gt; tag.\n\nThese are interpreted hierarchically, so put together in one selector, this identifies all td elements inside a tr whose ID is “importantrow” inside some tag with class “datatable”. This will match two td elements in the example above. (Note that the tbody is not in the selector, but that is not a problem; any td inside a tr#importantrow matches, even if there are enclosing tags in between.)\nTag IDs and classes are particularly important for selectors, since they’re often used to mark specific parts of the page, or tags with specific uses. A tag can only have one ID, and an ID can only appear once in a page; but a tag can have multiple classes, separated by spaces in the class attribute:\n&lt;div id=\"sidebar\" class=\"left nav fixed\"&gt;\n  ...\n&lt;/div&gt;\nWebsite authors add lots of class and ID attributes throughout a page because of their use in selectors. CSS is used to specify color, layout, backgrounds, sizes, and other features of tags on the page, and it uses selectors to identify which tags each rule applies to:\np {\n    font-family: serif;\n    font-size: 110%;\n    color: #333;\n}\n\n.datatable tr#importantrow td {\n    color: #f00;\n    font-weight: bold;\n}\nYou’ll see CSS included in &lt;style&gt; tags on the page, or the page header will specify external CSS files that the browser is supposed to download and apply.\n\n\n\nTo “parse” a particular data format means to read it and extract all the pieces into some form you can manipulate in code. For HTML, that means turning the text above, with all its tags and attributes in one big string, into some kind of Python or R data structure we can use.\nHTML’s complex structure makes it difficult to parse; the HTML standard chapter on syntax has headings going as deep as “13.2.5.80 Numeric character reference end state”. This means you should not try to write a parser yourself. Nor should you attempt to parse HTML with regular expressions.\nIn Python, we can use the Beautiful Soup package for parsing and working with HTML. It can parse HTML and then provides all kinds of functions to work with the tree of tags. See the installation instructions to get it installed.\nFor example, for the HTML shown above:\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(open(\"example.html\", \"r\").read())\n\nps = soup.find_all(\"p\") # get a list of the 3 &lt;p&gt; tags\n\nps[0][\"id\"] # \"intro\"\n\nps[-1].string # 'Notice the use of &lt;thead&gt; and &lt;tbody&gt; tags, which are\\n      actually optional.\\n  '\n\ntds = soup.css.select(\".datatable tr#importantrow td\")\n\ntds # [&lt;td&gt;Foo&lt;/td&gt;, &lt;td&gt;Bar&lt;/td&gt;]\nThe Beautiful Soup documentation gives an extensive tutorial on how to use it to access specific attributes of a page, so we don’t need to recap that here. Instead, let’s consider the question: How do you figure out what parts of the HTML you need to extract?\n\n\n\nBack in ?@sec-urls, we discussed the structure of URLs like http://www.example.com/foo/bar.html. When you parse a Web page, you may find URLs like this in a tags:\n&lt;a href=\"http://www.example.com/foo/bar.html\"&gt;the page&lt;/a&gt;\nThis is a complete URL. But just as there are absolute paths and relative paths for files on your computer, you can also use relative paths in URLs. For example, if we’re on the page http://www.example.com/foo/index.html, we can write a link like this:\n&lt;a href=\"bar.html\"&gt;the bar page&lt;/a&gt;\nThis is a relative URL. In this case, because we’re viewing it on a page in the http://www.example.com/foo/ directory, it points to http://www.example.com/foo/bar.html. Other relative paths include:\n\n../baz/index.html: points to http://www.example.com/baz/index.html, as .. represents going to the parent directory\n/index.html: points to http://www.example.com/index.html regardless of which directory on the site we are in, as the leading / always indicates the root\n\nSo when you extract URLs from links and other tags, you may need to parse those URLs to get the actual target URL, particularly if you want to make follow-up HTTP requests. Python’s urllib module provides tools to do this:\nimport urllib.parse\n\nurllib.parse.urljoin(\"http://www.example.com/foo/index.html\",\n                     \"bar.html\")\nurllib.parse.urljoin(\"http://www.example.com/foo/index.html\",\n                     \"../baz/index.html\")\nurllib.parse.urljoin(\"http://www.example.com/foo/index.html\",\n                     \"/index.html\")\nurllib.parse.urljoin(\"http://www.example.com/foo/index.html\",\n                     \"http://foo.example.com/index.html\")\nThe urljoin() method takes a base URL (the first argument) and a new URL (the second argument) and works out the absolute URL the new URL would point to.\n\n\n\n\nLet’s say you’d like to scrape some data from a web page. For instance, you’d like to pull information about books from an online bookstore: you’d like to get prices, review ratings, titles, and related information for an analysis of book markets.\nInstead of Amazon, let’s use Books to Scrape, an online store that exists to be scraped for demos like this.\n\n\nThe page HTML contains lots of tags. (Web pages are often full of tags many layers deep, for all the buttons, boxes, popups, images, links, headers, and other junk on every webpage.) How do we find what we need?\nStep 1 is to open the site in our web browser and find out which pages contain the information we need.\nStep 2 is to open those pages and find the tags we want. Your browser’s web developer tools may be useful for this.\n\nExercise 1 (In-class activity: select some book titles) Install Beautiful Soup using\npip install beautifulsoup4\nNow load it and Requests:\nfrom bs4 import BeautifulSoup\nimport requests\nNow pull the Books to Scrape website using Requests and use Beautiful Soup to parse the HTML:\nr = requests.get(\"http://books.toscrape.com\")\n\nsoup = BeautifulSoup(r.content)\nUse CSS selectors to extract a list of elements from which you can get:\n\nthe title of each book\na link to the page with details about the book\n\nRun the code and verify that it works.\n\n\n\n\n\n\nExercise 2 (Scraping some books) Let’s continue from Exercise 1 and scrape http://books.toscrape.com.\nFor each book, we’d like:\n\nThe book title\nIts current price\nIts UPC (Universal Product Code)\nA link to the page with further details about the book\n\nWrite a Python script that can be run from the command line that does this. It should write its output to standard output in CSV format, so you can run the command\npython scrape-books.py &gt; books.csv\nand get a file books.csv with four columns.\nTo do this, you’ll need to load each page of book listings from the site, use selectors to extract information, and load each book’s individual page to get the UPC. You’ll then store each book’s information somewhere so you can print it out, or just print it out immediately as you go.\nOrganize your code so that there are separate functions for getting information about books, and the printing of CSV output is done in separate code that calls those functions. That ensures you could later use the same functions to do something other than printing a CSV. Your CSV output should correctly handle book information containing commas, quotation marks, newlines, and any other characters with special meaning in CSVs."
  },
  {
    "objectID": "large-scale-data/web-scraping.html#the-internet",
    "href": "large-scale-data/web-scraping.html#the-internet",
    "title": "Web Scraping",
    "section": "",
    "text": "The Internet is the collective name for all the computers and routers connected together in one big system that we use. Email, Web sites, video calls, your Internet-connected doorbell – they all use the Internet to communicate with other systems, even if they do not use HTTP or Web pages as we know them.\n\n\nA machine connected to the Internet is a host. Each host has a hostname. On a Mac or Linux machine, you can run the hostname command to see your computer’s hostname; mine is currently odobenus.local.\nThat hostname means I’m on a network called local and my computer is called odobenus, apparently because I had a fondness for walruses when I first named it.\nHostnames can be assembled into domain names, which identify hosts on the Internet. For example, andrew.cmu.edu is a fully qualified domain name. Domain names are hierarchical, separated by dots, and are read right-to-left:\n\n[root zone]: Conceptually, every fully qualified domain name is part of the root zone, which is controlled by the Internet Corporation for Assigned Names and Numbers (ICANN). (ICANN was originally run under contract with the US Department of Commerce, but since 2016 it is independent, operating with input from 111 countries.)\nedu: A top-level domain (TLD) controlled by the organization Educause.\ncmu: Carnegie Mellon’s domain name.\nandrew: A specific host in Carnegie Mellon’s network.\n\nTop-level domains are created under ICANN’s authority, granting specific organizations authority to operate specific TLDs. Those TLDs (like .edu or .org) then sell name registrations to organizations like CMU. CMU then can create its own domains underneath its cmu.edu registration, acting as its own registration authority for names like stat.cmu.edu and www.cmu.edu.\n(Incidentally, it is quite important that domain names are hierarchical. This is how you know that securepayments.yourbank.com is run by the same people who run yourbank.com, whereas securepayments.yourbank.com.tech.mafia.ru is run by the Russian Mafia, despite containing the same substring. Phishing sites will often use this trick to try to confuse you; remember to read right-to-left!)\nNot every computer has a publicly accessible fully qualified domain name. My laptop currently does not, for example; outside of my local network, the hostname odobenus.local means nothing to anyone.\nDomain names have to follow certain rules: for example, they do not contain spaces or underscore characters. Note that http://www.stat.cmu.edu or andrew.cmu.edu/foo/bar are not domain names; they are Uniform Resource Locators and refer to specific services hosted on specific domain names.\n\n\n\nNow, a domain name doesn’t get you much. If I want my computer to send something to the host at andrew.cmu.edu, how am I supposed to do that? I need a way to know which physical machine to deliver to.\nInternet routers and switches don’t know how to find domain names, but they do understand IP addresses. An IP address is a numerical address; every machine connected to the public Internet has an IP address. (We’ll skip NAT for now.)\nIPv4 uses addresses like 172.16.254.1, with four parts each containing an 8-bit (0 to 255) number; IPv6, the successor, uses addresses like 2001:db8:0:1234:0:567:8:1, with eight parts each containing a 16-bit number encoded in hexadecimal. (The encoding is just for humans to look at; computers just look at the full 32-bit or 128-bit numbers.)\nCrucially, IP addresses are hierarchical as well. For example, Carnegie Mellon owns the entire block of addresses from 128.2.0.0 to 128.2.255.255, which includes every address beginning with 128.2. The details are a bit out of scope here, but using BGP, CMU’s routers announce to other routers they’re connected to, “Hey, I know how to deliver to any address starting with 128.2”, and those routers advertise to their neighbors “Hey, I have a way to get to 128.2,” and so on, and so every router on the Internet knows someone who knows someone who knows someone who can deliver the message. This involves something called the Border Gateway Protocol.\nThe Internet is, basically, a big extended family where if you get a parking ticket, your sister says “oh, talk to our aunt, she knows the sister of a guy who was roommates with the cousin of the county clerk”, and the message gets passed from step to step until it gets to the right person.\nBut I don’t want to type in 128.2.12.64 to get the website at stat.cmu.edu. I want to just type in stat.cmu.edu. How do I do that?\n\n\n\nThe answer: the Domain Name System. When I type in stat.cmu.edu, my computer sends a DNS query to its friendly neighborhood DNS resolver (usually run by your Internet Service Provider). The DNS resolver follows several steps:\n\n[root zone]: It asks DNS servers run by the root zone where to find the DNS servers for .edu. A master list of root DNS servers and their IP addresses has to be distributed manually; several organizations maintain root zone servers on behalf of ICANN. The root zone server responds with the IP address of a DNS server for .edu, such as 2001:501:b1f9::30.\nedu: It asks the DNS server for .edu where to find the DNS server for cmu.edu, and receives a response like 128.237.148.168.\ncmu: It asks the DNS server for cmu.edu where to find stat.cmu.edu, and receives a response like 128.2.12.64.\n\nObviously this involves a lot of back-and-forth communication, so resolvers usually have a cache. Every DNS server gives a time-to-live for its responses, indicating how long they can be relied upon; the resolver saves all responses it has received for that time period, so subsequent requests can be given the same answer.\nTypical TTLs are on the range of hours to a day or two, which is why sometimes after website maintenance it can take a while for your access to be restored – your resolver might have an old invalid DNS response cached.\nWhen you see error messages like “example.com could not be found”, your computer could not find a record for it in the Domain Name System.\nSome systems distribute data via DNS. Some organizations run DNSBLs or RBLs (DNS Blackhole List or Real-time Blackhole List), which provide ways to query if a domain name or host is known to be involved in spam mail. If your email server receives a message from 192.168.42.23, it might do a DNS query for 23.42.168.192.dnsbl.example.net; if the example.net RBL indicates that this IP is known to send spam, it will return an IP address, otherwise it will return a message that no such record is known.\n\n\n\nOnce you have received the IP address corresponding to a domain name, how do you communicate with it?\nThere are layers here; seven layers, specifically. Your computer needs a physical connection to other computers, either by cable or via electromagnetic field; communication over that connection has to be coordinated; messages have to be sent over that connection; those messages need to be reassembled into larger pieces with meaning; and those pieces need to be given meaning.\nWe’ll skip the physical layers. Suffice to say that Ethernet cables or Wi-Fi connections are involved.\nLet’s talk about how we send messages and reassemble them. There are several ways to do this, but the one most often used is the Transmission Control Protocol.\nTCP is a way of delivering messages between machines. It does not discriminate on the content of those messages; they may involve emails, video calls, Web pages, DNS queries, World of Warcraft game data, or anything else. TCP does not care or know anything about the specific uses of the messages.\nTCP merely provides a way to deliver messages. It does so in steps:\n\nYour computer sends a TCP message asking to connect to a particular server. That server replies with a message saying “Sounds good to me”, and your computer replies with a message saying “Great, let’s get to it”. This is the connection handshake. It’s what’s happening when you see “connecting to host…” messages.\nYour computer takes the content of the message it wants to send and breaks it up into small pieces, called packets. Each packet is stamped with its destination IP address and a number indicating the order they’re supposed to be reassembled in. Your computer sends these to be delivered to the server.\nAs the server receives the packets, it sends acknowledgments (“ACK”). If a packet is lost, your computer can re-send it.\nThe server may reply with messages of its own, delivered in the same way, and messages can flow back and forth until one computer decides to close the connection and sends a “FIN” message (which has to be ACKed and matched with another FIN from the other computer).\n\nTCP is meant to be robust: if packets are lost (which happens surprisingly often!) they are re-sent, and packets can arrive in any order and be reassembled.\nWhen you see error messages like “Could not connect to host” or “Connection refused”, there was a problem at step 1: the server failed to reply to your TCP connection request. Maybe there is no server actually listening at that IP address, or that computer is temporarily disconnected.\nNow, if we connect to a server and make an HTTP request, what do we get?"
  },
  {
    "objectID": "large-scale-data/web-scraping.html#html",
    "href": "large-scale-data/web-scraping.html#html",
    "title": "Web Scraping",
    "section": "",
    "text": "HTML, the HyperText Markup Language, is a way of marking up text with various attributes and features to define a structure – a structure of paragraphs, boxes, headers, and so on, which can be given colors and styles and sizes with another language, CSS (Cascading Style Sheets).\nFor our purposes we don’t need to worry about CSS, since we don’t care what a page looks like, just what’s included in its HTML.\nHTML defines a hierarchical tree structure. Let’s look at a minimal example:\n&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;title&gt;An Awesome Website&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1 id=\"awesome\"&gt;Awesome Website&lt;/h1&gt;\n    &lt;p id=\"intro\"&gt;\n      This is a paragraph of text linking\n      to &lt;a href=\"http://example.com/page\"&gt;another Web page.&lt;/a&gt;\n    &lt;p&gt;This is another paragraph to introduce data &amp; stuff:\n      &lt;table class=\"datatable\" id=\"bigdata\"&gt;\n        &lt;thead&gt;\n          &lt;tr&gt;&lt;th&gt;Key&lt;/th&gt;&lt;th&gt;Value&lt;/th&gt;&lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n          &lt;tr id=\"importantrow\"&gt;&lt;td&gt;Foo&lt;/td&gt;&lt;td&gt;Bar&lt;/td&gt;&lt;/tr&gt;\n          &lt;tr&gt;&lt;td&gt;Baz&lt;/td&gt;&lt;td&gt;Bam&lt;/td&gt;&lt;/tr&gt;\n        &lt;/tbody&gt;\n      &lt;/table&gt;\n      &lt;!-- This is a comment and is ignored --&gt;\n      &lt;p&gt;Notice the use of &lt;thead&gt; and &lt;tbody&gt; tags, which are\n      actually optional.\n  &lt;/body&gt;\n&lt;/html&gt;\nNotice some features of HTML:\n\nTags: Tags are named inside square brackets, like &lt;h1&gt;. There is a set of tags with predefined meanings, like &lt;p&gt; for paragraphs and &lt;table&gt; for tables. Tag names are case-insensitive, so &lt;P&gt; and &lt;p&gt; mean the same thing.\nTag hierarchy: Tags enclose text and other tags: tags open with a form like &lt;p&gt; and close with &lt;/p&gt;, and everything in between is enclosed in those tags. All tags have to be closed, except those that don’t. (Since people writing web pages were very bad at remembering to close tags, browsers now have standard rules for inferring when you meant to close a tag; notice the paragraphs above aren’t closed. Tables can’t be inside paragraphs, so the previous &lt;p&gt; does not contain the &lt;table&gt;.)\nAttributes: Tags can have attributes, which are key-value pairs describing the content inside them. Many attributes have specific meaning: id is used for unique identifiers for elements, which can be used in JavaScript or CSS to modify those elements, and a class can be assigned to many elements which should somehow behave in the same way.\nEscaping: Characters like &lt;, &gt;, and & have specific meanings in HTML. If you want to write &lt; without it starting a new tag, you have to escape it by writing &lt;. There are many escapes, like &copy; for the copyright symbol, and numeric escapes for specifying arbitrary Unicode characters. These are called “HTML entities”.\nWhitespace: Whitespace has no meaning in HTML. You could put the above HTML document on one line or four hundred, if you’d like. Spaces are collapsed: writing two spaces between words is the same as writing one space between words. Hence lines that wrap, above, don’t have excess spaces because of the indentation.\n\nConceptually, HTML tags form a tree. Each tag has a parent (the enclosing tag) and children (the tags within). For example, above, the &lt;body&gt; tag’s parent is &lt;html&gt;, while its direct children are &lt;h1&gt;, &lt;table&gt;, and &lt;p&gt; tags.\n\n\nTo pull information out of an HTML document, we may need only the contents of a few specific tags. There are several ways to do this, but they come down to needing a selector: some specification of the type or name of the tag we want.\nThere are several common types of selector. The simplest is the CSS selector, used when making Cascading Style Sheets. A CSS selector might look like this: .datatable tr#importantrow td.\nThat means:\n\n.datatable: Any element with the class attribute “datatable”.\ntr#importantrow: Any tr element with the id attribute “importantrow”.\ntd: Any &lt;td&gt; tag.\n\nThese are interpreted hierarchically, so put together in one selector, this identifies all td elements inside a tr whose ID is “importantrow” inside some tag with class “datatable”. This will match two td elements in the example above. (Note that the tbody is not in the selector, but that is not a problem; any td inside a tr#importantrow matches, even if there are enclosing tags in between.)\nTag IDs and classes are particularly important for selectors, since they’re often used to mark specific parts of the page, or tags with specific uses. A tag can only have one ID, and an ID can only appear once in a page; but a tag can have multiple classes, separated by spaces in the class attribute:\n&lt;div id=\"sidebar\" class=\"left nav fixed\"&gt;\n  ...\n&lt;/div&gt;\nWebsite authors add lots of class and ID attributes throughout a page because of their use in selectors. CSS is used to specify color, layout, backgrounds, sizes, and other features of tags on the page, and it uses selectors to identify which tags each rule applies to:\np {\n    font-family: serif;\n    font-size: 110%;\n    color: #333;\n}\n\n.datatable tr#importantrow td {\n    color: #f00;\n    font-weight: bold;\n}\nYou’ll see CSS included in &lt;style&gt; tags on the page, or the page header will specify external CSS files that the browser is supposed to download and apply.\n\n\n\nTo “parse” a particular data format means to read it and extract all the pieces into some form you can manipulate in code. For HTML, that means turning the text above, with all its tags and attributes in one big string, into some kind of Python or R data structure we can use.\nHTML’s complex structure makes it difficult to parse; the HTML standard chapter on syntax has headings going as deep as “13.2.5.80 Numeric character reference end state”. This means you should not try to write a parser yourself. Nor should you attempt to parse HTML with regular expressions.\nIn Python, we can use the Beautiful Soup package for parsing and working with HTML. It can parse HTML and then provides all kinds of functions to work with the tree of tags. See the installation instructions to get it installed.\nFor example, for the HTML shown above:\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(open(\"example.html\", \"r\").read())\n\nps = soup.find_all(\"p\") # get a list of the 3 &lt;p&gt; tags\n\nps[0][\"id\"] # \"intro\"\n\nps[-1].string # 'Notice the use of &lt;thead&gt; and &lt;tbody&gt; tags, which are\\n      actually optional.\\n  '\n\ntds = soup.css.select(\".datatable tr#importantrow td\")\n\ntds # [&lt;td&gt;Foo&lt;/td&gt;, &lt;td&gt;Bar&lt;/td&gt;]\nThe Beautiful Soup documentation gives an extensive tutorial on how to use it to access specific attributes of a page, so we don’t need to recap that here. Instead, let’s consider the question: How do you figure out what parts of the HTML you need to extract?\n\n\n\nBack in ?@sec-urls, we discussed the structure of URLs like http://www.example.com/foo/bar.html. When you parse a Web page, you may find URLs like this in a tags:\n&lt;a href=\"http://www.example.com/foo/bar.html\"&gt;the page&lt;/a&gt;\nThis is a complete URL. But just as there are absolute paths and relative paths for files on your computer, you can also use relative paths in URLs. For example, if we’re on the page http://www.example.com/foo/index.html, we can write a link like this:\n&lt;a href=\"bar.html\"&gt;the bar page&lt;/a&gt;\nThis is a relative URL. In this case, because we’re viewing it on a page in the http://www.example.com/foo/ directory, it points to http://www.example.com/foo/bar.html. Other relative paths include:\n\n../baz/index.html: points to http://www.example.com/baz/index.html, as .. represents going to the parent directory\n/index.html: points to http://www.example.com/index.html regardless of which directory on the site we are in, as the leading / always indicates the root\n\nSo when you extract URLs from links and other tags, you may need to parse those URLs to get the actual target URL, particularly if you want to make follow-up HTTP requests. Python’s urllib module provides tools to do this:\nimport urllib.parse\n\nurllib.parse.urljoin(\"http://www.example.com/foo/index.html\",\n                     \"bar.html\")\nurllib.parse.urljoin(\"http://www.example.com/foo/index.html\",\n                     \"../baz/index.html\")\nurllib.parse.urljoin(\"http://www.example.com/foo/index.html\",\n                     \"/index.html\")\nurllib.parse.urljoin(\"http://www.example.com/foo/index.html\",\n                     \"http://foo.example.com/index.html\")\nThe urljoin() method takes a base URL (the first argument) and a new URL (the second argument) and works out the absolute URL the new URL would point to."
  },
  {
    "objectID": "large-scale-data/web-scraping.html#a-scraping-tutorial",
    "href": "large-scale-data/web-scraping.html#a-scraping-tutorial",
    "title": "Web Scraping",
    "section": "",
    "text": "Let’s say you’d like to scrape some data from a web page. For instance, you’d like to pull information about books from an online bookstore: you’d like to get prices, review ratings, titles, and related information for an analysis of book markets.\nInstead of Amazon, let’s use Books to Scrape, an online store that exists to be scraped for demos like this.\n\n\nThe page HTML contains lots of tags. (Web pages are often full of tags many layers deep, for all the buttons, boxes, popups, images, links, headers, and other junk on every webpage.) How do we find what we need?\nStep 1 is to open the site in our web browser and find out which pages contain the information we need.\nStep 2 is to open those pages and find the tags we want. Your browser’s web developer tools may be useful for this.\n\nExercise 1 (In-class activity: select some book titles) Install Beautiful Soup using\npip install beautifulsoup4\nNow load it and Requests:\nfrom bs4 import BeautifulSoup\nimport requests\nNow pull the Books to Scrape website using Requests and use Beautiful Soup to parse the HTML:\nr = requests.get(\"http://books.toscrape.com\")\n\nsoup = BeautifulSoup(r.content)\nUse CSS selectors to extract a list of elements from which you can get:\n\nthe title of each book\na link to the page with details about the book\n\nRun the code and verify that it works."
  },
  {
    "objectID": "large-scale-data/web-scraping.html#exercises",
    "href": "large-scale-data/web-scraping.html#exercises",
    "title": "Web Scraping",
    "section": "",
    "text": "Exercise 2 (Scraping some books) Let’s continue from Exercise 1 and scrape http://books.toscrape.com.\nFor each book, we’d like:\n\nThe book title\nIts current price\nIts UPC (Universal Product Code)\nA link to the page with further details about the book\n\nWrite a Python script that can be run from the command line that does this. It should write its output to standard output in CSV format, so you can run the command\npython scrape-books.py &gt; books.csv\nand get a file books.csv with four columns.\nTo do this, you’ll need to load each page of book listings from the site, use selectors to extract information, and load each book’s individual page to get the UPC. You’ll then store each book’s information somewhere so you can print it out, or just print it out immediately as you go.\nOrganize your code so that there are separate functions for getting information about books, and the printing of CSV output is done in separate code that calls those functions. That ensures you could later use the same functions to do something other than printing a CSV. Your CSV output should correctly handle book information containing commas, quotation marks, newlines, and any other characters with special meaning in CSVs."
  },
  {
    "objectID": "large-scale-data/rest-apis.html",
    "href": "large-scale-data/rest-apis.html",
    "title": "REST APIs",
    "section": "",
    "text": "Lets say you’re responsible for software that manages a large database. Maybe it’s your database of customers, or of students, or the database of all products sold by your company; or maybe it’s the geographic data behind Google Maps, or the millions of words of text in Wikipedia. Many people—other teams in your company, random people in the public, other software systems—want to interact with your database to do things.\nYou could give them direct access. If you use a SQL database system, you could give them access to write their own SQL queries. (Most SQL database software supports permissions, so you could allow certain users to make SELECT queries but not to INSERT or UPDATE data.) But that can cause problems. Your users might write large, complicated queries that are slow to run, and now it’s your problem that your database is working slowly. Also, your users will depend on your specific database schema: if you want to reorganize, change column names, or otherwise change how data is stored, you must now coordinate with everyone who happens to send a SELECT query to you.\nWhen you learned object-oriented programming, you learned about the principle of encapsulation. The internal details of a class should be hidden from the outside world, which can only interact with it through a publicly declared interface. This means the internal details of the class can change—or it can be replaced by an entirely different implementation—without any users having to know the difference.\nWe often refer to the public interfaces of a product, such as the methods exposed by its classes, as its application programming interface (API). You might hear software engineers referring to a library that provides “an API for reticulating splines”, or complaining that a particular API is too complicated and hard to use. PostgreSQL has a special API for submitting SQL queries and fetching results.\nIn the spirit of encapsulation, it seems like large databases should have APIs to do common tasks. You can provide an API for the rest of your company to use, and then you can improve the backend (the schema, SQL engine, caching system, and whatever else) without having to coordinate with the rest of the company, which only uses your API. And you’d like this API to be available over the network, so different servers—or random people on their laptops—can make requests to the API.\nSo how do you build a system that is widely accessible, allows authorized users to submit requests, and returns results, in a way that is convenient, easy to use, and easy to update?\nThere have been many solutions. For example, SOAP defines a way to send and receive requests that are written in XML; it was very popular in the early 2000s but soon gained a reputation for being verbose and overly complicated.\nA key realization was that HTTP already provides for ways to make requests of remote machines and receive responses from them. Why not provide an API as an HTTP server that responds to specific queries?\n\n\nLet’s briefly recap HTTP, the HyperText Transport Protocol underlying REST APIs and every web page you visit. HTTP is a way to make requests to servers for webpages (or other resources) and receive their responses, and it communicates its data by using protocols like TCP.\n\n\nResources available over HTTP (and some other protocols) are identified with Uniform Resource Locators.\nURLs come in parts; here’s the syntax as given by Wikipedia:\nURI = scheme:[//authority]path[?query][#fragment]\nauthority = [userinfo@]host[:port]\nLet’s break that down:\n\nScheme: The type of thing we want. Common schemes are http and https for Web pages, mailto for email links, file for files on the local file system, and so on.\nuserinfo: A username and potentially a password (as username:password). Optional. Sometimes used for FTP links so the links specifies the username to use. Normally it’s not good to include a password in a URL, because then it’s visible to anyone.\nhost: A domain name or IP address for the server that owns this content.\nport: The port number to use when connecting.\npath: A sequence of parts separated by slashes (/ – no, these are not backslashes). The path specifies the specific resource or file being requested.\nquery: Optionally, a query, starting with a question mark. Queries are typically key-value pairs separated by ampersands, as in foo=bar&baz=bin.\nfragment: A name, preceded by the hash sign #, pointing to a specific part of the document. For example, in an HTML page, the fragment may refer to a specific section or heading on the page.\n\nSome URL examples:\nhttps://en.wikipedia.org/wiki/URL?thing=baz\n\nhttps://scholar.google.com/citations?user=AxT2apgAAAAJ&hl=en\n\nftp://alex:hunter2@ftp.example.com/home/alex/passwords\nURLs can only contain a specific set of characters (mainly the Latin alphabet, digits, and some punctuation and symbol characters). Everything else must be “URL encoded”, meaning the character is replaced by a percent sign and two hexadecimal digits giving its UTF-8 numeric encoding. (If the encoding is more than one byte, each byte is encoded separately.)\nFor example, the space character is not permitted in URLs, so it is replaced with %20. Similarly, if we want to use = or & inside a query argument without it being interpreted as separating key-value pairs, we can URL-encode them.\n\n\n\nAn HTTP request is sent by a client (i.e. you) to a server to request a specific resource. It consists of several parts. For example, here’s a request I sent to Wikipedia for the page https://en.wikipedia.org/wiki/World_Wide_Web:\nGET /wiki/World_Wide_Web HTTP/2.0\nHost: en.wikipedia.org\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:64.0) Gecko/20100101 Firefox/64.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate, br\nDNT: 1\nConnection: keep-alive\nCookie: enwikiUserName=Cap%27n+Refsmmat; [more redacted]\nUpgrade-Insecure-Requests: 1\nCache-Control: max-age=0\nIf-Modified-Since: Sun, 27 Jan 2019 03:20:05 GMT\nThere’s a lot going on here. Let’s break it down.\nGET /wiki/World_Wide_Web HTTP/2.0\nHost: en.wikipedia.org\nThere are several types of HTTP requests, but this is a GET request, using HTTP version 2.0. (See Section 1.1.3 for other request types.) The first line specifies the type of request and the specific resource I’m requesting, /wiki/World_Wide_Web. The second line is the beginning of the headers, providing metadata about the request. These headers are in Key: value format, one per line. Here are a few selected headers and their meanings:\n\nHost: Specifies domain name I am making the request for – it’s possible for multiple different domain names to have the same IP address, so I have to tell the receiving server which domain name I’m interested in.\nUser-Agent: Tells the server what kind of program I am. Servers sometimes use this to tell what version of a web page to serve – maybe a page uses some features that only work in Chrome or in specific versions of Internet Explorer. If you’re writing a program that makes HTTP requests, it’s considered good practice to set a User-Agent identifying your program, so if server operators notice it causing a problem, they know who to complain to.\nAccept: Specifies the formats of responses I am willing to accept. We’ll see later that some APIs let us specify if we want responses as CSV, JSON, or other formats.\nAccept-Encoding: Here my browser says it is willing to receive content compressed with gzip, DEFLATE, or Brotli. The server can automatically compress its responses, and the HTTP client automatically decompresses them, saving network bandwidth.\n\nAfter the server receives this request, it can send a response. Responses have a similar format to requests. They begin with a status line and headers:\nHTTP/2.0 200 OK\ndate: Wed, 30 Jan 2019 19:14:42 GMT\ncontent-type: text/html; charset=UTF-8\ncontent-encoding: gzip\n[...]\nThe first line gives the status code 200, meaning “OK”. Other response codes include the famous 404 (“not found”), 403 (“forbidden”), 301 (“moved permanently”), and 418 (“I’m a teapot”). You can find a full list on MDN. The client can check the status code to determine if their request was successful.\nSubsequent lines give headers for the response. The server has specified the date the response was generated, the type of content, and its encoding; other headers I have omitted set various cookies and give other caching information.\nThe headers are followed by the response body. In this case, since the Content-Encoding is gzip, the response body is the gzip-compressed HTML of the web page.\n\n\n\nAbove we saw a GET request, which is the simplest and most common type of HTTP request. GET is one of several methods that can be used. The common methods are:\n\nGET: Retrieve data from a specific resource (i.e. fetch a specific web page). GET requests should not have side effects, like deleting things.\nPOST: Send data to the server so it can do something. Submitting a form to a website often sends a POST request containing the contents of the form. POST requests often have side effects, and automated scrapers should be very careful about submitting POST requests.\nHEAD: Same as GET, except without the body of the response – only the response headers will be sent.\nPUT, PATCH, …: Other methods are less commonly used. PUT sends a file to a server and asks it to put it at a specific URL, for example, while PATCH asks for modifications to be made to a file.\n\nA POST request can contain arbitrary amounts of data, typically as key-value pairs.\nHere’s an example POST request adapted from MDN:\nPOST /submit HTTP/1.1\nHost: example.com\nContent-Type: application/x-www-form-urlencoded\nContent-Length: 13\n\nsay=Hi&to=Mom\nWe’re sending the keys say and to to the server at example.com.\nThere are other ways of encoding the contents of a POST request; this method is called urlencoded (because the key-value pairs resemble those in the query component of a URL), but we could send JSON or other data formats if the server knows how to read them, and we’d set the Content-Type appropriately.\n\n\n\nThere are HTTP packages for almost any programming language. In Python, your best choice is probably Requests, which you can install with\npip install requests\nThe package is fairly straightforward. For example:\nimport requests\n\nr = requests.get(\"https://en.wikipedia.org/wiki/World_Wide_Web\",\n                 headers={\"User-Agent\": \"toy demo 1.0\"})\n\nr.status_code #&gt; 200\n\nr.text #&gt; \"'&lt;!DOCTYPE html&gt;\\n&lt;html ...\"\nTo add query parameters, we can either add them to the URL manually, or provide a dictionary of parameters:\nr = requests.get(\"https://forecast.weather.gov/zipcity.php\",\n                 params={\"inputstring\": \"15213\"})\nBy providing a dictionary, we allow Requests to do any necessary URL encoding for us, so there’s no problem if one of the values contains & or = characters.\nFor more details, the requests quickstart guide and API reference are useful starting points.\n\n\n\n\nYou use HTTP every day when browsing websites. When you open https://www.instagram.com, your web browser sends a GET request and receives a response; when you post a photo, your browser sends a POST request containing the photo data and information like the photo caption and tags.\nBut let’s return to the idea of providing an interface to databases. How can we turn HTTP into a general interface for specifying requests, receiving data, and sending updates?\n\n\nThe basic design idea is called representational state transfer, or REST.\nREST defines a basic architecture:\n\nA client makes requests of a server. The server worries about how to store and manipulate the data; the client just has to specify what it wants.\nThe requests are stateless. This doesn’t mean they qualify for Nansen passports; it means that each HTTP request is independent, and doesn’t depend on the previous requests you may have made. This prevents complexity from creeping in.\nResources are identified with URLs. A “resource” might be a web page (as in normal web browsing), but it might also represent a user, an article, a post, or some other thing stored by the system. Each resource as a URL in a specific form.\nOperations are done with GET and POST requests to specific URLs. A GET request to the URL for a resource might return information about that resource, while a particular POST request might be used to add or update data.\n\nSometimes the URLs to perform actions are known as endpoints, and so we refer to the “API endpoint” to fetch certain data.\n\n\n\nWe can implement this for a particular system by making a web server that responds to HTTP requests by looking up data or modifying it accordingly. Then a client can issue the right requests to read and modify data.\nLet’s use a real API to see how this works. GitHub provides a REST API that lets you obtain information about users, repositories, issues, pull requests, and so on. Each of these is a resource. All API requests are sent to https://api.github.com, so URLs build on that address.\nFor example, each user is a resource, and there is a URL for each user. For example, we can make a GET request to /users/{username} to get information about a user with a particular username: https://api.github.com/users/capnrefsmmat\nIf we open this link in a web browser, we see that GitHub returns data in JSON format about me. The data includes my name and website, my user ID number, and various other details. GitHub also helpfully includes the address of other resources as well, like the repos_url to get a list of all my public Git repositories.\nIn Requests, we can get this JSON-encoded data into Python.\nimport requests\n\nr = requests.get(\"https://api.github.com/users/capnrefsmmat\")\n\nr.status_code #&gt; 200\n\nr.text #&gt; the text of the JSON-encoded data\n\nj = r.json() #&gt; the JSON decoded into a Python dictionary\n\nj['name'] #&gt; Alex Reinhart\n\n\n\nFor interactive use, this is straightforward enough. But if you’re writing code that wraps the GitHub API in functions, you’ll need to handle errors that might occur. For example, consider the following function:\ndef get_github_user(username):\n    r = requests.get(f\"https://api.github.com/users/{username}\")\n\n    return r.json()\nThis fetches the API data for a particular user and returns it as a Python dictionary. It works great – unless there’s a problem. Suppose we were looping through some usernames to do something to each account:\nfor username in usernames:\n    user_info = get_github_user(username)\n\n    user_login = user_info[\"login\"]\n    user_bio = user_info[\"bio\"]\n    user_avatar = user_info[\"avatar_url\"]\n\n    # do stuff, print stuff, calculate stuff...\n    # etc.\nNow suppose usernames contains an invalid username. We get the following error:\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[8], line 4\n      1 for username in usernames:\n      2     user_info = get_github_user(username)\n----&gt; 4     user_login = user_info[\"login\"]\n      5     user_bio = user_info[\"bio\"]\n      6     user_avatar = user_info[\"avatar_url\"]\n\nKeyError: 'login'\nThat’s not helpful! A KeyError means we tried to access a key in the dictionary that does not exist. Evidently the GitHub API returns a different set of data when we request an invalid username, and we’d have to check before using it.\nBut before we start writing if 'login' in user_info, we should consider what HTTP gives us for free. Each HTTP response has a status code. In this case, GitHub gives a 404 code for invalid usernames:\nr = requests.get(\"https://api.github.com/users/capnrefsm mat\")\nr.status_code\nRather than checking if r.status_code == 404, which wouldn’t cover other error codes, we can let Requests do the work. Here’s an updated version of the function:\ndef get_github_user(username):\n    r = requests.get(f\"https://api.github.com/users/{username}\")\n\n    r.raise_for_status()\n\n    return r.json()\nThe raise_for_status() method raises a Python exception if the request failed. Running our loop over usernames again, we get the following error:\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[16], line 2\n      1 for username in usernames:\n----&gt; 2     user_info = get_github_user(username)\n      4     user_login = user_info[\"login\"]\n      5     user_bio = user_info[\"bio\"]\n\nCell In[15], line 4, in get_github_user(username)\n      1 def get_github_user(username):\n      2     r = requests.get(f\"https://api.github.com/users/{username}\")\n----&gt; 4     r.raise_for_status()\n      6     return r.json()\n\nFile ~/miniconda3/lib/python3.10/site-packages/requests/models.py:960, in Response.raise_for_status(self)\n    957     http_error_msg = u'%s Server Error: %s for url: %s' % (self.status_code, reason, self.url)\n    959 if http_error_msg:\n--&gt; 960     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 404 Client Error: Not Found for url: https://api.github.com/users/capnrefsmm%20at\nBy raising an exception, we have turned this problem into an error we can handle with standard Python techniques. For example, we could adapt our loop to handle errors:\nfor username in usernames:\n    try:\n        user_info = get_github_user(username)\n    except requests.HTTPError as e:\n        print(f\"Unable to get information for user '{username}'\")\n        print(f\"{e.response.status_code} {e.response.reason}\")\n        continue\n\n    user_login = user_info[\"login\"]\n    user_bio = user_info[\"bio\"]\n    user_avatar = user_info[\"avatar_url\"]\n\n    # do stuff, print stuff, calculate stuff...\n    # etc.\nIn this version, we handle the error (by getting useful information from the exception object) and then proceed to process the rest of the list. Your error-handling strategy may vary depending on the type of problem encountered.\n\n\n\nMany APIs require authentication: you must be an authorized user to access the API or perform certain operations. Because REST requires that APIs be stateless – the server doesn’t have to remember your status between requests – authentication typically involves providing a special token or key with each request you make.\nFor example, GitHub provides personal access tokens. If you create such a token, you can use it to access its API. The token is simply a unique random string that is connected to your account. If we don’t provide a token, we only get to see what logged-out users see.\nI have a private GitHub repository, and let’s try to get information about it:\nr = requests.get(\"https://api.github.com/repos/capnrefsmmat/think-alouds\")\n\nr.status_code  #=&gt; 404\nNotice that GitHub says 404 (Not Found) and not 403 (Not Authorized), because it doesn’t want users to be able to find out the names of private repositories – it simply denies all knowledge of them if you don’t have permission to see them.\nOn the other hand, we can fetch information about the repository if we provide my token:\nr = requests.get(\"https://api.github.com/repos/capnrefsmmat/think-alouds\",\n                 headers={\"Authorization\": \"Bearer my-token-goes-here\"})\n\nr.json()\nWith the right token, this produces a bunch of data:\n{'id': 141750320,\n 'node_id': 'MDEwOlJlcG9zaXRvcnkxNDE3NTAzMjA=',\n 'name': 'think-alouds',\n 'full_name': 'capnrefsmmat/think-alouds',\n 'private': True,\n 'html_url': 'https://github.com/capnrefsmmat/think-alouds',\n 'description': 'Think-aloud questions for intro statistics',\n 'fork': False,\n 'url': 'https://api.github.com/repos/capnrefsmmat/think-alouds',\n ...\n}\nOther APIs may give the header a different name or expect a different kind of token, but this is the basic approach.\n\n\n\nAs we discussed above (Section 1.1.3), if you want to make a request that has side effects – adds new data, edits a record, deletes something, sends assassins to your enemies – you will likely use a POST request rather than a GET request.\nIn principle, a POST request consists of headers and a body, and the body can be anything that the server understands. In practice, there are a few common types and formats of request body.\nMost bodies are a bunch of key-value pairs. For example, when I edit the preferences for my Wikipedia account and hit “Save”, my browser sends the following request:\nPOST /wiki/Special:Preferences HTTP/2\nHost: en.wikipedia.org\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/111.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate, br\nReferer: https://en.wikipedia.org/wiki/Special:Preferences\nContent-Type: application/x-www-form-urlencoded\nContent-Length: 3410\nOrigin: https://en.wikipedia.org\n[...]\n\ntitle=Special%3APreferences&wplanguage=en&wpgender=unknown&wpemail-blacklist=&wpskin=vector-2022&wpskin-responsive=1&[...]\nI’ve trimmed some of the superfluous headers and most of the request body, but you can see how the form works: each form field is sent with a key (the field name) and a value (the setting I chose).\nWe could make such a request in Requests by using requests.post():\nr = requests.post(\"https://en.wikipedia.org/wiki/Special:Preferences\",\n                  data={\"title\": \"Special:Preferences\",\n                        \"wplanguage\": \"en\",\n                        \"wpgender\": \"unknown\",\n                        \"wpemail-blacklist\": None,\n                        \"wpskin\": \"vector-2022\",\n                        [...]})\nThat’s convenient, but this “urlencoded” format limits us to a simple set of key-value pairs. It’s hard to do more complicated data structures. Each value must be a string, so we can’t easily send lists except by sending the same key multiple times; and we can’t nest things unless we manually convert the nested data into strings.\nThat’s why many web applications and APIs use JSON. JSON is a standard way to write dictionaries, lists, and other simple data as text. For example, you could write\n{\n    \"andrewid\": \"areinhar\",\n    \"semester\": \"2023-spring\",\n    \"courses\": [\n        {\n            \"number\": 46927,\n            \"title\": \"Machine Learning II\",\n        },\n        {\n            \"number\": 36615,\n            \"title\": \"Software for Large-Scale Data\",\n        },\n        {\n            \"number\": 36616,\n            \"title\": \"Computational Methods for Statistics\",\n        },\n    ]\n}\nThis actually matches Python syntax for dictionaries very closely, so it should be easy to see how a particular Python dictionary will be translated into a JSON string.\nIn Requests, you can simply pass a dictionary and ask for it to be sent as JSON:\nr = requests.post(\"https://www.example.com/some-api\",\n                  json=some_big_dictionary)\nThis automatically adds the Content-Type: application/json header, to notify the web server to process the request body as JSON. In some cases, APIs will process everything as JSON automatically; in other cases, they support multiple formats and require you to specify a Content-Type to ensure the data is handled correctly.\n\n\n\n\n\nExercise 1 (Weather forecasting) The National Weather Service, a US government agency, operates a weather API that provides access to current weather conditions, forecasts, storm warnings, and related data in JSON format. All this weather data is publicly and freely available, so anyone who needs weather information can obtain it from the government at no cost. (In fact, commercial weather forecasters, like the Weather Channel or the weather apps built into phones, mainly process and repackage data from the National Weather Service.) The API is hosted at https://api.weather.gov.\nIn this exercise, your goal is to build a command-line Python script that can display the hourly forecast for a given location specified at the command line, using latitude and longitude. For example:\n$ python forecaster.py --interval=3 --periods=5 40.4397 -79.976\n\nHourly forecast for Pittsburgh, PA:\n\nThu 7 AM: Slight Chance Rain Showers, 54°F.\nThu 10 AM: Chance Rain Showers, 52°F.\nThu 1 PM: Chance Rain Showers, 53°F.\nThu 4 PM: Slight Chance Rain Showers, 53°F.\nThu 7 PM: Mostly Cloudy, 52°F.\nNotice that here we’ve asked for forecasts every 3 hours for 5 periods, but the API can give hourly forecasts several days into the future. You can add additional output formatting if you’d like, such as breaking the forecasts into blocks by date, adding umbrella and sunshine emoji, or even color-coding forecasts.\nSome requirements and tips for your script:\n\nUse the argparse module to process the command-line arguments. Your script should accept the --interval and --periods arguments, as above, as well as the latitude and longitude. If --interval is not provided, default to an hourly forecast; if --periods is not provided, default to 24 periods.\nThe script should gracefully handle errors. For example:\n\nWhat if the location requested does not have any forecasts available in the API?\nWhat if the user requests more --periods than the API provides forecast, e.g. they request forecasts 400 hours into the future?\nWhat if the API returns a server error or other bad status code?\n\nUse the requests package to make the API requests and decode the JSON into a Python dictionary.\nSet a user-agent of “forecaster (yourandrew@andrew.cmu.edu)” for all requests you make to the API.\nThe Specification tab of the API Web Service page documents all the available API URLs and what they require.\nYour script will have to use two endpoints. The /points/{point} endpoint lets you look up a point by latitude and longitude and get the URL of the hourly forecast for that point; that URL will be to the /gridpoints/{wfo}/{x},{y}/forecast/hourly endpoint, which returns the hourly forecast.\nAll times returned by the API are in ISO 8601 format. Python’s datetime module can turn these into datetime objects using the datetime.fromisoformat() method. You can then use the strftime() method to format the times for printing however you’d like.\nUse good program design principles. That means splitting the code into reusable functions, and separating the fetching of data from its formatting for output.\n\nAs a starting point, open https://api.weather.gov/points/40.4397,-79.976 in your web browser. You should get a JSON reply giving details about that point, including the name of the nearby city, API URLs for its forecasts. Examining this manually will show you which fields your code should extract and use when printing its output. Similarly, you can open the hourly forecast endpoint in your browser to figure out how it is formatted and how to process it.\nTurn in your Python script and an example hourly forecast for Pittsburgh, whose coordinates are shown above.\nNote: Please be careful and ensure your script does not send thousands of API requests in a loop. You should only need to make two requests each time the script is run. We don’t want to annoy the National Weather Service and risk a penalty hailstorm."
  },
  {
    "objectID": "large-scale-data/rest-apis.html#sec-http",
    "href": "large-scale-data/rest-apis.html#sec-http",
    "title": "REST APIs",
    "section": "",
    "text": "Let’s briefly recap HTTP, the HyperText Transport Protocol underlying REST APIs and every web page you visit. HTTP is a way to make requests to servers for webpages (or other resources) and receive their responses, and it communicates its data by using protocols like TCP.\n\n\nResources available over HTTP (and some other protocols) are identified with Uniform Resource Locators.\nURLs come in parts; here’s the syntax as given by Wikipedia:\nURI = scheme:[//authority]path[?query][#fragment]\nauthority = [userinfo@]host[:port]\nLet’s break that down:\n\nScheme: The type of thing we want. Common schemes are http and https for Web pages, mailto for email links, file for files on the local file system, and so on.\nuserinfo: A username and potentially a password (as username:password). Optional. Sometimes used for FTP links so the links specifies the username to use. Normally it’s not good to include a password in a URL, because then it’s visible to anyone.\nhost: A domain name or IP address for the server that owns this content.\nport: The port number to use when connecting.\npath: A sequence of parts separated by slashes (/ – no, these are not backslashes). The path specifies the specific resource or file being requested.\nquery: Optionally, a query, starting with a question mark. Queries are typically key-value pairs separated by ampersands, as in foo=bar&baz=bin.\nfragment: A name, preceded by the hash sign #, pointing to a specific part of the document. For example, in an HTML page, the fragment may refer to a specific section or heading on the page.\n\nSome URL examples:\nhttps://en.wikipedia.org/wiki/URL?thing=baz\n\nhttps://scholar.google.com/citations?user=AxT2apgAAAAJ&hl=en\n\nftp://alex:hunter2@ftp.example.com/home/alex/passwords\nURLs can only contain a specific set of characters (mainly the Latin alphabet, digits, and some punctuation and symbol characters). Everything else must be “URL encoded”, meaning the character is replaced by a percent sign and two hexadecimal digits giving its UTF-8 numeric encoding. (If the encoding is more than one byte, each byte is encoded separately.)\nFor example, the space character is not permitted in URLs, so it is replaced with %20. Similarly, if we want to use = or & inside a query argument without it being interpreted as separating key-value pairs, we can URL-encode them.\n\n\n\nAn HTTP request is sent by a client (i.e. you) to a server to request a specific resource. It consists of several parts. For example, here’s a request I sent to Wikipedia for the page https://en.wikipedia.org/wiki/World_Wide_Web:\nGET /wiki/World_Wide_Web HTTP/2.0\nHost: en.wikipedia.org\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:64.0) Gecko/20100101 Firefox/64.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate, br\nDNT: 1\nConnection: keep-alive\nCookie: enwikiUserName=Cap%27n+Refsmmat; [more redacted]\nUpgrade-Insecure-Requests: 1\nCache-Control: max-age=0\nIf-Modified-Since: Sun, 27 Jan 2019 03:20:05 GMT\nThere’s a lot going on here. Let’s break it down.\nGET /wiki/World_Wide_Web HTTP/2.0\nHost: en.wikipedia.org\nThere are several types of HTTP requests, but this is a GET request, using HTTP version 2.0. (See Section 1.1.3 for other request types.) The first line specifies the type of request and the specific resource I’m requesting, /wiki/World_Wide_Web. The second line is the beginning of the headers, providing metadata about the request. These headers are in Key: value format, one per line. Here are a few selected headers and their meanings:\n\nHost: Specifies domain name I am making the request for – it’s possible for multiple different domain names to have the same IP address, so I have to tell the receiving server which domain name I’m interested in.\nUser-Agent: Tells the server what kind of program I am. Servers sometimes use this to tell what version of a web page to serve – maybe a page uses some features that only work in Chrome or in specific versions of Internet Explorer. If you’re writing a program that makes HTTP requests, it’s considered good practice to set a User-Agent identifying your program, so if server operators notice it causing a problem, they know who to complain to.\nAccept: Specifies the formats of responses I am willing to accept. We’ll see later that some APIs let us specify if we want responses as CSV, JSON, or other formats.\nAccept-Encoding: Here my browser says it is willing to receive content compressed with gzip, DEFLATE, or Brotli. The server can automatically compress its responses, and the HTTP client automatically decompresses them, saving network bandwidth.\n\nAfter the server receives this request, it can send a response. Responses have a similar format to requests. They begin with a status line and headers:\nHTTP/2.0 200 OK\ndate: Wed, 30 Jan 2019 19:14:42 GMT\ncontent-type: text/html; charset=UTF-8\ncontent-encoding: gzip\n[...]\nThe first line gives the status code 200, meaning “OK”. Other response codes include the famous 404 (“not found”), 403 (“forbidden”), 301 (“moved permanently”), and 418 (“I’m a teapot”). You can find a full list on MDN. The client can check the status code to determine if their request was successful.\nSubsequent lines give headers for the response. The server has specified the date the response was generated, the type of content, and its encoding; other headers I have omitted set various cookies and give other caching information.\nThe headers are followed by the response body. In this case, since the Content-Encoding is gzip, the response body is the gzip-compressed HTML of the web page.\n\n\n\nAbove we saw a GET request, which is the simplest and most common type of HTTP request. GET is one of several methods that can be used. The common methods are:\n\nGET: Retrieve data from a specific resource (i.e. fetch a specific web page). GET requests should not have side effects, like deleting things.\nPOST: Send data to the server so it can do something. Submitting a form to a website often sends a POST request containing the contents of the form. POST requests often have side effects, and automated scrapers should be very careful about submitting POST requests.\nHEAD: Same as GET, except without the body of the response – only the response headers will be sent.\nPUT, PATCH, …: Other methods are less commonly used. PUT sends a file to a server and asks it to put it at a specific URL, for example, while PATCH asks for modifications to be made to a file.\n\nA POST request can contain arbitrary amounts of data, typically as key-value pairs.\nHere’s an example POST request adapted from MDN:\nPOST /submit HTTP/1.1\nHost: example.com\nContent-Type: application/x-www-form-urlencoded\nContent-Length: 13\n\nsay=Hi&to=Mom\nWe’re sending the keys say and to to the server at example.com.\nThere are other ways of encoding the contents of a POST request; this method is called urlencoded (because the key-value pairs resemble those in the query component of a URL), but we could send JSON or other data formats if the server knows how to read them, and we’d set the Content-Type appropriately.\n\n\n\nThere are HTTP packages for almost any programming language. In Python, your best choice is probably Requests, which you can install with\npip install requests\nThe package is fairly straightforward. For example:\nimport requests\n\nr = requests.get(\"https://en.wikipedia.org/wiki/World_Wide_Web\",\n                 headers={\"User-Agent\": \"toy demo 1.0\"})\n\nr.status_code #&gt; 200\n\nr.text #&gt; \"'&lt;!DOCTYPE html&gt;\\n&lt;html ...\"\nTo add query parameters, we can either add them to the URL manually, or provide a dictionary of parameters:\nr = requests.get(\"https://forecast.weather.gov/zipcity.php\",\n                 params={\"inputstring\": \"15213\"})\nBy providing a dictionary, we allow Requests to do any necessary URL encoding for us, so there’s no problem if one of the values contains & or = characters.\nFor more details, the requests quickstart guide and API reference are useful starting points."
  },
  {
    "objectID": "large-scale-data/rest-apis.html#rest",
    "href": "large-scale-data/rest-apis.html#rest",
    "title": "REST APIs",
    "section": "",
    "text": "You use HTTP every day when browsing websites. When you open https://www.instagram.com, your web browser sends a GET request and receives a response; when you post a photo, your browser sends a POST request containing the photo data and information like the photo caption and tags.\nBut let’s return to the idea of providing an interface to databases. How can we turn HTTP into a general interface for specifying requests, receiving data, and sending updates?\n\n\nThe basic design idea is called representational state transfer, or REST.\nREST defines a basic architecture:\n\nA client makes requests of a server. The server worries about how to store and manipulate the data; the client just has to specify what it wants.\nThe requests are stateless. This doesn’t mean they qualify for Nansen passports; it means that each HTTP request is independent, and doesn’t depend on the previous requests you may have made. This prevents complexity from creeping in.\nResources are identified with URLs. A “resource” might be a web page (as in normal web browsing), but it might also represent a user, an article, a post, or some other thing stored by the system. Each resource as a URL in a specific form.\nOperations are done with GET and POST requests to specific URLs. A GET request to the URL for a resource might return information about that resource, while a particular POST request might be used to add or update data.\n\nSometimes the URLs to perform actions are known as endpoints, and so we refer to the “API endpoint” to fetch certain data.\n\n\n\nWe can implement this for a particular system by making a web server that responds to HTTP requests by looking up data or modifying it accordingly. Then a client can issue the right requests to read and modify data.\nLet’s use a real API to see how this works. GitHub provides a REST API that lets you obtain information about users, repositories, issues, pull requests, and so on. Each of these is a resource. All API requests are sent to https://api.github.com, so URLs build on that address.\nFor example, each user is a resource, and there is a URL for each user. For example, we can make a GET request to /users/{username} to get information about a user with a particular username: https://api.github.com/users/capnrefsmmat\nIf we open this link in a web browser, we see that GitHub returns data in JSON format about me. The data includes my name and website, my user ID number, and various other details. GitHub also helpfully includes the address of other resources as well, like the repos_url to get a list of all my public Git repositories.\nIn Requests, we can get this JSON-encoded data into Python.\nimport requests\n\nr = requests.get(\"https://api.github.com/users/capnrefsmmat\")\n\nr.status_code #&gt; 200\n\nr.text #&gt; the text of the JSON-encoded data\n\nj = r.json() #&gt; the JSON decoded into a Python dictionary\n\nj['name'] #&gt; Alex Reinhart\n\n\n\nFor interactive use, this is straightforward enough. But if you’re writing code that wraps the GitHub API in functions, you’ll need to handle errors that might occur. For example, consider the following function:\ndef get_github_user(username):\n    r = requests.get(f\"https://api.github.com/users/{username}\")\n\n    return r.json()\nThis fetches the API data for a particular user and returns it as a Python dictionary. It works great – unless there’s a problem. Suppose we were looping through some usernames to do something to each account:\nfor username in usernames:\n    user_info = get_github_user(username)\n\n    user_login = user_info[\"login\"]\n    user_bio = user_info[\"bio\"]\n    user_avatar = user_info[\"avatar_url\"]\n\n    # do stuff, print stuff, calculate stuff...\n    # etc.\nNow suppose usernames contains an invalid username. We get the following error:\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[8], line 4\n      1 for username in usernames:\n      2     user_info = get_github_user(username)\n----&gt; 4     user_login = user_info[\"login\"]\n      5     user_bio = user_info[\"bio\"]\n      6     user_avatar = user_info[\"avatar_url\"]\n\nKeyError: 'login'\nThat’s not helpful! A KeyError means we tried to access a key in the dictionary that does not exist. Evidently the GitHub API returns a different set of data when we request an invalid username, and we’d have to check before using it.\nBut before we start writing if 'login' in user_info, we should consider what HTTP gives us for free. Each HTTP response has a status code. In this case, GitHub gives a 404 code for invalid usernames:\nr = requests.get(\"https://api.github.com/users/capnrefsm mat\")\nr.status_code\nRather than checking if r.status_code == 404, which wouldn’t cover other error codes, we can let Requests do the work. Here’s an updated version of the function:\ndef get_github_user(username):\n    r = requests.get(f\"https://api.github.com/users/{username}\")\n\n    r.raise_for_status()\n\n    return r.json()\nThe raise_for_status() method raises a Python exception if the request failed. Running our loop over usernames again, we get the following error:\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[16], line 2\n      1 for username in usernames:\n----&gt; 2     user_info = get_github_user(username)\n      4     user_login = user_info[\"login\"]\n      5     user_bio = user_info[\"bio\"]\n\nCell In[15], line 4, in get_github_user(username)\n      1 def get_github_user(username):\n      2     r = requests.get(f\"https://api.github.com/users/{username}\")\n----&gt; 4     r.raise_for_status()\n      6     return r.json()\n\nFile ~/miniconda3/lib/python3.10/site-packages/requests/models.py:960, in Response.raise_for_status(self)\n    957     http_error_msg = u'%s Server Error: %s for url: %s' % (self.status_code, reason, self.url)\n    959 if http_error_msg:\n--&gt; 960     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 404 Client Error: Not Found for url: https://api.github.com/users/capnrefsmm%20at\nBy raising an exception, we have turned this problem into an error we can handle with standard Python techniques. For example, we could adapt our loop to handle errors:\nfor username in usernames:\n    try:\n        user_info = get_github_user(username)\n    except requests.HTTPError as e:\n        print(f\"Unable to get information for user '{username}'\")\n        print(f\"{e.response.status_code} {e.response.reason}\")\n        continue\n\n    user_login = user_info[\"login\"]\n    user_bio = user_info[\"bio\"]\n    user_avatar = user_info[\"avatar_url\"]\n\n    # do stuff, print stuff, calculate stuff...\n    # etc.\nIn this version, we handle the error (by getting useful information from the exception object) and then proceed to process the rest of the list. Your error-handling strategy may vary depending on the type of problem encountered.\n\n\n\nMany APIs require authentication: you must be an authorized user to access the API or perform certain operations. Because REST requires that APIs be stateless – the server doesn’t have to remember your status between requests – authentication typically involves providing a special token or key with each request you make.\nFor example, GitHub provides personal access tokens. If you create such a token, you can use it to access its API. The token is simply a unique random string that is connected to your account. If we don’t provide a token, we only get to see what logged-out users see.\nI have a private GitHub repository, and let’s try to get information about it:\nr = requests.get(\"https://api.github.com/repos/capnrefsmmat/think-alouds\")\n\nr.status_code  #=&gt; 404\nNotice that GitHub says 404 (Not Found) and not 403 (Not Authorized), because it doesn’t want users to be able to find out the names of private repositories – it simply denies all knowledge of them if you don’t have permission to see them.\nOn the other hand, we can fetch information about the repository if we provide my token:\nr = requests.get(\"https://api.github.com/repos/capnrefsmmat/think-alouds\",\n                 headers={\"Authorization\": \"Bearer my-token-goes-here\"})\n\nr.json()\nWith the right token, this produces a bunch of data:\n{'id': 141750320,\n 'node_id': 'MDEwOlJlcG9zaXRvcnkxNDE3NTAzMjA=',\n 'name': 'think-alouds',\n 'full_name': 'capnrefsmmat/think-alouds',\n 'private': True,\n 'html_url': 'https://github.com/capnrefsmmat/think-alouds',\n 'description': 'Think-aloud questions for intro statistics',\n 'fork': False,\n 'url': 'https://api.github.com/repos/capnrefsmmat/think-alouds',\n ...\n}\nOther APIs may give the header a different name or expect a different kind of token, but this is the basic approach.\n\n\n\nAs we discussed above (Section 1.1.3), if you want to make a request that has side effects – adds new data, edits a record, deletes something, sends assassins to your enemies – you will likely use a POST request rather than a GET request.\nIn principle, a POST request consists of headers and a body, and the body can be anything that the server understands. In practice, there are a few common types and formats of request body.\nMost bodies are a bunch of key-value pairs. For example, when I edit the preferences for my Wikipedia account and hit “Save”, my browser sends the following request:\nPOST /wiki/Special:Preferences HTTP/2\nHost: en.wikipedia.org\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/111.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate, br\nReferer: https://en.wikipedia.org/wiki/Special:Preferences\nContent-Type: application/x-www-form-urlencoded\nContent-Length: 3410\nOrigin: https://en.wikipedia.org\n[...]\n\ntitle=Special%3APreferences&wplanguage=en&wpgender=unknown&wpemail-blacklist=&wpskin=vector-2022&wpskin-responsive=1&[...]\nI’ve trimmed some of the superfluous headers and most of the request body, but you can see how the form works: each form field is sent with a key (the field name) and a value (the setting I chose).\nWe could make such a request in Requests by using requests.post():\nr = requests.post(\"https://en.wikipedia.org/wiki/Special:Preferences\",\n                  data={\"title\": \"Special:Preferences\",\n                        \"wplanguage\": \"en\",\n                        \"wpgender\": \"unknown\",\n                        \"wpemail-blacklist\": None,\n                        \"wpskin\": \"vector-2022\",\n                        [...]})\nThat’s convenient, but this “urlencoded” format limits us to a simple set of key-value pairs. It’s hard to do more complicated data structures. Each value must be a string, so we can’t easily send lists except by sending the same key multiple times; and we can’t nest things unless we manually convert the nested data into strings.\nThat’s why many web applications and APIs use JSON. JSON is a standard way to write dictionaries, lists, and other simple data as text. For example, you could write\n{\n    \"andrewid\": \"areinhar\",\n    \"semester\": \"2023-spring\",\n    \"courses\": [\n        {\n            \"number\": 46927,\n            \"title\": \"Machine Learning II\",\n        },\n        {\n            \"number\": 36615,\n            \"title\": \"Software for Large-Scale Data\",\n        },\n        {\n            \"number\": 36616,\n            \"title\": \"Computational Methods for Statistics\",\n        },\n    ]\n}\nThis actually matches Python syntax for dictionaries very closely, so it should be easy to see how a particular Python dictionary will be translated into a JSON string.\nIn Requests, you can simply pass a dictionary and ask for it to be sent as JSON:\nr = requests.post(\"https://www.example.com/some-api\",\n                  json=some_big_dictionary)\nThis automatically adds the Content-Type: application/json header, to notify the web server to process the request body as JSON. In some cases, APIs will process everything as JSON automatically; in other cases, they support multiple formats and require you to specify a Content-Type to ensure the data is handled correctly."
  },
  {
    "objectID": "large-scale-data/rest-apis.html#exercises",
    "href": "large-scale-data/rest-apis.html#exercises",
    "title": "REST APIs",
    "section": "",
    "text": "Exercise 1 (Weather forecasting) The National Weather Service, a US government agency, operates a weather API that provides access to current weather conditions, forecasts, storm warnings, and related data in JSON format. All this weather data is publicly and freely available, so anyone who needs weather information can obtain it from the government at no cost. (In fact, commercial weather forecasters, like the Weather Channel or the weather apps built into phones, mainly process and repackage data from the National Weather Service.) The API is hosted at https://api.weather.gov.\nIn this exercise, your goal is to build a command-line Python script that can display the hourly forecast for a given location specified at the command line, using latitude and longitude. For example:\n$ python forecaster.py --interval=3 --periods=5 40.4397 -79.976\n\nHourly forecast for Pittsburgh, PA:\n\nThu 7 AM: Slight Chance Rain Showers, 54°F.\nThu 10 AM: Chance Rain Showers, 52°F.\nThu 1 PM: Chance Rain Showers, 53°F.\nThu 4 PM: Slight Chance Rain Showers, 53°F.\nThu 7 PM: Mostly Cloudy, 52°F.\nNotice that here we’ve asked for forecasts every 3 hours for 5 periods, but the API can give hourly forecasts several days into the future. You can add additional output formatting if you’d like, such as breaking the forecasts into blocks by date, adding umbrella and sunshine emoji, or even color-coding forecasts.\nSome requirements and tips for your script:\n\nUse the argparse module to process the command-line arguments. Your script should accept the --interval and --periods arguments, as above, as well as the latitude and longitude. If --interval is not provided, default to an hourly forecast; if --periods is not provided, default to 24 periods.\nThe script should gracefully handle errors. For example:\n\nWhat if the location requested does not have any forecasts available in the API?\nWhat if the user requests more --periods than the API provides forecast, e.g. they request forecasts 400 hours into the future?\nWhat if the API returns a server error or other bad status code?\n\nUse the requests package to make the API requests and decode the JSON into a Python dictionary.\nSet a user-agent of “forecaster (yourandrew@andrew.cmu.edu)” for all requests you make to the API.\nThe Specification tab of the API Web Service page documents all the available API URLs and what they require.\nYour script will have to use two endpoints. The /points/{point} endpoint lets you look up a point by latitude and longitude and get the URL of the hourly forecast for that point; that URL will be to the /gridpoints/{wfo}/{x},{y}/forecast/hourly endpoint, which returns the hourly forecast.\nAll times returned by the API are in ISO 8601 format. Python’s datetime module can turn these into datetime objects using the datetime.fromisoformat() method. You can then use the strftime() method to format the times for printing however you’d like.\nUse good program design principles. That means splitting the code into reusable functions, and separating the fetching of data from its formatting for output.\n\nAs a starting point, open https://api.weather.gov/points/40.4397,-79.976 in your web browser. You should get a JSON reply giving details about that point, including the name of the nearby city, API URLs for its forecasts. Examining this manually will show you which fields your code should extract and use when printing its output. Similarly, you can open the hourly forecast endpoint in your browser to figure out how it is formatted and how to process it.\nTurn in your Python script and an example hourly forecast for Pittsburgh, whose coordinates are shown above.\nNote: Please be careful and ensure your script does not send thousands of API requests in a loop. You should only need to make two requests each time the script is run. We don’t want to annoy the National Weather Service and risk a penalty hailstorm."
  }
]