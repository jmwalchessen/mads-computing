<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.34">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>spark-data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-c8ad9e5dbd60b7b70b38521ab19b7da4.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-57255bc51a02dad1602480f616b5f366.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="fullcontent quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="spark-data-management" class="level1">
<h1>Spark Data Management</h1>
<p>So far, I have suggested that distributed data systems like Spark (or MapReduce/Hadoop) are best suited for problems where your data is so enormous that it cannot conveniently fit on one machine. This is for a few reasons:</p>
<ol type="1">
<li>Any system that runs on multiple machines will be more complicated, and harder to use, than a system that runs on just one machine.</li>
<li>Coordinating and transmitting data between machines is almost always slower than using the data on one machine.</li>
<li>Single-machine database systems have been in use for decades, so their ecosystems are <em>very</em> mature. You can find detailed books, packages for every conceivable programming languages, thousands of online tutorials, example projects, and so on. Distributed systems are much newer and change rapidly, so it can be hard to find up-to-date information.</li>
</ol>
<p>But sometimes you do need distributed data. Given your statistical skills, the distribution of this data is likely to be handled by someone else: other teams at your company will be running the systems that produce and store the data, and your job will be to analyze it to answer business questions. So let’s talk about how data would be stored and how you get it into Spark.</p>
<section id="distributed-file-systems" class="level2">
<h2 class="anchored" data-anchor-id="distributed-file-systems">Distributed file systems</h2>
<p>Back in <strong>?@sec-distributing-data</strong>, we discussed distributed file systems. These run on multiple servers and automatically store files redundantly. To store a petabyte of data, we might have 100 machines, each with 40 TB of hard drives installed—allowing the distributed file system enough space to store multiple copies of each file, on multiple machines, so that the failure of a few machines does not mean the loss of any data.</p>
<p>Apache Spark supports several distributed file systems, including HDFS, natively. It can load data directly from HDFS, and also from S3 and several other popular storage systems.</p>
</section>
<section id="loading-data-from-files" class="level2">
<h2 class="anchored" data-anchor-id="loading-data-from-files">Loading data from files</h2>
<p>Spark supports a whole bunch of data formats—a confusingly large number, actually. You can read text files, CSVs, JSON, Parquet, ORC, Avro, HBase, and various other things with connectors and libraries.</p>
<p>The <code>SparkSession</code> object you get when initializing PySpark has a <code>read</code> attribute that returns a <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html#pyspark.sql.DataFrameReader"><code>DataFrameReader</code> object</a> with methods for loading numerous different data formats. Let’s look at two common formats, CSV and Parquet.</p>
<section id="sec-spark-read-csv" class="level3">
<h3 class="anchored" data-anchor-id="sec-spark-read-csv">CSVs</h3>
<p>CSV files can be read with the <code>csv()</code> method on the <code>DataFrameReader</code>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>spark.read.csv(<span class="st">"some-csv-file.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This returns a <code>DataFrame</code>. However, you may need some additional steps to get the <code>DataFrame</code> you want. By default, Spark does not read the first line to get column headers, so the columns will have names like <code>_c0</code>, <code>_c1</code>, <code>_c2</code>, etc. (much like R uses <code>V1</code>, <code>V2</code>, and so on). Use the <code>header=True</code> option to have it read column headers.</p>
<p>Second, CSV files do not specify the types of their columns. The type must be inferred: if you read a column and it only contains numbers, you can guess it’s a numeric column. But that requires reading the entire file to check its contents, a costly operation for a large dataset. The <code>inferSchema</code> option can be used to turn this on.</p>
<p>Alternately, the <code>schema</code> argument can be used to specify the column types, using the same syntax you’d use inside a SQL <code>CREATE TABLE</code> command:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>spark.read.csv(<span class="st">"some-csv-file.csv"</span>,</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>               schema<span class="op">=</span><span class="st">"id INTEGER, name STRING, post_date DATE"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>A <a href="https://spark.apache.org/docs/latest/sql-ref-datatypes.html">table of supported types</a> (partway down the page) is given in the Spark documentation.</p>
<p>Finally, you may need to tell Spark how null (missing) values are represented. For example, if they’re stored as <code>NA</code> in the CSV file, use the <code>nullValue="NA"</code> argument.</p>
<p>Besides CSVs, many other formats are supported. So what should you choose? When you’re scraping your data and preparing it for Spark, what file format is best?</p>
<p>For perfectly ordinary data files, CSVs are fine. But as the data file gets large—tens or hundreds of megabytes—reading the CSV can become slow. To access a single column or a small subset of data you usually have to parse the entire file. On huge datasets that defeats the purpose of having a distributed system.</p>
</section>
<section id="parquet" class="level3">
<h3 class="anchored" data-anchor-id="parquet">Parquet</h3>
<p><a href="https://parquet.apache.org/">Parquet</a> is designed as a <em>columnar</em> data store. Instead of storing one row at a time, it stores one <em>column</em> at a time, with metadata indicating where to find each column in the file. It’s very efficient to load a subset of columns. Parquet files can also be <em>partitioned</em>, meaning the data is split into multiple files according to the value of a certain column (e.g., the year of the entry). This is nice for huge datasets, and on a distributed file system lets different chunks exist on different servers.</p>
<p>Reading a Parquet file is similar to reading a CSV:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>spark.read.parquet(<span class="st">"some-file.parquet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This produces a <code>DataFrame</code>. Parquet files support all the data types of Spark, so things like timestamps, booleans, and arrays are automatically read and converted, unlike a CSV where you might have to manually parse a date or specify a schema.</p>
<p>Arrow is the name of the <em>in-memory</em> format for Parquet, i.e.&nbsp;how the data is laid out in memory once it is loaded. This is standardized so you can load Parquet data into Arrow and pass it between libraries in programming languages without any conversion.</p>
<p>There is a <a href="https://arrow.apache.org/docs/python/parquet.html">Python library</a> for Parquet and Arrow, and an <a href="https://arrow.apache.org/docs/r/index.html">R package</a> that can read and write Parquet.</p>
</section>
</section>
<section id="sec-partitioning-bucketing" class="level2">
<h2 class="anchored" data-anchor-id="sec-partitioning-bucketing">Partitioning and bucketing</h2>
<p>The whole point of a distributed file system, and of distributed data analysis, is that data might be too big to fit on a machine. If we have an enormous dataset, it can be broken into pieces for storage. That also provides an advantage in analysis with Spark: each Spark executor can read in pieces of the file instead of the whole thing.</p>
<p>These pieces are called <em>partitions</em>. When reading a large data file from a block file system like HDFS, Spark will create one partition per block, meaning each executor will hold only part of the <code>DataFrame</code>.</p>
<p>Depending on the operations you do to the data, Spark might have to move the partitioned data between executors: if you sort the <code>DataFrame</code>, for instance, it will need to collect it all into one place in sorted order. This will cost time as the data is sent between machines. But it does suggest we could choose how to partition the data for maximum efficiency. Spark allows for data files to themselves be partitioned or bucketed, meaning the data is split into multiple files when stored.</p>
<section id="manual-partitioning" class="level3">
<h3 class="anchored" data-anchor-id="manual-partitioning">Manual partitioning</h3>
<p>Suppose you are working with account data from a bank. Each transaction in a bank account has a date, and most of your analyses use data within a certain date range: you produce monthly reports, quarterly reports, and annual reports, each doing complicated analysis in that date range, but rarely need to work with the entire dataset from all dates.</p>
<p>If your data has a column for month or year, you could choose to partition the data manually by that column. That means storing the data in a special folder where every month or year has its own data file whose name includes the date.</p>
<p>If we are writing the data from Spark, we can use the <code>partitionBy()</code> method on <code>DataFrameWriter</code> to do this:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df.write.partitionBy(<span class="st">"year"</span>).parquet(<span class="st">"foo.parquet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Instead of a single file <code>foo.parquet</code>, we’ll instead get a folder named <code>foo.parquet/</code>, containing folders with names like <code>year=2022/</code>, themselves containing Parquet data files.</p>
<p>We can read the file normally:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df_new <span class="op">=</span> spark.read.parquet(<span class="st">"foo.parquet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Spark automatically recognizes this is a partitioned file that has been split by year. If we write a Spark operation like</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>df_new.<span class="bu">filter</span>(col(<span class="st">"year"</span>) <span class="op">&gt;=</span> <span class="dv">2020</span>) <span class="op">\</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    .groupBy(<span class="st">"accountno"</span>) <span class="op">\</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    .agg(...) <span class="op">\</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    ... <span class="co"># more Spark stuff here</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>then when we execute an action like <code>.collect()</code> or <code>.show()</code>, Spark will know that its executors need only read the files for 2020 and later. The rest of the data will never be read into memory. And if we write an operation like</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>df_new.<span class="bu">filter</span>(col(<span class="st">"year"</span>) <span class="op">&gt;=</span> <span class="dv">2020</span>) <span class="op">\</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    .groupBy(<span class="st">"year"</span>) <span class="op">\</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    .agg(...) <span class="op">\</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    ... <span class="co"># more Spark stuff here</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>then each file can be read on one executor, and the grouping and aggregating can be performed on the same executor—requiring no shuffling of data between machines.</p>
<p>Explicitly partitioning by a specific column works best for columns that have dozens of unique values, not millions; you want partitions that are large enough that each can be comfortably managed by an executor, but are not so small that Spark must read millions of tiny files. (Many distributed file systems are not good at managing many small files, and are better at working with fewer larger files.)</p>
</section>
<section id="bucketing" class="level3">
<h3 class="anchored" data-anchor-id="bucketing">Bucketing</h3>
<p>In <em>bucketing</em>, we can organize the data in a more flexible way than partitioning. We can specify multiple columns to use in bucketing, and the result is something like a hash table: the columns are combined together and hashed into a single value, and that value determines which bucket the data is stored in. Multiple unique values can be stored in the same bucket, unlike in partitioning, where every partition is a separate file.</p>
<p>For example,</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>df.write.bucketBy(<span class="dv">10</span>, <span class="st">"year"</span>, <span class="st">"month"</span>, <span class="st">"account"</span>).parquet(<span class="st">"foo.parquet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>tells Spark to use the <code>year</code>, <code>month</code>, and <code>account</code> columns to split the data into 10 buckets. This ensures that all records for a particular account in a particular month will be placed into the same bucket, and hence the same file. Again, <code>foo.parquet</code> will be a directory of multiple files, and Spark will automatically understand the bucketing when we try to read the data.</p>
<p>If we load this data into a Spark <code>DataFrame</code>, it will only read the buckets necessary to fulfill a query. This is useful for filtering—if we want a particular account’s records in a particular month—but also is useful for joins. Suppose we want to join the <em>entire</em> table with another data frame, and the join is on the columns used in bucketing. Each executor can read in one bucket and join it with the matching rows in the other table, and the results can be concatenated together at the end. If the other table is also bucketed, each executor may only need to read certain buckets from each table, and do the joining within those buckets, rather than needing to read the entire tables into memory.</p>
</section>
</section>
<section id="sec-spark-catalog" class="level2">
<h2 class="anchored" data-anchor-id="sec-spark-catalog">Working with data catalogs</h2>
<p>So far we have seen two ways of accessing data from Spark. The boring way is to read a data file directly from the file system. Spark can read from the local filesystem (if the data is available on all executors), but it can also read from distributed filesystems like HDFS, or from cloud data stores like Amazon S3 and Azure Blob Storage.</p>
<p>The more interesting way is through a <em>metastore</em>. A metastore is essentially a registry of available data files, their types, their locations, and additional metadata—such as who is allowed to access and modify them. This becomes useful in a large organization, where you need <em>data governance</em>: control over who is officially in charge of certain data sets and who is allowed to access them, records showing when they were used, a search system to find datasets from across the organization, and so on.</p>
<p>For this reason, metastores seem pointless when you are just noodling around with data on your own. And they are useless in that context. But think of them as a technical solution to the organizational problems that arise when your company is big and complex enough to require a data processing system as big and complex as Spark.</p>
<p>Traditionally, Spark uses the <a href="https://hive.apache.org/">Apache Hive</a> metastore. This is still very common.</p>
<p>Databricks provides its own proprietary system, Unity Catalog, that includes more advanced features. (It is unfortunately not available on our class Azure Databricks setup.) But the basic principles are the same.</p>
<section id="managed-tables" class="level3">
<h3 class="anchored" data-anchor-id="managed-tables">Managed tables</h3>
<p>We have already used the Hive metastore when we write Spark commands like this:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>events <span class="op">=</span> spark.read.table(<span class="st">"hive_metastore.default.events"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The string <code>hive_metastore.default.events</code> is the name of a <em>managed table</em>. Here “managed” means that Hive is responsible for its storage, and all access goes through Hive (via Spark, in our case). The name has three parts:</p>
<ul>
<li>The catalog: <code>hive_metastore</code>. A catalog contains schemas. Different users can have access permissions to different catalogs.</li>
<li>The schema: <code>default</code>. A schema contains tables. Different users can have access permissions to different schemas.</li>
<li>The table: <code>events</code>. A table refers to an actual data table with rows and columns. Different users can have access permissions to different tables.</li>
</ul>
<p>This creates a hierarchy. If I want to access <code>hive_metastore.default.events</code>, I need access to the <code>hive_metastore</code> catalog, its <code>default</code> schema, and the <code>events</code> table contained therein.</p>
<p>You can’t put tables directly inside catalogs, or put schemas inside tables, or anything like that: you are restricted to a three-level hierarchy.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>With managed tables, the data is stored wherever Hive is configured to store it. In systems like Azure Databricks or Amazon’s EMR, it’s usually configured to store its data in the cloud’s object storage system.</p>
</section>
<section id="creating-a-managed-table" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-managed-table">Creating a managed table</h3>
<p>Data frames can be written to managed tables using the <code>write</code> attribute. This returns a <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html"><code>DataFrameWriter</code></a>, which has further options to control partitioning and bucketing. For example:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>df.write.partitionBy(<span class="st">"some_column"</span>) <span class="op">\</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    .saveAsTable(<span class="st">"hive_metastore.default.example_table"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Similarly, bucketing can be done with the <code>bucketBy()</code> method.</p>
</section>
</section>
<section id="using-data-from-sql-databases" class="level2">
<h2 class="anchored" data-anchor-id="using-data-from-sql-databases">Using data from SQL databases</h2>
<p>One strength of Spark is the ability to load data from many different sources and work with them in identical ways within Spark. One alternative data source is a relational database that speaks SQL, like PostgreSQL.</p>
<p>Of course, if all your data fits in a SQL database on one large server, it probably makes sense to do your analysis on that server in SQL. But sometimes you must use data from a SQL database as part of a larger analysis. For example, your company might have log data stored on HDFS in Parquet files, but your analysis needs to match up those logs to user accounts stored in PostgreSQL.</p>
<p>Spark supports accessing any SQL database that supports JDBC (Java Database Connectivity), a standard Java interface for connecting to databases. Because both SQL and Java are extremely popular, almost every relational database has a JDBC package available. Using it is pretty easy:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> spark.read.<span class="bu">format</span>(<span class="st">"jdbc"</span>) <span class="op">\</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    .option(<span class="st">"url"</span>, <span class="st">"jdbc:postgresql://sculptor.stat.cmu.edu/databasename/"</span>) <span class="op">\</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    .option(<span class="st">"user"</span>, YOURUSERNAME) <span class="op">\</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    .option(<span class="st">"password"</span>, YOURPASSWORD) <span class="op">\</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    .option(<span class="st">"dbtable"</span>, <span class="st">"name_of_table"</span>) <span class="op">\</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    .load()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As usual, Spark will try to be smart about how it reads the database table, and it will not simply do <code>SELECT * FROM name_of_table</code> to read all the data from memory. For example, if you do a <code>filter()</code> in Spark, it will try to turn this into a <code>WHERE</code> clause for the database. It will also make connections from each executor that try to read different parts of the data in parallel. You can thus benefit from the SQL server’s ability to use indexes to make queries faster, while still benefiting from Spark’s distributed data analysis. You might join the table with other large datasets you’ve loaded into Spark.</p>
</section>
<section id="exercises" class="level2">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<div id="exr-lendingclub-partition" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1 (Partioned LendingClub data)</strong></span> Return to the LendingClub data you loaded in <strong>?@exr-spark-lendingclub</strong>.</p>
<p>Spark does not provide a direct way to tell what variable a <code>DataFrame</code> was partitioned by, if any. But <code>DataFrame</code> objects do have an <code>inputFiles()</code> method that lists the one or more data files used as their input.</p>
<p>Get the <code>inputFiles()</code> list for the LendingClub data file. Examine the file names. What variable is the Parquet file partitioned by?</p>
</div>
<div id="exr-fisc-partition" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2 (Fiscally standardized cities)</strong></span> A dataset of information on “fiscally standardized” cities is available at <code>abfss://sampledata@lsd2025storage.dfs.core.windows.net/fisc_full_dataset_2020_update.csv</code>. You can read the <a href="https://cmustatistics.github.io/data-repository/politics/standard-cities.html">full dataset description</a> for details on what it contains; the short version is that it gives revenue, expenses, and tax information on over 200 American cities for over 40 years, adjusting for differences in government structure across states.</p>
<ol type="1">
<li>Load the data using <code>spark.read.csv()</code>. Check the schema with the <code>printSchema()</code> method. Make any necessary tweaks to have the column names and schema read correctly (see <a href="#sec-spark-read-csv" class="quarto-xref">Section&nbsp;1.2.1</a>).</li>
<li>Add a <code>net_income</code> column to the data, calculated as the difference between its total revenue (<code>rev_general</code>) and total spending (<code>spending_total</code>).</li>
<li>Once you have the data correctly formatted, save it as a table in the Hive metastore. Name it something like <code>fisc_andrewid</code>, with your Andrew ID, so it does not collide with any other student’s table. Partition it by the variable that seems most useful to partition by in the remaining steps of this exercise.</li>
<li>Complete the following steps in Spark SQL using the table you created. First, produce one row per city, giving the year it had the <em>worst</em> net income – the most negative net income – and its revenue, spending, and net income for that year. Order by city and state.</li>
<li>Now produce one row per city, listing its name, average net income from 1977 to 2020, and standard deviation of net income over that time period.</li>
</ol>
</div>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I don’t know why Hive (and Unity Catalog) are restricted to three-level hierarchies, instead of allowing a general hierarchy of any depth. It’s like if every file on your computer had to be stored exactly two folders deep.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>