<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.34">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>spark-ml</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-c8ad9e5dbd60b7b70b38521ab19b7da4.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-57255bc51a02dad1602480f616b5f366.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="sec-spark-ml" class="level1">
<h1>Machine Learning in Spark</h1>
<p>Besides managing large datasets and applying SQL-style operations to them, we data scientists often want to use large datasets to fit statistical models. Sometimes you will use Spark to extract features from a huge dataset, then fit models to the features on your own computer—the hard part is extracting the relevant features from the enormous dataset, not fitting the model. But sometimes the model is to be fit to too much data, and you’d like to fit it within Spark as well. That’s where MLlib comes in.</p>
<p>MLlib is a collection of regression, classification, clustering, and related methods implemented within Spark and accessible within Spark applications. Within one Spark application, you can write all the necessary steps:</p>
<ol type="1">
<li>Load the data</li>
<li>Extract features from the data</li>
<li>Transform the features, scale them, recode them, etc.</li>
<li>Split the data into training, test, and evaluation sets, as needed</li>
<li>Specify models to fit</li>
<li>Fit the models</li>
<li>Evaluate the model performance</li>
</ol>
<p>To automate much of this, we can create a Spark <em>pipeline</em>, which represents the sequence of operations. The pipeline system makes it easy to fit multiple models, with different tuning parameters and settings, to the same data so you can compare their results.</p>
<section id="basic-setup" class="level2">
<h2 class="anchored" data-anchor-id="basic-setup">Basic setup</h2>
<p>There are two versions of MLlib:</p>
<ul>
<li>the old version using Resilient Distributed Datasets (RDDs), which Spark used before it added the <code>DataFrame</code> system</li>
<li>the new version using <code>DataFrame</code>s</li>
</ul>
<p>We will discuss the new version, which you can get by importing from <code>spark.ml</code>. The old version is under <code>spark.mllib</code>. The new version supports nearly all the features of the old one, and will get all the new features and updates; but you will still find examples and code using the old one, which can be confusing.</p>
<p>Let’s do a simple example to illustrate the new system, following <span class="citation" data-cites="SparkGuide">@SparkGuide</span>, chapter 24. (Their <a href="https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data">sample data</a> and <a href="https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/code/Advanced_Analytics_and_Machine_Learning-Chapter_24_Advanced_Analytics_and_Machine_Learning.py">code</a> is available.) We’ll start by loading the data:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark <span class="im">import</span> SparkFiles</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>data_url <span class="op">=</span> <span class="st">"https://www.refsmmat.com/courses/msp-computing/data/simple-ml.json"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>sc.addFile(data_url)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> spark.read.json(SparkFiles.get(<span class="st">"simple-ml.json"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Spark’s ML pipelines are based around two core concepts: <code>Transformer</code>s and <code>Estimator</code>s. These are Python classes with certain basic methods; all the operations for transforming data, fitting models, and evaluating error metrics are based on classes that inherit from either <code>Transformer</code> or <code>Estimator</code>. They implement certain basic operations:</p>
<ul>
<li><code>Transformer</code>:
<ul>
<li><code>.transform(dataset)</code>: Takes a dataset, transforms it in some way, and returns a new dataset.</li>
</ul></li>
<li><code>Estimator</code>:
<ul>
<li><code>.fit(dataset)</code>: Fits this estimator to a dataset and returns one or more <code>Transformer</code> objects.</li>
</ul></li>
</ul>
<p>We will see how an entire pipeline can be built from these two basic methods.</p>
</section>
<section id="transforming-data" class="level2">
<h2 class="anchored" data-anchor-id="transforming-data">Transforming data</h2>
<section id="arbitrary-transformations-on-dataframes" class="level3">
<h3 class="anchored" data-anchor-id="arbitrary-transformations-on-dataframes">Arbitrary transformations on <code>DataFrame</code>s</h3>
<p>Often we must transform our source data before using it to fit models. Perhaps we need to aggregate data to produce new features, or do transformations, or rename columns, or whatever.</p>
<p>The most direct way to do this in Spark is with a <code>SQLTransformer</code>, which lets you use arbitrary SQL to transform a data frame. A <code>Transformer</code> object can take any dataset and return a new one, so our SQL query must take a data frame and return a new one. That implies we must use a <code>SELECT</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> SQLTransformer</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>transformation <span class="op">=</span> SQLTransformer(</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    statement<span class="op">=</span><span class="st">"SELECT persona, SUM(score) FROM __THIS__ GROUP BY persona"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Notice we refer to the table as <code>__THIS__</code>, which always refers to the data frame passed to <code>transformation.transform()</code>.</p>
<p>This may seem redundant: why use a <code>SQLTransformer</code> when you can just use <code>spark.sql()</code> to do any data manipulation you need? But as we’ll see, Spark has convenient features for working for pipelines composed exclusively of <code>Transformer</code> and <code>Estimator</code> objects, so expressing this task as a <code>Transformer</code> will have benefits.</p>
</section>
<section id="producing-a-column-of-features" class="level3">
<h3 class="anchored" data-anchor-id="producing-a-column-of-features">Producing a column of features</h3>
<p>A typical Spark <code>DataFrame</code> has various columns for different features. Most of Spark’s ML methods, however, expect to have one numerical column for the response variable and one numerical column containing a <em>vector</em> of features, not multiple separate columns of features.</p>
<p>Also, unlike in R, Spark’s ML systems won’t do automatic coding of design matrices for you. That is, if you have several categorical (factor) variables, R’s <code>lm()</code> would automatically produce the necessary dummy variables (contrasts) based on your model formula. If your formula is <code>y ~ x + some_factor</code>, R will create a design matrix <span class="math inline">\(X\)</span> containing columns for the intercept, <code>x</code>, and the levels of the factor. Similarly, if we specify interactions, R will add columns with the correct products.</p>
<p>In Spark, we need to do this step manually. Fortunately Spark provides <a href="https://spark.apache.org/docs/3.3.1/api/python/reference/api/pyspark.ml.feature.RFormula.html">RFormula</a>, which understands the basics of R’s formula syntax.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> RFormula</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>supervised <span class="op">=</span> RFormula(formula<span class="op">=</span><span class="st">"lab ~ . + color:value1 + color:value2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <code>supervised</code> object is an <code>Estimator</code>: it must be fit to data before it can be used, since it must examine the data to see how many factor levels there are before it can create the right columns. Once it’s fit, we get a <code>Transformer</code>, and this can transform the data to produce the new columns.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>fittedRF <span class="op">=</span> supervised.fit(df)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>preparedDF <span class="op">=</span> fittedRF.transform(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now the prepared data frame contains a <code>features</code> column with the transformed data. The <code>label</code> column contains the response (<code>lab</code>) coded as 0 or 1. We could control the names of these columns with the <code>featuresCol</code> and <code>labelCol</code> arguments to <code>RFormula()</code>, if we needed to.</p>
<p>Sometimes your data has missing values for some rows. In R, when you use <code>lm()</code> or <code>glm()</code> to fit a model to data with <code>NA</code> values for some predictors, it simply skips the affected rows. By default, however, Spark’s modeling functions throw an error when they encounter <code>null</code>, Spark’s equivalent of <code>NA</code>. With the <code>handleInvalid</code> argument to <code>RFormula</code>, we can set a different option. For example, by setting <code>handleInvalid="skip"</code>, we can tell <code>RFormula</code> that when it transforms the data, it should throw away (skip) rows with <code>null</code> values for any of the variables in the formula.</p>
</section>
<section id="making-traintest-splits" class="level3">
<h3 class="anchored" data-anchor-id="making-traintest-splits">Making train/test splits</h3>
<p>If we’re using a train/test split for model evaluation, we can do that now:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>train, test <span class="op">=</span> preparedDF.randomSplit([<span class="fl">0.7</span>, <span class="fl">0.3</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="transforming-and-rescaling-features" class="level3">
<h3 class="anchored" data-anchor-id="transforming-and-rescaling-features">Transforming and rescaling features</h3>
<p>The <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#feature"><code>pyspark.ml.feature</code> module</a> contains a number of tools for manipulating and rescaling features.</p>
<p>Some machine learning methods require us to rescale features first. (For example, when using the lasso or ridge regression, it’s good to put all features on the same scale so the penalty applies to all features equally.) Using <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.MaxAbsScaler.html#pyspark.ml.feature.MaxAbsScaler"><code>MaxAbsScaler</code></a>, <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.MinMaxScaler.html#pyspark.ml.feature.MinMaxScaler"><code>MinMaxScaler</code></a>, <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.RobustScaler.html#pyspark.ml.feature.RobustScaler"><code>RobustScaler</code></a>, or <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StandardScaler.html#pyspark.ml.feature.StandardScaler"><code>StandardScaler</code></a>, we can rescale all our features.</p>
<p>For example, <code>StandardScaler</code> makes all features have mean 0 and variance 1. It expects the features to already be prepared as a vector of features in a single column, like <code>RFormula</code> creates for us. <code>StandardScaler</code> is an <code>Estimator</code>, so it must be fit on data (to calculate the means and variances) before it is used to transform (standardize) the data. It takes an <code>inputCol</code> argument naming the column to standardize and <code>outputCol</code> naming the new, standardized column to create; the other scalers all work in a similar way.</p>
<p>To use it, we first create the scaler and then fit it:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> StandardScaler</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># by default, StandardScaler does not center the data; set withMean=True to</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># center (subtract the mean)</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler(inputCol<span class="op">=</span><span class="st">"features"</span>, outputCol<span class="op">=</span><span class="st">"scaledFeatures"</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                        withMean<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>fitted_scaler <span class="op">=</span> scaler.fit(train)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> fitted_scaler.transform(train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now <code>train</code> contains a <code>scaledFeatures</code> column where each feature has mean 0 and variance 1.</p>
<p>Notice that we are training the scaler using the <em>training</em> data, not the entire dataset. This is important because it ensures that we do not use the test data in any way, even for scaling, when fitting models. We can later use the <code>fitted_scaler</code> to rescale the test data.</p>
</section>
</section>
<section id="fitting-models" class="level2">
<h2 class="anchored" data-anchor-id="fitting-models">Fitting models</h2>
<p>Once the data is prepared, we can fit a model. Spark has a whole range of supported models for <a href="https://spark.apache.org/docs/3.3.1/api/python/reference/pyspark.ml.html#classification">classification</a>, <a href="https://spark.apache.org/docs/3.3.1/api/python/reference/pyspark.ml.html#clustering">clustering</a>, and <a href="https://spark.apache.org/docs/3.3.1/api/python/reference/pyspark.ml.html#regression">regression</a>. In this toy example, we’d like to do logistic regression to predict the binary response.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> LogisticRegression</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> LogisticRegression(labelCol<span class="op">=</span><span class="st">"label"</span>, featuresCol<span class="op">=</span><span class="st">"scaledFeatures"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This <code>lr</code> object represents an abstract logistic regression with particular columns for features and labels (responses). We must tell it where to find the true labels (<code>labelCol</code>) and where to find the features (<code>featuresCol</code>); by default, it looks in columns named <code>label</code> and <code>features</code>, matching the default column names created by <code>RFormula</code>.</p>
<p><code>lr</code> is again an <code>Estimator</code> object. To fit this to the data, we must run</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>fittedLR <span class="op">=</span> lr.fit(train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now Spark has fit the model. We can get its coefficients with <code>fittedLR.coefficients</code>.</p>
<p>Note that logistic regression in Spark is not just ordinary logistic regression: it supports lasso and ridge penalization through the elastic net, and it can support various other constraints on the coefficients. Using <code>lr.explainParams()</code> you can have Spark explain all the available options.</p>
<p>Fitted models in Spark all have a common interface: they are <code>Transformer</code> objects that can transform a dataset to a new dataset. In models, the transformation is adding a column of predictions; in <code>RFormula</code>, as we saw, the transformation is adding a column of features.</p>
</section>
<section id="making-pipelines" class="level2">
<h2 class="anchored" data-anchor-id="making-pipelines">Making pipelines</h2>
<p>Often you will want to try multiple models with multiple parameter settings, fitting them all to the same data and evaluating them using common metrics. Instead of writing code to do this manually, we can use Spark’s <em>pipeline</em> system, which lets us specify a pipeline of tasks to complete with a range of parameter settings.</p>
<p>Let’s start with a random split of the original dataset (not the prepared data).</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>train, test <span class="op">=</span> df.randomSplit([<span class="fl">0.7</span>, <span class="fl">0.3</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now let’s specify our pipeline. We will first prepare the data with a model formula, as before, and then we will fit a logistic regression. Notice here we have not specified the actual formula or regression options:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>rForm <span class="op">=</span> RFormula()</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> LogisticRegression(labelCol<span class="op">=</span><span class="st">"label"</span>, featuresCol<span class="op">=</span><span class="st">"features"</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline().setStages([rForm, lr])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Each stage in the pipeline can be any kind of <code>Estimator</code> or <code>Transformer</code>. The pipeline itself is an <code>Estimator</code>. We use it in two steps:</p>
<ul>
<li><code>pipeline.fit(dataset)</code>: Provides the dataset to the first stage. If it’s an <code>Estimator</code>, call the <code>fit(dataset)</code> method, then call <code>transform(dataset)</code> on the fitted object. If it’s a <code>Transformer</code>, call <code>transform(dataset)</code>. Take the output and repeat on the next stage of the pipeline. This returns a <code>PipelineModel</code>, which is a <code>Transformer</code> containing all the fitted stages.</li>
<li><code>pipelinemodel.transform(dataset)</code>: Each stage of the pipeline is now a <code>Transformer</code>, because it was previously fit to data. Call `transform(dataset) on each stage in turn to produce the pipeline output.</li>
</ul>
<p>To set the options for each pipeline stage, we can use <code>ParamGridBuilder</code>. This lets us specify a grid of options—a grid because when we use this in our pipeline, it will try every combination of the options.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.tuning <span class="im">import</span> ParamGridBuilder</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> ParamGridBuilder() <span class="op">\</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  .addGrid(rForm.formula, [</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"lab ~ . + color:value1"</span>,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"lab ~ . + color:value1 + color:value2"</span>]) <span class="op">\</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>  .addGrid(lr.elasticNetParam, [<span class="fl">0.0</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>]) <span class="op">\</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>  .addGrid(lr.regParam, [<span class="fl">0.1</span>, <span class="fl">2.0</span>]) <span class="op">\</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  .build()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This grid represents <span class="math inline">\(2 \times 3 \times 2 = 12\)</span> different models to try.</p>
</section>
<section id="evaluating-models" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-models">Evaluating models</h2>
<p>Now that we have specified the pipeline and the range of parameter options, we must tell Spark how we plan to evaluate our models. There is a basic <code>Evaluator</code> class to support evaluation, with <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#evaluation">several specific kinds of evaluator available</a>. The <code>BinaryClassificationEvaluator</code> lets us choose from a variety of classification metrics; here we will use the AUC:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.evaluation <span class="im">import</span> BinaryClassificationEvaluator</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>evaluator <span class="op">=</span> BinaryClassificationEvaluator() <span class="op">\</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  .setMetricName(<span class="st">"areaUnderROC"</span>) <span class="op">\</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  .setRawPredictionCol(<span class="st">"prediction"</span>) <span class="op">\</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  .setLabelCol(<span class="st">"label"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Notice we have also told it which column the predictions will be in and where the true labels are, so it can compare prediction to truth.</p>
<p>Now we need a strategy for Spark to evaluate each model. One way is a train/validation split: split the data into two pieces, train on the training piece, and evaluate on the test piece. This is all on the original test set created with <code>df.randomSplit()</code>, since we are saving the original test set for final evaluation.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.tuning <span class="im">import</span> TrainValidationSplit</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>tvs <span class="op">=</span> TrainValidationSplit() <span class="op">\</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  .setTrainRatio(<span class="fl">0.75</span>) <span class="op">\</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  .setEstimatorParamMaps(params) <span class="op">\</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  .setEstimator(pipeline) <span class="op">\</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  .setEvaluator(evaluator)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>tvs</code> is now an object that can be fit to data, and will produce a model object representing the fitted model:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>tvsFitted <span class="op">=</span> tvs.fit(train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This sets the parameters in the pipeline object, calls <code>fit()</code> on it, gets the predictions via <code>transform()</code>, evaluates them, and repeats for all parameter combinations. <code>tvsFitted</code> is now a <code>TrainValidationSplitModel</code> object, which is a <code>Transformer</code>. We use its <code>transform()</code> method to make predictions with the best model (as evaluated by our <code>evaluator</code>) on our final held-out test set, then get the final AUC:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>evaluator.evaluate(tvsFitted.transform(test))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We might also want to get the tuning parameters of the best model. We had <code>TrainValidationSplit</code> estimate an entire pipeline, and so the “best model” is actually a <code>PipelineModel</code> from fitting our <code>pipeline</code>. This is stored in <code>tvsFitted.bestModel</code>.</p>
<p>This object contains a <code>.stages</code> attribute giving all the stages; in our case, this is a list of the fitted <code>RFormula</code> (<code>rform</code>) and the fitted logistic regression (<code>lr</code>). So we can get parameters from the logistic regression:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>tvsFitted.bestModel.stages[<span class="dv">1</span>].getRegParam()</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>tvsFitted.bestModel.stages[<span class="dv">1</span>].getElasticNetParam()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Besides <code>TrainValidationSplit</code>, there is also a <code>CrossValidator</code> that works the same way, but does <em>K</em>-fold cross-validation to get the error estimates.</p>
</section>
<section id="reviewing-the-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="reviewing-the-pipeline">Reviewing the pipeline</h2>
<p>We’ve seen a lot of pieces: Estimators, transformers, tools for coding variables, tools for scaling and transforming, models, evaluators, hyperparameter tuning, and so on. It’s hard to see how this all fits together.</p>
<p>The basic pipeline is meant to look like this:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    data[Data]
    rform[Make feature/label columns]
    scale[Scale and transform data]
    fit[Fit model]
    predict[Make predictions]

    data --&gt; rform
    rform --&gt; scale
    scale --&gt; fit
    fit --&gt; predict
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Each step in this pipeline is one or more <code>Transformer</code> or <code>Estimator</code> objects, and the entire pipeline is represented as a <code>Pipeline</code> object with multiple stages. A <code>Pipeline</code> is itself an <code>Estimator</code>: the entire pipeline can be fit to data and then used to transform new data.</p>
<p>But pipelines have parameters that need to be tuned to optimize some evaluation metric. And we need some kind of train/test split to evaluate fairly. So we use a tuner like <code>TrainValidationSplit</code>:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    split["Split training and test data &lt;br&gt;(TrainValidationSplit)"]
    subgraph pipeline["Pipeline() object"]
        rform[Make feature/label columns]
        scale[Scale and transform data]
        fit[Fit model]

        rform --&gt; scale
        scale --&gt; fit
    end

    split--&gt;|Training data|pipeline

    fitted["Fitted pipeline&lt;br&gt;(PipelineModel)"]
    fit --&gt; fitted
    split--&gt;|Test data|fitted

    predictions[Predictions]
    fitted --&gt; predictions

    eval[Evaluator]
    predictions --&gt; eval
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This may need to be repeated for each combination of parameters we want to test, so we can choose the best.</p>
</section>
<section id="saving-models-for-later" class="level2">
<h2 class="anchored" data-anchor-id="saving-models-for-later">Saving models for later</h2>
<p>If you’re working on a model for a product, once you arrive at a final model, you’re going to want to be able to use it later. Fitted models (such as fitted <code>PipelineModel</code> objects) have a <code>write()</code> method that can be used to save them:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>tvsFitted.write().save(<span class="st">"path/to/fitted-logit-model"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As with all Spark operations that involve reading and writing files, the Spark executors write most of the crucial data, so the path provided should be one that all executors have access to, such as a directory stored in HDFS or a shared filesystem.</p>
<p>Since we saved a <code>PipelineModel</code>, we can also load the saved model back with <code>PipelineModel</code>.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>reloaded_model <span class="op">=</span> PipelineModel.load(<span class="st">"path/to/fitted-logit-model"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is now a <code>PipelineModel</code> object we can use just like <code>tvsFitted</code> above, except we could have exited Spark in between saving the model and loading it. Now you can work with the same model across many Spark sessions or reuse it in many applications.</p>
</section>
<section id="exercises" class="level2">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<div id="exr-spark-ml-galaxy" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1 (Galaxy mass prediction)</strong></span> The <code>hive_metastore.default.buzzard_dc_1</code> table contains data from a physical simulation of 111,172 galaxies. The data <a href="https://cmustatistics.github.io/data-repository/astronomy/buzzard.html">is described here</a>. Review the description so you’re familiar with the data.</p>
<p>We would like to predict galaxy stellar masses (<code>log.mass</code>) using <code>redshift</code> and the magnitude variables <code>u</code>, <code>g</code>, <code>r</code>, <code>i</code>, <code>z</code>, and <code>y</code>.</p>
<p>Note that Spark uses <code>.</code> to refer to entries in structs (records), so <code>log.mass</code> will be interpreted as accessing the <code>mass</code> field of the <code>log</code> object. You must rename the variables and remove any dots (even in the columns you do not use, like <code>u.err</code>) to avoid confusing it.</p>
<p>First, set aside a final test set of 30% of the data, randomly selected. (Use the <code>randomSplit()</code> method of <code>DataFrame</code> objects to do so.) We will use this to evaluate your pipelines.</p>
<p>Next, build three pipelines to predict <code>log.mass</code>. Each has parameters you should tune with 5-fold cross-validation:</p>
<ol type="1">
<li>Linear regression. Tune the regularization parameters, and also consider using either (a) a linear function of the original features, (b) a quadratic function of them, or (c) a cubic function of them. The <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.PolynomialExpansion.html"><code>PolynomialExpansion</code> transformer</a> may be helpful.</li>
<li>A decision tree (<a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.DecisionTreeRegressor.html">using <code>pyspark.ml.regression.DecisionTreeRegressor</code></a>). Tune the maximum depth and maximum number of bins.</li>
<li>A random forest (using <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html"><code>pyspark.ml.regression.RandomForestRegressor</code></a>). Tune the maximum tree depth and the maximum number of bins.</li>
</ol>
<p>Tune each to select the model with the best mean absolute error (MAE). Report the best predictor, its MAE, and its tuning parameters.</p>
</div>
<div id="exr-spark-lending-club-ml" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2 (Machine learning for LendingClub)</strong></span> For this exercise, we’ll continue using the LendingClub data from <strong>?@exr-spark-lendingclub</strong>.</p>
<p>If you were an investor deciding which loans to fund, you’d be interested in predicting whether the borrower would pay the loan back on time. You might split loans into three categories:</p>
<ul>
<li>Loans fully paid off, with no late payments</li>
<li>Loans paid back, but late</li>
<li>Loans never fully paid back</li>
</ul>
<p>The <code>loan_status</code> variable records the status of loans. There are several possible codes:</p>
<ul>
<li><code>Fully Paid</code>: The loan has been paid off</li>
<li><code>Current</code>: The borrower has been paying on time, but still has more payments to make</li>
<li><code>In Grace Period</code>: The borrower is up to 15 days late in their payments; no late fees are charged until they’re over 15 days late</li>
<li><code>Late (16-30 days)</code>: The borrower is 16-30 days late in their payments and will be charged a late fee</li>
<li><code>Late (31-120 days)</code>: The borrower is 31-120 days late in their payments</li>
<li><code>Default</code>: The borrower is more than 120 days late in payments, despite repeated reminders from LendingClub. LendingClub will now begin closing the loan</li>
<li><code>Charged Off</code>: The borrower has defaulted on the loan and LendingClub no longer expects any further payments to be made. They may sell the loan to a debt collection agency, or the borrower may have declared bankruptcy, in which case a court will decide how much money LendingClub may receive.</li>
</ul>
<p>The status gives the current status (at the time this data was downloaded), but even a current or fully paid loan may have been late in the past and charged a late fee. The <code>total_rec_late_fee</code> column records the total late fees paid by the borrower.</p>
<p>Create a notebook that completes the following tasks.</p>
<p>Create a new column categorizing loans into the three categories above. You’ll have to filter out loans that are current and have not been paid off yet, because we do not know their ultimate status – they may become late or default later.</p>
<p>An investor may want to predict this categorization using information available at the time the borrower applies for the loan, such as: annual income (<code>annual_inc</code>), the number of times they’ve been delinquent on other accounts in the past two years (<code>delinq_2yrs</code>), their reason for asking for the loan (<code>purpose</code>), how many mortgages the borrower has (<code>mort_acc</code>), how many bankruptcies the borrower has had (<code>pub_rec_bankruptcies</code>), the borrower’s FICO credit score (<code>fico_range_high</code> and <code>fico_range_low</code>), and the amount they owe on credit cards (<code>max_bal_bc</code>). (There are many other covariates in the data, but we should start simple.)</p>
<p>Extract these features from the data. Set aside 30% of the data to use as a test set.</p>
<p>On the training set, use Spark’s <code>MinMaxScaler</code> to scale all variables to be on the same scale. Then fit three classifiers to predict the loan categorization:</p>
<ol type="1">
<li>A decision tree (<a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.DecisionTreeClassifier.html"><code>DecisionTreeClassifier</code></a>), using the default tuning parameters</li>
<li>A random forest (<a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.RandomForestClassifier.html"><code>RandomForestClassifier</code></a>), using the default tuning parameters (which fit 20 trees with a depth of 5)</li>
<li>A multilayer perceptron with three layers of 40 nodes each (<a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.MultilayerPerceptronClassifier.html"><code>MultilayerPerceptronClassifier</code></a>). The <code>layers</code> argument lets you specify the size of each layer, <em>including</em> the input and output layers; here you should have 20 inputs (from your predictors, including dummies for categorical variables) and 3 outputs (for the 3 classes).</li>
</ol>
<p>Evaluate the three classifiers on the test set and report their accuracy and F1 scores. (With <code>MulticlassClassificationEvaluator</code>, you’ll need <code>metricName="f1"</code> and <code>metricName="accuracy"</code>.)</p>
<p>Turn in your notebook that does this complete task, starting with loading the data and ending with saving the test set metrics to a file. Include a table of accuracy and F1 scores.</p>
<p><em>Hints:</em> See <strong>?@sec-creating-dataframes</strong> to see how to create a data frame containing the evaluation results, so you can save it to a file. Review <strong>?@sec-spark-expressions</strong>, and following sections, to see how to manipulate and create columns in Spark.</p>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>