<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.34">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>spark-cloud</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-c8ad9e5dbd60b7b70b38521ab19b7da4.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-57255bc51a02dad1602480f616b5f366.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="fullcontent quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="spark-in-the-cloud" class="level1">
<h1>Spark in the Cloud</h1>
<p>As we saw previously (<strong>?@sec-spark</strong>), Apache Spark is designed for distributed data analysis: splitting up a data analysis task to operate on chunks of the data that may be hosted on many different machines. In a cloud computing context, that means the data is often hosted on many rented machines, or stored in the cloud provider’s object storage system and analyzed by many machines at a time.</p>
<p>First, let’s return to <strong>?@sec-spark-architecture</strong> and recall how Spark is set up and runs on multiple machines.</p>
<p>Now that we recall the details about drivers and executors, let’s talk about how we can write Spark jobs that run in the cloud, instead of just on our laptops.</p>
<p>Previously we used <code>pyspark</code>, a program that opened an interactive Python shell and let us type commands and see their output. That’s great to test things out, or for a quick interactive data analysis, but it’s not so good for a big data analysis job requiring many hours to run. Let’s see how to submit jobs at the command line.</p>
<section id="submitting-a-job" class="level2">
<h2 class="anchored" data-anchor-id="submitting-a-job">Submitting a job</h2>
<p>When installed on a computer, Spark provides a number of utility programs. Let’s say Spark is installed in the <code>$SPARK_HOME</code> – this is a shell notation for an environment variable, so the actual location is stored in a variable. (If you log in to our Spark server via SSH, this variable should already be defined in your shell; try running <code>echo $SPARK_HOME</code>.) Spark’s programs are stored in <code>$SPARK_HOME/bin</code>.</p>
<p>In there, you’ll find the <code>pyspark</code> program you used previously. You’ll also find <code>spark-submit</code>, the tool for submitting scripts to a Spark cluster.</p>
<p>As we described before, Spark can run Python, Java, or Scala programs, but we’ll be working in Python. Here’s a simple usage of <code>spark-submit</code> for a Python script:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="va">$SPARK_HOME</span><span class="ex">/bin/spark-submit</span> <span class="dt">\</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">--master</span> spark://localhost:7077 <span class="dt">\</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">--deploy-mode</span> client <span class="dt">\</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    some-analysis-script.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This sends <code>some-analysis-script.py</code> to Spark. Specifically:</p>
<ul>
<li>We choose the <code>client</code> deployment mode, meaning that our script is run on the local machine and sends Spark operations to the cluster.</li>
<li>The script becomes the Spark <em>driver</em>. It loads the <code>SparkContext</code> and contains all the code telling Spark what analyses to do (what data to load, what aggregations and calculations to do, etc.)</li>
<li>The script connects to the specified Spark cluster master. Here we’ve specified <code>spark://localhost:7077</code>, meaning the script will connect to a Spark cluster running on the same machine (<code>localhost</code>) on port 7077. The master is part of the Spark cluster manager, and is responsible for creating Spark executors to actually do the data analysis.</li>
</ul>
<p>There are many other options, but we’ll consider them later if we need them.</p>
</section>
<section id="an-example-spark-cluster" class="level2">
<h2 class="anchored" data-anchor-id="an-example-spark-cluster">An example Spark cluster</h2>
<p>Let’s suppose we have a Spark cluster with five machines:</p>
<ul>
<li>spark-users: Has accounts for all the data analysts who need to use Spark, and lets them log in to run <code>spark-submit</code> or <code>pyspark</code>.</li>
<li>spark-master: Runs the cluster manager.</li>
<li>spark-worker-1 through spark-worker-3: Members of the cluster that run executors when needed.</li>
</ul>
<p>Each of these is a separate machine with its own operating system and files.</p>
</section>
<section id="an-example-job" class="level2">
<h2 class="anchored" data-anchor-id="an-example-job">An example job</h2>
<p>Now let’s suppose we log in to the spark-users server and use <code>spark-submit</code> to run the following Python file:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql <span class="im">import</span> SparkSession</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the SparkSession, which makes this script the driver and</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># connects to the cluster manager to get executors</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>spark <span class="op">=</span> SparkSession <span class="op">\</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    .builder <span class="op">\</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    .appName(<span class="st">"Python Spark SQL basic example"</span>) <span class="op">\</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    .getOrCreate()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data. The path is a location on the *executor* servers, so</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># the data must be available there</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> spark.read.json(<span class="st">"examples/src/main/resources/people.json"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Do an action</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>df.groupBy(<span class="st">"age"</span>).count()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the “client” deploy-mode, this Python script runs on spark-users, so it has access to the Python libraries available on that machine.</p>
<section id="getting-resources-for-the-job" class="level3">
<h3 class="anchored" data-anchor-id="getting-resources-for-the-job">Getting resources for the job</h3>
<p><code>spark-submit</code> will connect to the cluster manager (as specified by the <code>--master</code> command-line argument) and request executors on which to run this application. We can control how many executors it will request from the cluster manager. For example, if we include <code>--total-executor-cores 2</code> when calling <code>spark-submit</code>, our job will only get two CPUs on the cluster to run on; if we do not include the option, it will request all currently available cores, however many that may be.</p>
<p>We can also request specific amounts of memory, if that should be necessary. By default, each executor gets 1 GB of memory, but we can ask for different amounts with the <code>--executor-memory</code> command-line argument. For example, use <code>--executor-memory 10G</code> to request 10 GB for each executor. This might be necessary if you’re doing enormous joins on large datasets.</p>
<p>Be careful, though: you’re limited by what is available in your cluster. If you request more cores than are available, you’ll only get what’s available. If someone else is currently using <em>all</em> the cores, you may see a message like this:</p>
<pre><code>WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources</code></pre>
<p>This could happen because there are no cores available at all, or because there is insufficient memory available. In standalone cluster mode, <code>spark-submit</code> will simply retry every 15 seconds until sufficient resources are available; other cluster systems, like Mesos or YARN, have queuing systems where jobs queue up to run and are served in order.</p>
</section>
<section id="breaking-the-job-into-pieces" class="level3">
<h3 class="anchored" data-anchor-id="breaking-the-job-into-pieces">Breaking the job into pieces</h3>
<p>Recall that in Spark, nothing is actually calculated until we perform an <em>action</em>. Actions include things like printing a data frame or writing it to a file. Until then, all of our operations (<code>groupBy()</code>, <code>filter()</code>, and so on) are collected together.</p>
<p>When we perform the action, Spark breaks those operations into <em>stages</em>. A stage is a group of operations that can be done simultaneously on many machines. After each stage, data may be shuffled between machines, putting it where it is needed for the next stage of operations.</p>
<p>However, the output of this action doesn’t go anywhere visible to us: <code>spark-submit</code> provides lots of debugging output, but not the values printed from the script. We’ll need to write output to data files as needed. We’ll discuss files and data management next.</p>
</section>
</section>
<section id="sec-working-with-hdfs" class="level2">
<h2 class="anchored" data-anchor-id="sec-working-with-hdfs">Working with distributed data</h2>
<p>Back in <strong>?@sec-distributed-data</strong>, we talked about the idea of <em>distributed data</em>. A distributed file system looks like any other file system from the outside – with files stored in directories inside other directories – but stores its data on multiple machines, often redundantly, and fetches data from machines as needed.</p>
<p>With Spark, a common choice is the <a href="https://hadoop.apache.org/docs/current1/hdfs_design.html">Hadoop Distributed File System</a>. HDFS is designed to work when spread across thousands of machines for enormous datasets, but it presents a file system that looks much like any other.</p>
<p>It’s organized by using a single NameNode and multiple DataNodes. The NameNode is a machine that keeps track of which files are stored in the filesystem and which machines they live on; the DataNodes are the machines that actually store the data. A client trying to open a file asks the NameNode which DataNodes have it, and then fetches the contents from the DataNodes. The NameNode periodically checks the status of the DataNodes to find out if any of them has gone down or failed.</p>
<p>Typically, then, a company using HDFS would have dozens or hundreds of servers, each with as many hard drives as can fit, serving as DataNodes. In a basic setup, there is only one NameNode, so its failure would prevent any user from accessing files; since Hadoop 2.0, it’s been possible to set up a <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">“High Availability” cluster</a> that has multiple NameNodes, one serving requests and others waiting in reserve in case the primary fails.</p>
<p>To access data on HDFS from Spark, we need the address of the HDFS NameNode. We can then provide a path that begins with <code>hdfs://</code>. For example, we could run</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> spark.read.parquet(<span class="st">"hdfs://10.0.0.4:9000/lending-club.parquet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>to read a Parquet file called <code>lending-club.parquet</code> from the NameNode at <code>10.0.0.4:9000</code>.</p>
<p>We can also view and manipulate HDFS files from the command line. The program <code>$HADOOP_HOME/bin/hdfs</code> has a sub-command <code>dfs</code> to manipulate the file system. For example,</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="va">$HADOOP_HOME</span><span class="ex">/bin/hdfs</span> dfs <span class="at">-ls</span> /</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>lists the files in the root directory. Run <code>$HADOOP_HOME/bin/hdfs dfs</code> to see a list of all available commands. The <code>-copyFromLocal</code> and <code>-copyToLocal</code> commands may be particularly useful, as they allow you to copy a file from your local filesystem into HDFS, and to copy a file from HDFS back into your local filesystem.</p>
</section>
<section id="sec-using-spark-cluster" class="level2">
<h2 class="anchored" data-anchor-id="sec-using-spark-cluster">Using our Spark cluster</h2>
<p>TODO Update to Databricks</p>
<p>We have a class Spark cluster set up on Azure. It has several machines:</p>
<ul>
<li>msp-cool-stuff: Configured as the Spark cluster manager. You’ll log in to this machine to run <code>spark-submit</code> or <code>pyspark</code>. You can log in via SSH following the instructions in the email you received.</li>
<li>msp-spark-worker-1 through msp-spark-worker-3: Spark workers. Each has 8 cores and 32 GB of RAM.</li>
</ul>
<p>This means our cluster has 24 cores and 96 GB available for student use.</p>
<p>The cluster runs HDFS at <code>hdfs://10.0.0.4:9000/</code>. Each student has a directory in HDFS that matches your username. If your username is <code>jdoe</code>, you have access to a directory <code>/jdoe/</code>, and can write results to that directory. So if you need to write output files, write them to paths beginning with <code>hdfs://10.0.0.4:9000/yourusername/</code>.</p>
<p>Some basic rules for use:</p>
<ul>
<li><p><strong>If you run the PySpark shell,</strong> set <code>--total-executor-cores 2</code>. Otherwise you will get all the available cores; if nobody else is using the cluster at the time, you’ll get the entire cluster, and nobody will be able to use it until you exit PySpark.</p>
<p>You can hence start PySpark by running:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="va">$SPARK_HOME</span><span class="ex">/bin/pyspark</span> <span class="dt">\</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">--master</span> spark://10.0.0.4:7077 <span class="dt">\</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">--total-executor-cores</span> 2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>If you’re submitting a job</strong> with <code>spark-submit</code>, you can use the defaults and request all available cores – but do not allow your job to take more than a few minutes, so other students do not have to wait. (You can use Ctrl-C to stop the job.) If other students are using the cluster, you may have to wait for their jobs to finish before your job can start. Just leave <code>spark-submit</code> open until it gets resources and starts running your job.</p>
<p>You run <code>spark-submit</code> with the command</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="va">$SPARK_HOME</span><span class="ex">/bin/spark-submit</span> <span class="dt">\</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">--master</span> spark://10.0.0.4:7077 <span class="dt">\</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">--deploy-mode</span> client <span class="dt">\</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    your-script.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="exercises" class="level2">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<p>TODO</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>